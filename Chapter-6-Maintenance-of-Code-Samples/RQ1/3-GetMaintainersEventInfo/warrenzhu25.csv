event_id,event_type,event_repository,code_sample_event,event_created_at,event_payload,event_action,language
20907573795,PushEvent,warrenzhu25/spark,0.0,2022-03-24T05:08:00Z,"{'push_id': 9433517948, 'size': 560, 'distinct_size': 350, 'ref': 'refs/heads/master', 'head': '057c051285ec32c665fb458d0670c1c16ba536b2', 'before': 'f7dd37c40fa1ef5ed813b044080c07a19589e3d1', 'commits': [{'sha': '1431a4aa74cf69b3ee607e313dece6fed8390de6', 'author': {'email': 'viirya@gmail.com', 'name': 'Liang-Chi Hsieh'}, 'message': ""[SPARK-37887][CORE] Fix the check of repl log level\n\n### What changes were proposed in this pull request?\n\nThis patch fixes the check of repl's log level. So we can correctly know if the repl class is set with log level or not.\n\n### Why are the changes needed?\n\nSame as the check in `SparkShellLoggingFilter`, `getLevel` cannot be used anymore to check if the log level is set or not for a logger in log4j2.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo\n\n### How was this patch tested?\n\nManual verified locally.\n\nCloses #35198 from viirya/SPARK-37887.\n\nAuthored-by: Liang-Chi Hsieh <viirya@gmail.com>\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>"", 'distinct': False, 'url': 'https://api.github.com/repos/warrenzhu25/spark/commits/1431a4aa74cf69b3ee607e313dece6fed8390de6'}, {'sha': 'b092321077400e4344f8ea11592600f5e759041b', 'author': {'email': 'gurwls223@apache.org', 'name': 'Hyukjin Kwon'}, 'message': ""[SPARK-37879][INFRA] Show test report in GitHub Actions builds from PRs\n\n### What changes were proposed in this pull request?\n\nThis PR is a retry of https://github.com/apache/spark/pull/35179. This PR does the same thing - replacing Actions view to Check run view for `See test results` link.\n\nThe main difference with the PR https://github.com/apache/spark/pull/35179 is that we now keep the Actions run id as is as metadata so this Actions run id can be used to update the status of tests in PRs at Apache Spark:\n\nhttps://github.com/apache/spark/blob/85efc85f9aa93b3fac9e591c96efa38d4414adf8/.github/workflows/update_build_status.yml#L63-L74\n\nNow this PR shouldn't affect [update_build_status.yml](https://github.com/apache/spark/blob/master/.github/workflows/update_build_status.yml) which was the main reason of a followup and revert.\n\n### Why are the changes needed?\n\nFor developers to see the test report, and they can easily detect which test is failed.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo, dev-only\n\n### How was this patch tested?\n\nTested in https://github.com/HyukjinKwon/spark/pull/51.\n\nCloses #35193 from HyukjinKwon/SPARK-37879-retry.\n\nAuthored-by: Hyukjin Kwon <gurwls223@apache.org>\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>"", 'distinct': False, 'url': 'https://api.github.com/repos/warrenzhu25/spark/commits/b092321077400e4344f8ea11592600f5e759041b'}, {'sha': 'db9807443dabaee8237a4748c99572c010ddb0c9', 'author': {'email': 'mszymkiewicz@gmail.com', 'name': 'zero323'}, 'message': ""[MINOR][PYTHON] Replace Iterable import from collections with collections.abc\n\n### What changes were proposed in this pull request?\n\nReplace\n\n```python\nfrom collections import Iterable\n```\n\nwith\n\n```python\nfrom collections.abc import Iterable\n```\n\n### Why are the changes needed?\n\n> Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.10 it will stop working.\n\nIn other places, like `pyspark.pandas.indexing`\n\nhttps://github.com/apache/spark/blob/99805558fc80743747f32c7008cb7cc99c1cda01/python/pyspark/pandas/indexing.py#L22\n\nwe already import from `collections.abc`, but this one somehow passed under the radar.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo.\n\n### How was this patch tested?\n\nExisting unit tests.\n\nCloses #35197 from zero323/TYPING-ABC-ITERABLE.\n\nAuthored-by: zero323 <mszymkiewicz@gmail.com>\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>"", 'distinct': False, 'url': 'https://api.github.com/repos/warrenzhu25/spark/commits/db9807443dabaee8237a4748c99572c010ddb0c9'}, {'sha': '31d8489f3993f608ed2c8d39727b345ac71170b8', 'author': {'email': 'ueshin@databricks.com', 'name': 'Takuya UESHIN'}, 'message': '[SPARK-37903][PYTHON] Replace string_typehints with get_type_hints\n\n### What changes were proposed in this pull request?\n\nReplaces `string_typehints` with `get_type_hints`.\n\n### Why are the changes needed?\n\nCurrently we have a hacky way to resolve type hints written as strings, but we can use `get_type_hints` instead.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo.\n\n### How was this patch tested?\n\nExisting tests.\n\nCloses #35200 from ueshin/issues/SPARK-37903/string_typehints.\n\nAuthored-by: Takuya UESHIN <ueshin@databricks.com>\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>', 'distinct': False, 'url': 'https://api.github.com/repos/warrenzhu25/spark/commits/31d8489f3993f608ed2c8d39727b345ac71170b8'}, {'sha': '2ed827a8169a0ad098c5ee4f6101bd003f8c6de2', 'author': {'email': 'huaxin_gao@apple.com', 'name': 'Huaxin Gao'}, 'message': '[SPARK-37627][SQL][FOLLOWUP] Separate SortedBucketTransform from BucketTransform\n\n### What changes were proposed in this pull request?\n\n1. Currently only a single bucket column is supported in `BucketTransform`, fix the code to make multiple bucket columns work.\n2. Separate `SortedBucketTransform` from `BucketTransform`, and make the `arguments` in `SortedBucketTransform` in the format of `columns numBuckets sortedColumns` so we have a way to find out the `columns` and `sortedColumns`.\n3. add more test coverage.\n\n### Why are the changes needed?\n\nFix bugs in `BucketTransform` and `SortedBucketTransform`.\n\n### Does this PR introduce _any_ user-facing change?\nNo\n\n### How was this patch tested?\nNew tests\n\nCloses #34914 from huaxingao/sorted_followup.\n\nLead-authored-by: Huaxin Gao <huaxin_gao@apple.com>\nCo-authored-by: huaxingao <huaxin.gao11@gmail.com>\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>', 'distinct': False, 'url': 'https://api.github.com/repos/warrenzhu25/spark/commits/2ed827a8169a0ad098c5ee4f6101bd003f8c6de2'}, {'sha': 'a1e86373253d77329b2b252c653a69ae8ac0bd6c', 'author': {'email': 'karen.feng@databricks.com', 'name': 'Karen Feng'}, 'message': ""[SPARK-37859][SQL] Do not check for metadata during schema comparison\n\n### What changes were proposed in this pull request?\n\nIgnores the metadata when comparing the user-provided schema and the actual schema during BaseRelation resolution.\n\n### Why are the changes needed?\n\nMakes it possible to read tables with Spark 3.2 that were written with Spark 3.1, as https://github.com/apache/spark/blob/bd24b4884b804fc85a083f82b864823851d5980c/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils.scala#L312 added a new metadata field that broke compatibility.\n\n### Does this PR introduce _any_ user-facing change?\n\nYes. Previously, an error was thrown when a SQL table written with JDBC in Spark 3.1 was read in Spark 3.2. Now, no error is thrown.\n\n### How was this patch tested?\n\nUnit test and manual test with a SQL table written with Spark 3.1.\n\nQuery:\n\n```\nselect * from jdbc_table\n```\n\nBefore:\n\n```\norg.apache.spark.sql.AnalysisException: The user-specified schema doesn't match the actual schema:\n```\n\nAfter: no error\n\nCloses #35158 from karenfeng/SPARK-37859.\n\nAuthored-by: Karen Feng <karen.feng@databricks.com>\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>"", 'distinct': False, 'url': 'https://api.github.com/repos/warrenzhu25/spark/commits/a1e86373253d77329b2b252c653a69ae8ac0bd6c'}, {'sha': '1ef56e969f42726c630572f4e781cbeb3b57b888', 'author': {'email': 'dongjoon@apache.org', 'name': 'Dongjoon Hyun'}, 'message': '[SPARK-37893][CORE][TESTS] Avoid ConcurrentModificationException related to SparkFunSuite.LogAppender#_threshold""\n\n### What changes were proposed in this pull request?\nWhen `mvn clean install -pl sql/core -am -Pscala-2.13` is executed, `AdaptiveQueryExecSuite` failed with a very small probability, the error stack as follows:\n\n```\n- Logging plan changes for AQE *** FAILED ***\n  java.util.ConcurrentModificationException: mutation occurred during iteration\n  at scala.collection.mutable.MutationTracker$.checkMutations(MutationTracker.scala:43)\n  at scala.collection.mutable.CheckedIndexedSeqView$CheckedIterator.hasNext(CheckedIndexedSeqView.scala:47)\n  at scala.collection.StrictOptimizedIterableOps.filterImpl(StrictOptimizedIterableOps.scala:225)\n  at scala.collection.StrictOptimizedIterableOps.filterImpl$(StrictOptimizedIterableOps.scala:222)\n  at scala.collection.mutable.ArrayBuffer.filterImpl(ArrayBuffer.scala:43)\n  at scala.collection.StrictOptimizedIterableOps.filterNot(StrictOptimizedIterableOps.scala:220)\n  at scala.collection.StrictOptimizedIterableOps.filterNot$(StrictOptimizedIterableOps.scala:220)\n  at scala.collection.mutable.ArrayBuffer.filterNot(ArrayBuffer.scala:43)\n  at org.apache.spark.SparkFunSuite$LogAppender.loggingEvents(SparkFunSuite.scala:288)\n  at org.apache.spark.sql.execution.adaptive.AdaptiveQueryExecSuite.$anonfun$new$152(AdaptiveQueryExecSuite.scala:1487)\n```\n\nSo this pr adds synchronous access control for `SparkFunSuite$LogAppender.loggingEvents` to avoid the above exceptions\n\n### Why are the changes needed?\nBug fix\n\n### Does this PR introduce _any_ user-facing change?\nNo\n\n### How was this patch tested?\nPass GA\n\nCloses #35190 from LuciferYang/SPARK-37893.\n\nLead-authored-by: Dongjoon Hyun <dongjoon@apache.org>\nCo-authored-by: yangjie01 <yangjie01@baidu.com>\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>', 'distinct': False, 'url': 'https://api.github.com/repos/warrenzhu25/spark/commits/1ef56e969f42726c630572f4e781cbeb3b57b888'}, {'sha': '489391a56dc16459dde27bf29168381ac90e5a34', 'author': {'email': 'yangjie01@baidu.com', 'name': 'yangjie01'}, 'message': '[SPARK-37880][BUILD] Upgrade Scala to 2.13.8\n\n### What changes were proposed in this pull request?\nThis pr aims to update from Scala 2.13.7 to Scala 2.13.8 for Apache Spark 3.3.\n\n### Why are the changes needed?\nScala 2.13.8 is a maintenance release for 2.13 line and the release notes as follows:\n- https://github.com/scala/scala/releases/tag/v2.13.8\n\n### Does this PR introduce _any_ user-facing change?\nNo\n\n### How was this patch tested?\n\n- Pass the GitHub Action Scala 2.13 job\n\n- Manual test (Will add)\n\nCloses #35181 from LuciferYang/SPARK-37880.\n\nAuthored-by: yangjie01 <yangjie01@baidu.com>\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>', 'distinct': False, 'url': 'https://api.github.com/repos/warrenzhu25/spark/commits/489391a56dc16459dde27bf29168381ac90e5a34'}, {'sha': 'ef837ca71020950b841f9891c70dc4b29d968bf1', 'author': {'email': 'dongjoon@apache.org', 'name': 'Dongjoon Hyun'}, 'message': '[SPARK-37905][INFRA] Make `merge_spark_pr.py` set primary author from the first commit in case of ties\n\n### What changes were proposed in this pull request?\n\nThis PR aim to make `merge_spark_pr.py` set the primary author from the first commit in case of ties.\n\n### Why are the changes needed?\n\nCurrently, `merge_spark_pr.py` chooses the primary author randomly when there are two commits from two authors.\n\nhttps://github.com/apache/spark/pull/35190\n\nThe best case could choose the primary author based on the number of lines, but it seems to hard. So, this PR aims to become better than before.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo. This is a dev only.\n\n### How was this patch tested?\n\nManually.\n\nCloses #35205 from dongjoon-hyun/SPARK-37905.\n\nAuthored-by: Dongjoon Hyun <dongjoon@apache.org>\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>', 'distinct': False, 'url': 'https://api.github.com/repos/warrenzhu25/spark/commits/ef837ca71020950b841f9891c70dc4b29d968bf1'}, {'sha': 'fcc51767aa4a594868eff33a7ddd8c5242006cd8', 'author': {'email': 'wankun@apache.org', 'name': 'Kun Wan'}, 'message': '[SPARK-36967][CORE] Report accurate shuffle block size if its skewed\n\n### What changes were proposed in this pull request?\n\nA shuffle block is considered as skewed and will be accurately recorded in HighlyCompressedMapStatus if its size if larger than this factor multiplying  the median shuffle block size.\n\nBefore this change\n\n![map_status_before](https://user-images.githubusercontent.com/3626747/137251903-08a3544c-dc77-4b78-8ae5-93b42a54bd03.png)\n\nAfter this change\n\n![map_status_after](https://user-images.githubusercontent.com/3626747/137251871-355db24d-d66b-4702-8766-216db30a39e0.jpg)\n\n### Why are the changes needed?\n\nNow map task will report accurate shuffle block size if the block size is greater than ""spark.shuffle.accurateBlockThreshold""( 100M by default ). But if there are a large number of map tasks and the shuffle block sizes of these tasks are smaller than ""spark.shuffle.accurateBlockThreshold"", there may be unrecognized data skew.\n\nFor example, there are 10000 map task and 10000 reduce task, and each map task create 50M shuffle blocks for reduce 0, and 10K shuffle blocks for the left reduce tasks, reduce 0 is data skew, but the stat of this plan do not have this information.\n\n### Does this PR introduce _any_ user-facing change?\nNo\n\n### How was this patch tested?\n\nUpdate exists UTs\n\nCloses #34234 from wankunde/map_status.\n\nAuthored-by: Kun Wan <wankun@apache.org>\nSigned-off-by: attilapiros <piros.attila.zsolt@gmail.com>', 'distinct': False, 'url': 'https://api.github.com/repos/warrenzhu25/spark/commits/fcc51767aa4a594868eff33a7ddd8c5242006cd8'}, {'sha': '6f908de99b8f486996d77f0dfab6ba0577ea93ba', 'author': {'email': 'yikunkero@gmail.com', 'name': 'Yikun Jiang'}, 'message': ""[SPARK-37372][K8S] Removing redundant label addition\n\n### What changes were proposed in this pull request?\nRemove redundant Pod label addtions in driver and executor.\n\n### Why are the changes needed?\nThese labels are already included by conf.labels as preset labels, we don't need do a extra addition.\n\n### Does this PR introduce _any_ user-facing change?\nNO\n\n### How was this patch tested?\nUT passed:\nEspecially:\nhttps://github.com/apache/spark/blob/a3886ba976469bef0dfafc3da8686a53c5a59d95/resource-managers/kubernetes/core/src/test/scala/org/apache/spark/deploy/k8s/KubernetesConfSuite.scala#L157-L164\n\nCloses #34646 from Yikun/SPARK-labels-improve.\n\nAuthored-by: Yikun Jiang <yikunkero@gmail.com>\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>"", 'distinct': False, 'url': 'https://api.github.com/repos/warrenzhu25/spark/commits/6f908de99b8f486996d77f0dfab6ba0577ea93ba'}, {'sha': 'b12fc70ef9e9b661e1fc029ba5e66635afe0dd99', 'author': {'email': 'mszymkiewicz@gmail.com', 'name': 'zero323'}, 'message': '[SPARK-37429][PYTHON][MLLIB] Inline annotations for pyspark.mllib.linalg.__init__.py\n\n### What changes were proposed in this pull request?\n\nInline annotations for `pyspark.mllib.linalg.__init__.py`\n\n### Why are the changes needed?\n\nCurrently, there is type hint stub files  `pyspark.mllib.linalg.__init__.pyi` to show the expected types for functions, but we can also take advantage of static type checking within the functions by inlining the type hints.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo.\n\n### How was this patch tested?\n\nExisting tests.\n\nCloses #35020 from zero323/SPARK-37429.\n\nAuthored-by: zero323 <mszymkiewicz@gmail.com>\nSigned-off-by: zero323 <mszymkiewicz@gmail.com>', 'distinct': False, 'url': 'https://api.github.com/repos/warrenzhu25/spark/commits/b12fc70ef9e9b661e1fc029ba5e66635afe0dd99'}, {'sha': '034850866c957604f908b5a9952b2a1f18a03a96', 'author': {'email': 'mszymkiewicz@gmail.com', 'name': 'zero323'}, 'message': '[SPARK-37902][PYTHON] Resolve typing issues detected by mypy==0.931\n\n### What changes were proposed in this pull request?\n\nThis PR resolves the following typing issue detected by `mypy==0.931`:\n\n```\npython/pyspark/pandas/base.py:879: error: ""Sequence[Any]"" has no attribute ""tolist""  [attr-defined]\npython/pyspark/sql/tests/test_pandas_udf_typehints_with_future_annotations.py:268: error: Incompatible return value type (got ""floating[Any]"", expected ""float"")  [return-value]\npython/pyspark/sql/tests/test_pandas_udf_typehints.py:265: error: Incompatible return value type (got ""floating[Any]"", expected ""float"")  [return-value]\n```\n\n### Why are the changes needed?\n\nTo enable smooth migration to newer mypy versions.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo.\n\n### How was this patch tested?\n\nExisting unit tests and `dev/lint-python`.\n\nCloses #35199 from zero323/SPARK-37902.\n\nAuthored-by: zero323 <mszymkiewicz@gmail.com>\nSigned-off-by: zero323 <mszymkiewicz@gmail.com>', 'distinct': False, 'url': 'https://api.github.com/repos/warrenzhu25/spark/commits/034850866c957604f908b5a9952b2a1f18a03a96'}, {'sha': '747dcd6fa2dd0dc0d4eef18aad4871bbbf54a50a', 'author': {'email': 'mszymkiewicz@gmail.com', 'name': 'zero323'}, 'message': '[SPARK-37909] Replace global F403 exclude with file-specific rules\n\n### What changes were proposed in this pull request?\n\nThis PR rolls back global exclude on F403 introduced in SPARK-37909.\n\nInstead, we can use file-specific on major offenders (test files).\n\n### Why are the changes needed?\n\nBased on [CI behavior](https://github.com/apache/spark/pull/35199#discussion_r784526473) and local test runs, it seems like it takes precedence over F401 and silences unused import errors.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo.\n\n### How was this patch tested?\n\nExisting tests.\n\nCloses #35210 from zero323/SPARK-37909.\n\nAuthored-by: zero323 <mszymkiewicz@gmail.com>\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>', 'distinct': False, 'url': 'https://api.github.com/repos/warrenzhu25/spark/commits/747dcd6fa2dd0dc0d4eef18aad4871bbbf54a50a'}, {'sha': '29498656656779c5689e73dee11e73ac7190b205', 'author': {'email': 'yikunkero@gmail.com', 'name': 'Yikun Jiang'}, 'message': '[SPARK-37886][PYTHON][TESTS] Use ComparisonTestBase in pandas test\n\n### What changes were proposed in this pull request?\nUse `ComparisonTestBase` as base class instead of `PandasOnSparkTestCase` with self.psdf in pandas test\n\nhttps://github.com/apache/spark/blob/a70006d9a7b578721d152d0f89d1a894de38c25d/python/pyspark/testing/pandasutils.py#L265-L272\n\n### Why are the changes needed?\nWe have many testcase are using same logic to covert `pdf` to `psdf`, we can use ComparisonTestBase as base class to reduce redundant.\n\n### Does this PR introduce _any_ user-facing change?\nNO, test only\n\n### How was this patch tested?\nUT passed.\n\nCloses #35183 from Yikun/SPARK-37886.\n\nAuthored-by: Yikun Jiang <yikunkero@gmail.com>\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>', 'distinct': False, 'url': 'https://api.github.com/repos/warrenzhu25/spark/commits/29498656656779c5689e73dee11e73ac7190b205'}, {'sha': 'c7c51bcab5cb067d36bccf789e0e4ad7f37ffb7c', 'author': {'email': 'yangjie01@baidu.com', 'name': 'yangjie01'}, 'message': '[SPARK-37854][CORE] Replace type check with pattern matching in Spark code\n\n### What changes were proposed in this pull request?\n\nThere are many method use `isInstanceOf  + asInstanceOf` for type conversion in Spark code now, the main change of this pr is replace `type check` with `pattern matching` for code simplification.\n\n### Why are the changes needed?\nCode simplification\n\n### Does this PR introduce _any_ user-facing change?\nNo\n\n### How was this patch tested?\nPass GA\n\nCloses #35154 from LuciferYang/SPARK-37854.\n\nAuthored-by: yangjie01 <yangjie01@baidu.com>\nSigned-off-by: Sean Owen <srowen@gmail.com>', 'distinct': False, 'url': 'https://api.github.com/repos/warrenzhu25/spark/commits/c7c51bcab5cb067d36bccf789e0e4ad7f37ffb7c'}, {'sha': '8ae970790814a0080713857261a3b1c2e2b01dd7', 'author': {'email': 'ulyssesyou18@gmail.com', 'name': 'ulysses-you'}, 'message': '[SPARK-37862][SQL] RecordBinaryComparator should fast skip the check of aligning with unaligned platform\n\n### What changes were proposed in this pull request?\n\n`RecordBinaryComparator` compare the entire row, so it need to check if the platform is unaligned. #35078 had given the perf number to show the benefits. So this PR aims to do the same thing that fast skip the check of aligning with unaligned platform.\n\n### Why are the changes needed?\n\nImprove the performance.\n\n### Does this PR introduce _any_ user-facing change?\n\nno\n\n### How was this patch tested?\n\nPass CI. And the perf number should be same with #35078\n\nCloses #35161 from ulysses-you/unaligned.\n\nAuthored-by: ulysses-you <ulyssesyou18@gmail.com>\nSigned-off-by: Sean Owen <srowen@gmail.com>', 'distinct': False, 'url': 'https://api.github.com/repos/warrenzhu25/spark/commits/8ae970790814a0080713857261a3b1c2e2b01dd7'}, {'sha': '7614472950cb57ffefa0a51dd1163103c5d42df6', 'author': {'email': 'yangjie01@baidu.com', 'name': 'yangjie01'}, 'message': '[SPARK-37876][CORE][SQL] Move `SpecificParquetRecordReaderBase.listDirectory` to `TestUtils`\n\n### What changes were proposed in this pull request?\n`SpecificParquetRecordReaderBase.listDirectory`  is used to return the list of files at `path` recursively and the result will skips files that are ignored normally by MapReduce.\n\nThis method is only used by tests in Spark now and the tests also includes non-parquet test scenario, such as `OrcColumnarBatchReaderSuite`.\n\nSo this pr move this method from `SpecificParquetRecordReaderBase` to `TestUtils` to make it as a test method.\n\n### Why are the changes needed?\nRefactoring: move test method to `TestUtils`.\n\n### Does this PR introduce _any_ user-facing change?\nNo\n\n### How was this patch tested?\nPass GA\n\nCloses #35177 from LuciferYang/list-directory.\n\nAuthored-by: yangjie01 <yangjie01@baidu.com>\nSigned-off-by: Sean Owen <srowen@gmail.com>', 'distinct': False, 'url': 'https://api.github.com/repos/warrenzhu25/spark/commits/7614472950cb57ffefa0a51dd1163103c5d42df6'}, {'sha': '482439ff4620be9d30b36aa32a26722be9f4a30e', 'author': {'email': 'qcsd2011@163.com', 'name': 'stczwd'}, 'message': ""[SPARK-37920][BUILD] Remove tab character and trailing space in pom.xml\n\n### Why are the changes needed?\nThere are some tabs in pom.xml, which don't seem to be standardized. This pr tries to modify this problem.\n\n### Does this PR introduce _any_ user-facing change?\nNo\n\n### How was this patch tested?\norigin tests\n\nCloses #35218 from stczwd/SPARK-37920.\n\nAuthored-by: stczwd <qcsd2011@163.com>\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>"", 'distinct': False, 'url': 'https://api.github.com/repos/warrenzhu25/spark/commits/482439ff4620be9d30b36aa32a26722be9f4a30e'}, {'sha': '4c59a830a6a235400d0184fb6ce24c9e054d3e4b', 'author': {'email': 'william@apache.org', 'name': 'William Hyun'}, 'message': ""[SPARK-37921][TESTS] Update OrcReadBenchmark to use Hive ORC reader as the basis\n\n### What changes were proposed in this pull request?\nThis PR aims to update `OrcReadBenchmark` to use Hive ORC reader as the basis for comparison.\n\n### Why are the changes needed?\nThis will improve the visibility of native ORC reader's improvement because currently the new improvements are shown as `1.0x`.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo.\n\n### How was this patch tested?\nManually review.\n\nCloses #35219 from williamhyun/benchmark.\n\nAuthored-by: William Hyun <william@apache.org>\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>"", 'distinct': False, 'url': 'https://api.github.com/repos/warrenzhu25/spark/commits/4c59a830a6a235400d0184fb6ce24c9e054d3e4b'}]}",,Scala
20907523966,DeleteEvent,warrenzhu25/spark,0.0,2022-03-24T05:02:45Z,"{'ref': 'insight', 'ref_type': 'branch', 'pusher_type': 'user'}",,Scala
20907500038,DeleteEvent,warrenzhu25/spark,0.0,2022-03-24T05:00:14Z,"{'ref': 'dr-elephant', 'ref_type': 'branch', 'pusher_type': 'user'}",,Scala
20907493803,DeleteEvent,warrenzhu25/spark,0.0,2022-03-24T04:59:34Z,"{'ref': 'mongo', 'ref_type': 'branch', 'pusher_type': 'user'}",,Scala
20907488034,DeleteEvent,warrenzhu25/spark,0.0,2022-03-24T04:58:54Z,"{'ref': 'grid', 'ref_type': 'branch', 'pusher_type': 'user'}",,Scala
20907483765,DeleteEvent,warrenzhu25/spark,0.0,2022-03-24T04:58:23Z,"{'ref': 'diagonsis', 'ref_type': 'branch', 'pusher_type': 'user'}",,Scala
20907480006,DeleteEvent,warrenzhu25/spark,0.0,2022-03-24T04:57:58Z,"{'ref': 'livy', 'ref_type': 'branch', 'pusher_type': 'user'}",,Scala
20907476290,DeleteEvent,warrenzhu25/spark,0.0,2022-03-24T04:57:32Z,"{'ref': 'la', 'ref_type': 'branch', 'pusher_type': 'user'}",,Scala
20907454897,DeleteEvent,warrenzhu25/spark,0.0,2022-03-24T04:55:10Z,"{'ref': 'hadoop-3.3', 'ref_type': 'branch', 'pusher_type': 'user'}",,Scala
20907441256,DeleteEvent,warrenzhu25/spark,0.0,2022-03-24T04:53:38Z,"{'ref': 'k8slog', 'ref_type': 'branch', 'pusher_type': 'user'}",,Scala
20907436901,DeleteEvent,warrenzhu25/spark,0.0,2022-03-24T04:53:09Z,"{'ref': 'sql-3.1', 'ref_type': 'branch', 'pusher_type': 'user'}",,Scala
20907436693,DeleteEvent,warrenzhu25/spark,0.0,2022-03-24T04:53:07Z,"{'ref': 'cve', 'ref_type': 'branch', 'pusher_type': 'user'}",,Scala
20727414422,DeleteEvent,warrenzhu25/spark,0.0,2022-03-14T04:50:52Z,"{'ref': 'hive', 'ref_type': 'branch', 'pusher_type': 'user'}",,Scala
20727413638,DeleteEvent,warrenzhu25/spark,0.0,2022-03-14T04:50:45Z,"{'ref': 'sql-3.0', 'ref_type': 'branch', 'pusher_type': 'user'}",,Scala
20727413349,DeleteEvent,warrenzhu25/spark,0.0,2022-03-14T04:50:43Z,"{'ref': 'compress', 'ref_type': 'branch', 'pusher_type': 'user'}",,Scala
20727412558,DeleteEvent,warrenzhu25/spark,0.0,2022-03-14T04:50:36Z,"{'ref': 'me/33274', 'ref_type': 'branch', 'pusher_type': 'user'}",,Scala
20727410779,CreateEvent,warrenzhu25/spark,0.0,2022-03-14T04:50:22Z,"{'ref': 'cve', 'ref_type': 'branch', 'master_branch': 'master', 'description': 'Apache Spark', 'pusher_type': 'user'}",,Scala
20727410935,CreateEvent,warrenzhu25/spark,0.0,2022-03-14T04:50:24Z,"{'ref': 'me/33274', 'ref_type': 'branch', 'master_branch': 'master', 'description': 'Apache Spark', 'pusher_type': 'user'}",,Scala
20727410675,CreateEvent,warrenzhu25/spark,0.0,2022-03-14T04:50:22Z,"{'ref': 'compress', 'ref_type': 'branch', 'master_branch': 'master', 'description': 'Apache Spark', 'pusher_type': 'user'}",,Scala
20727410548,CreateEvent,warrenzhu25/spark,0.0,2022-03-14T04:50:21Z,"{'ref': 'sql-3.0', 'ref_type': 'branch', 'master_branch': 'master', 'description': 'Apache Spark', 'pusher_type': 'user'}",,Scala
20727407279,CreateEvent,warrenzhu25/spark,0.0,2022-03-14T04:49:56Z,"{'ref': 'hive', 'ref_type': 'branch', 'master_branch': 'master', 'description': 'Apache Spark', 'pusher_type': 'user'}",,Scala
20727407174,CreateEvent,warrenzhu25/spark,0.0,2022-03-14T04:49:55Z,"{'ref': 'hadoop-3.3', 'ref_type': 'branch', 'master_branch': 'master', 'description': 'Apache Spark', 'pusher_type': 'user'}",,Scala
20727407053,CreateEvent,warrenzhu25/spark,0.0,2022-03-14T04:49:54Z,"{'ref': 'k8slog', 'ref_type': 'branch', 'master_branch': 'master', 'description': 'Apache Spark', 'pusher_type': 'user'}",,Scala
20727406943,CreateEvent,warrenzhu25/spark,0.0,2022-03-14T04:49:53Z,"{'ref': 'sql-3.1', 'ref_type': 'branch', 'master_branch': 'master', 'description': 'Apache Spark', 'pusher_type': 'user'}",,Scala
20727406758,CreateEvent,warrenzhu25/spark,0.0,2022-03-14T04:49:52Z,"{'ref': 'ss-history', 'ref_type': 'branch', 'master_branch': 'master', 'description': 'Apache Spark', 'pusher_type': 'user'}",,Scala
20727323908,DeleteEvent,warrenzhu25/spark,0.0,2022-03-14T04:39:44Z,"{'ref': 'guava', 'ref_type': 'branch', 'pusher_type': 'user'}",,Scala
20727315681,DeleteEvent,warrenzhu25/spark,0.0,2022-03-14T04:38:45Z,"{'ref': 'v2.4.4-mt', 'ref_type': 'branch', 'pusher_type': 'user'}",,Scala
20727312859,DeleteEvent,warrenzhu25/spark,0.0,2022-03-14T04:38:23Z,"{'ref': 'v3.0.1-mt', 'ref_type': 'branch', 'pusher_type': 'user'}",,Scala
20727312836,DeleteEvent,warrenzhu25/spark,0.0,2022-03-14T04:38:23Z,"{'ref': 'v3.0.1-mt-release', 'ref_type': 'branch', 'pusher_type': 'user'}",,Scala
20727287174,PushEvent,warrenzhu25/spark,0.0,2022-03-14T04:35:17Z,"{'push_id': 9341981547, 'size': 1, 'distinct_size': 1, 'ref': 'refs/heads/rpc-source', 'head': '1533e2ffb14cd5dba83768c7767ac0006a65b619', 'before': '75122f3518b8355b5f59c06954faa1834347af87', 'commits': [{'sha': '1533e2ffb14cd5dba83768c7767ac0006a65b619', 'author': {'email': 'warren.zhu25@gmail.com', 'name': 'Warren Zhu'}, 'message': 'RpcSource draft', 'distinct': True, 'url': 'https://api.github.com/repos/warrenzhu25/spark/commits/1533e2ffb14cd5dba83768c7767ac0006a65b619'}]}",,Scala
20727266527,CreateEvent,warrenzhu25/spark,0.0,2022-03-14T04:32:53Z,"{'ref': '32288', 'ref_type': 'branch', 'master_branch': 'master', 'description': 'Apache Spark', 'pusher_type': 'user'}",,Scala
20727266265,CreateEvent,warrenzhu25/spark,0.0,2022-03-14T04:32:52Z,"{'ref': 'v3.0.1-mt', 'ref_type': 'branch', 'master_branch': 'master', 'description': 'Apache Spark', 'pusher_type': 'user'}",,Scala
20727266074,PushEvent,warrenzhu25/spark,0.0,2022-03-14T04:32:51Z,"{'push_id': 9341970815, 'size': 2, 'distinct_size': 2, 'ref': 'refs/heads/insight-wip', 'head': '1d57146471378965148061039743efa3286e9757', 'before': 'f0040d3407132bb9187899668c7a7a5cce6032da', 'commits': [{'sha': 'd037f3e5c2b595ae5f6f284455260d387a06b90b', 'author': {'email': 'warren.zhu25@gmail.com', 'name': 'Warren Zhu'}, 'message': 'Add failure summary page', 'distinct': True, 'url': 'https://api.github.com/repos/warrenzhu25/spark/commits/d037f3e5c2b595ae5f6f284455260d387a06b90b'}, {'sha': '1d57146471378965148061039743efa3286e9757', 'author': {'email': 'warren.zhu25@gmail.com', 'name': 'Warren Zhu'}, 'message': 'Add listing store for insights tab', 'distinct': True, 'url': 'https://api.github.com/repos/warrenzhu25/spark/commits/1d57146471378965148061039743efa3286e9757'}]}",,Scala
20727265998,CreateEvent,warrenzhu25/spark,0.0,2022-03-14T04:32:50Z,"{'ref': 'v3.0.1-mt-release', 'ref_type': 'branch', 'master_branch': 'master', 'description': 'Apache Spark', 'pusher_type': 'user'}",,Scala
20727265667,CreateEvent,warrenzhu25/spark,0.0,2022-03-14T04:32:48Z,"{'ref': 'livy', 'ref_type': 'branch', 'master_branch': 'master', 'description': 'Apache Spark', 'pusher_type': 'user'}",,Scala
20727265452,CreateEvent,warrenzhu25/spark,0.0,2022-03-14T04:32:47Z,"{'ref': 'grid', 'ref_type': 'branch', 'master_branch': 'master', 'description': 'Apache Spark', 'pusher_type': 'user'}",,Scala
20727264920,CreateEvent,warrenzhu25/spark,0.0,2022-03-14T04:32:43Z,"{'ref': 'v2.4.4-mt', 'ref_type': 'branch', 'master_branch': 'master', 'description': 'Apache Spark', 'pusher_type': 'user'}",,Scala
20727265084,CreateEvent,warrenzhu25/spark,0.0,2022-03-14T04:32:44Z,"{'ref': 'rpc-source', 'ref_type': 'branch', 'master_branch': 'master', 'description': 'Apache Spark', 'pusher_type': 'user'}",,Scala
20727264482,PushEvent,warrenzhu25/spark,0.0,2022-03-14T04:32:40Z,"{'push_id': 9341969951, 'size': 1000, 'distinct_size': 0, 'ref': 'refs/heads/master', 'head': 'f7dd37c40fa1ef5ed813b044080c07a19589e3d1', 'before': '560fe1f54cb0ac8abbb070eae3510f138d434746', 'commits': [{'sha': 'ff8cc4b800b82af3c95b113ab9aed32fab4b7c9f', 'author': {'email': 'dongjoon@apache.org', 'name': 'Dongjoon Hyun'}, 'message': '[SPARK-36629][BUILD] Upgrade `aircompressor` to 1.21\n\n### What changes were proposed in this pull request?\n\nThis PR aims to upgrade `aircompressor` dependency from 1.19 to 1.21.\n\n### Why are the changes needed?\n\nThis will bring the latest bug fix which exists in `aircompressor` 1.17 ~ 1.20.\n- https://github.com/airlift/aircompressor/commit/1e364f713390008eada1daa451e7b42cd6647250\n\n### Does this PR introduce _any_ user-facing change?\n\nNo.\n\n### How was this patch tested?\n\nPass the CIs.\n\nCloses #33883 from dongjoon-hyun/SPARK-36629.\n\nAuthored-by: Dongjoon Hyun <dongjoon@apache.org>\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>', 'distinct': False, 'url': 'https://api.github.com/repos/warrenzhu25/spark/commits/ff8cc4b800b82af3c95b113ab9aed32fab4b7c9f'}, {'sha': 'e33cdfb317498b04e077c4d6356fc3848cd78da0', 'author': {'email': 'bo.zhang@databricks.com', 'name': 'Bo Zhang'}, 'message': '[SPARK-36533][SS] Trigger.AvailableNow for running streaming queries like Trigger.Once in multiple batches\n\n### What changes were proposed in this pull request?\n\nThis change creates a new type of Trigger: Trigger.AvailableNow for streaming queries. It is like Trigger.Once, which process all available data then stop the query, but with better scalability since data can be processed in multiple batches instead of one.\n\nTo achieve this, this change proposes a new interface `SupportsTriggerAvailableNow`, which is an extension of `SupportsAdmissionControl`. It has one method, `prepareForTriggerAvailableNow`, which will be called at the beginning of streaming queries with Trigger.AvailableNow, to let the source record the offset for the current latest data at the time (a.k.a. the target offset for the query). The source should then behave as if there is no new data coming in after the beginning of the query, i.e., the source will not return an offset higher than the target offset when `latestOffset` is called.\n\nThis change also updates `FileStreamSource` to be an implementation of `SupportsTriggerAvailableNow`.\n\nFor other sources that does not implement `SupportsTriggerAvailableNow`, this change also provides a new class `FakeLatestOffsetSupportsTriggerAvailableNow`, which wraps the sources and makes them support Trigger.AvailableNow, by overriding their `latestOffset` method to always return the latest offset at the beginning of the query.\n\n### Why are the changes needed?\n\nCurrently streaming queries with Trigger.Once will always load all of the available data in a single batch. Because of this, the amount of data a query can process is limited, or Spark driver will run out of memory.\n\n### Does this PR introduce _any_ user-facing change?\n\nUsers will be able to use Trigger.AvailableNow (to process all available data then stop the streaming query) with this change.\n\n### How was this patch tested?\n\nAdded unit tests.\n\nCloses #33763 from bozhang2820/new-trigger.\n\nAuthored-by: Bo Zhang <bo.zhang@databricks.com>\nSigned-off-by: Jungtaek Lim <kabhwan.opensource@gmail.com>', 'distinct': False, 'url': 'https://api.github.com/repos/warrenzhu25/spark/commits/e33cdfb317498b04e077c4d6356fc3848cd78da0'}, {'sha': '799a0116a87359027c45815dc24a5da3fc4ec1ff', 'author': {'email': 'ktanimura@apple.com', 'name': 'Kazuyuki Tanimura'}, 'message': '[SPARK-36607][SQL] Support BooleanType in UnwrapCastInBinaryComparison\n\n### What changes were proposed in this pull request?\nThis PR proposes to add `BooleanType` support to the `UnwrapCastInBinaryComparison` optimizer that is currently supports `NumericType` only.\n\nThe main idea is to treat `BooleanType` as 1 bit integer so that we can utilize all optimizations already defined in `UnwrapCastInBinaryComparison`.\n\nThis work is an extension of SPARK-24994 and SPARK-32858\n\n### Why are the changes needed?\nCurrent implementation of Spark without this PR cannot properly optimize the filter for the following case\n```\nSELECT * FROM t WHERE boolean_field = 2\n```\nThe above query creates a filter of `cast(boolean_field, int) = 2`. The casting prevents from pushing down the filter. In contrast, this PR creates a `false` filter and returns early as there cannot be such a matching rows anyway (empty results.)\n\n### Does this PR introduce _any_ user-facing change?\nNo\n\n### How was this patch tested?\nPassed existing tests\n```\nbuild/sbt ""catalyst/test""\nbuild/sbt ""sql/test""\n```\nAdded unit tests\n```\nbuild/sbt ""catalyst/testOnly *UnwrapCastInBinaryComparisonSuite   -- -z SPARK-36607""\nbuild/sbt ""sql/testOnly *UnwrapCastInComparisonEndToEndSuite  -- -z SPARK-36607""\n```\n\nCloses #33865 from kazuyukitanimura/SPARK-36607.\n\nAuthored-by: Kazuyuki Tanimura <ktanimura@apple.com>\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>', 'distinct': False, 'url': 'https://api.github.com/repos/warrenzhu25/spark/commits/799a0116a87359027c45815dc24a5da3fc4ec1ff'}, {'sha': '77fdf5f0e43c37b8e1697e76eb2cb2623e4ba133', 'author': {'email': 'yodal@oss.nttdata.com', 'name': 'Leona Yoda'}, 'message': '[SPARK-36621][PYTHON][DOCS] Add Apache license headers to Pandas API on Spark documents\n\n### What changes were proposed in this pull request?\n\n Apache license headers to Pandas API on Spark documents.\n\n### Why are the changes needed?\n\nPandas API on Spark document sources do not have license headers, while the other docs have.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo\n\n### How was this patch tested?\n\n`make html`\n\nCloses #33871 from yoda-mon/add-license-header.\n\nAuthored-by: Leona Yoda <yodal@oss.nttdata.com>\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>', 'distinct': False, 'url': 'https://api.github.com/repos/warrenzhu25/spark/commits/77fdf5f0e43c37b8e1697e76eb2cb2623e4ba133'}, {'sha': 'e983ba8fce2b41f0c398fa279f376090376ab1f4', 'author': {'email': 'gurwls223@apache.org', 'name': 'Hyukjin Kwon'}, 'message': '[SPARK-36631][R] Ask users if they want to download and install SparkR in non Spark scripts\n\n### What changes were proposed in this pull request?\n\nThis PR proposes to ask users if they want to download and install SparkR when they install SparkR from CRAN.\n\n`SPARKR_ASK_INSTALLATION` environment variable was added in case other notebook projects are affected.\n\n### Why are the changes needed?\n\nThis is required for CRAN. Currently SparkR is removed: https://cran.r-project.org/web/packages/SparkR/index.html.\nSee also https://lists.apache.org/thread.html/r02b9046273a518e347dfe85f864d23d63d3502c6c1edd33df17a3b86%40%3Cdev.spark.apache.org%3E\n\n### Does this PR introduce _any_ user-facing change?\n\nYes, `sparkR.session(...)` will ask if users want to download and install Spark package or not if they are in the plain R shell or `Rscript`.\n\n### How was this patch tested?\n\n**R shell**\n\nValid input (`n`):\n\n```\n> sparkR.session(master=""local"")\nSpark not found in SPARK_HOME:\nWill you download and install (or reuse if it exists) Spark package under the cache [/.../Caches/spark]? (y/n): n\n```\n```\nError in sparkCheckInstall(sparkHome, master, deployMode) :\n  Please make sure Spark package is installed in this machine.\n- If there is one, set the path in sparkHome parameter or environment variable SPARK_HOME.\n- If not, you may run install.spark function to do the job.\n```\n\nInvalid input:\n\n```\n> sparkR.session(master=""local"")\nSpark not found in SPARK_HOME:\nWill you download and install (or reuse if it exists) Spark package under the cache [/.../Caches/spark]? (y/n): abc\n```\n```\nWill you download and install (or reuse if it exists) Spark package under the cache [/.../Caches/spark]? (y/n):\n```\n\nValid input (`y`):\n\n```\n> sparkR.session(master=""local"")\nWill you download and install (or reuse if it exists) Spark package under the cache [/.../Caches/spark]? (y/n): y\nSpark not found in the cache directory. Installation will start.\nMirrorUrl not provided.\nLooking for preferred site from apache website...\nPreferred mirror site found: https://ftp.riken.jp/net/apache/spark\nDownloading spark-3.3.0 for Hadoop 2.7 from:\n- https://ftp.riken.jp/net/apache/spark/spark-3.3.0/spark-3.3.0-bin-hadoop2.7.tgz\ntrying URL \'https://ftp.riken.jp/net/apache/spark/spark-3.3.0/spark-3.3.0-bin-hadoop2.7.tgz\'\n...\n```\n\n**Rscript**\n\n```\ncat tmp.R\n```\n```\nlibrary(SparkR, lib.loc = c(file.path(""."", ""R"", ""lib"")))\nsparkR.session(master=""local"")\n```\n\n```\nRscript tmp.R\n```\n\nValid input (`n`):\n\n```\nSpark not found in SPARK_HOME:\nWill you download and install (or reuse if it exists) Spark package under the cache [/.../Caches/spark]? (y/n): n\n```\n```\nError in sparkCheckInstall(sparkHome, master, deployMode) :\n  Please make sure Spark package is installed in this machine.\n- If there is one, set the path in sparkHome parameter or environment variable SPARK_HOME.\n- If not, you may run install.spark function to do the job.\nCalls: sparkR.session -> sparkCheckInstall\n```\n\nInvalid input:\n\n```\nSpark not found in SPARK_HOME:\nWill you download and install (or reuse if it exists) Spark package under the cache [/.../Caches/spark]? (y/n): abc\n```\n```\nWill you download and install (or reuse if it exists) Spark package under the cache [/.../Caches/spark]? (y/n):\n```\n\nValid input (`y`):\n\n```\n...\nSpark not found in SPARK_HOME:\nWill you download and install (or reuse if it exists) Spark package under the cache [/.../Caches/spark]? (y/n): y\nSpark not found in the cache directory. Installation will start.\nMirrorUrl not provided.\nLooking for preferred site from apache website...\nPreferred mirror site found: https://ftp.riken.jp/net/apache/spark\nDownloading spark-3.3.0 for Hadoop 2.7 from:\n...\n```\n\n`bin/sparkR` and `bin/spark-submit *.R` are not affected (tested).\n\nCloses #33887 from HyukjinKwon/SPARK-36631.\n\nAuthored-by: Hyukjin Kwon <gurwls223@apache.org>\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>', 'distinct': False, 'url': 'https://api.github.com/repos/warrenzhu25/spark/commits/e983ba8fce2b41f0c398fa279f376090376ab1f4'}, {'sha': '9c5bcac61ee56fbb271e890cc33f9a983612c5b0', 'author': {'email': 'gurwls223@apache.org', 'name': 'Hyukjin Kwon'}, 'message': ""[SPARK-36626][PYTHON] Support TimestampNTZ in createDataFrame/toPandas and Python UDFs\n\n### What changes were proposed in this pull request?\n\nThis PR proposes to implement `TimestampNTZType` support in PySpark's `SparkSession.createDataFrame`, `DataFrame.toPandas`, Python UDFs, and pandas UDFs with and without Arrow.\n\n### Why are the changes needed?\n\nTo complete `TimestampNTZType` support.\n\n### Does this PR introduce _any_ user-facing change?\n\nYes.\n\n- Users now can use `TimestampNTZType` type in `SparkSession.createDataFrame`, `DataFrame.toPandas`, Python UDFs, and pandas UDFs with and without Arrow.\n\n- If `spark.sql.timestampType` is configured to `TIMESTAMP_NTZ`, PySpark will infer the `datetime` without timezone as `TimestampNTZType`. If it has a timezone, it will be inferred as `TimestampType` in `SparkSession.createDataFrame`.\n\n    - If `TimestampType` and `TimestampNTZType` conflict during merging inferred schema, `TimestampType` has a higher precedence.\n\n- If the type is `TimestampNTZType`, treat this internally as an unknown timezone, and compute w/ UTC (same as JVM side), and avoid localization externally.\n\n### How was this patch tested?\n\nManually tested and unittests were added.\n\nCloses #33876 from HyukjinKwon/SPARK-36626.\n\nLead-authored-by: Hyukjin Kwon <gurwls223@apache.org>\nCo-authored-by: Dominik Gehl <dog@open.ch>\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>"", 'distinct': False, 'url': 'https://api.github.com/repos/warrenzhu25/spark/commits/9c5bcac61ee56fbb271e890cc33f9a983612c5b0'}, {'sha': '94c306284ae42487417632c35b1eb32960c8f828', 'author': {'email': 'sarutak@oss.nttdata.com', 'name': 'Kousuke Saruta'}, 'message': '[SPARK-36400][TEST][FOLLOWUP] Add test for redacting sensitive information in UI by config\n\n### What changes were proposed in this pull request?\n\nThis PR adds a test for SPARK-36400 (#33743).\n\n### Why are the changes needed?\n\nSPARK-36512 (#33741) was fixed so we can add this test now.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo.\n\n### How was this patch tested?\n\nNew test.\n\nCloses #33885 from sarutak/add-reduction-test.\n\nAuthored-by: Kousuke Saruta <sarutak@oss.nttdata.com>\nSigned-off-by: Kousuke Saruta <sarutak@oss.nttdata.com>', 'distinct': False, 'url': 'https://api.github.com/repos/warrenzhu25/spark/commits/94c306284ae42487417632c35b1eb32960c8f828'}, {'sha': '37f5ab07fa2343e77ae16b6460898ecbee4b3faf', 'author': {'email': 'cary@amperity.com', 'name': 'Cary Lee'}, 'message': ""[SPARK-36617][PYTHON] Fix type hints for `approxQuantile` to support multi-column version\n\n### What changes were proposed in this pull request?\nUpdate both `DataFrame.approxQuantile` and `DataFrameStatFunctions.approxQuantile` to support overloaded definitions when multiple columns are supplied.\n\n### Why are the changes needed?\nThe current type hints don't support the multi-column signature, a form that was added in Spark 2.2 (see [the approxQuantile docs](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.approxQuantile.html).) This change was also introduced to pyspark-stubs (https://github.com/zero323/pyspark-stubs/pull/552). zero323 asked me to open a PR for the upstream change.\n\n### Does this PR introduce _any_ user-facing change?\nThis change only affects type hints - it brings the `approxQuantile` type hints up to date with the actual code.\n\n### How was this patch tested?\nRan `./dev/lint-python`.\n\nCloses #33880 from carylee/master.\n\nAuthored-by: Cary Lee <cary@amperity.com>\nSigned-off-by: zero323 <mszymkiewicz@gmail.com>"", 'distinct': False, 'url': 'https://api.github.com/repos/warrenzhu25/spark/commits/37f5ab07fa2343e77ae16b6460898ecbee4b3faf'}, {'sha': '568ad6aa4435ce76ca3b5d9966e64259ea1f9b38', 'author': {'email': 'angers.zhu@gmail.com', 'name': 'Angerszhuuuu'}, 'message': ""[SPARK-36637][SQL] Provide proper error message when use undefined window frame\n\n### What changes were proposed in this pull request?\nTwo case of using undefined window frame as below should provide proper error message\n\n1. For case using undefined window frame with window function\n```\nSELECT nth_value(employee_name, 2) OVER w second_highest_salary\nFROM basic_pays;\n```\norigin error message is\n```\nWindow function nth_value(employee_name#x, 2, false) requires an OVER clause.\n```\nIt's confused that in use use a window frame `w` but it's not defined.\nNow the error message is\n```\nWindow specification w is not defined in the WINDOW clause.\n```\n\n2. For case using undefined window frame with aggregation function\n```\nSELECT SUM(salary) OVER w sum_salary\nFROM basic_pays;\n```\norigin error message is\n```\nError in query: unresolved operator 'Aggregate [unresolvedwindowexpression(sum(salary#2), WindowSpecReference(w)) AS sum_salary#34]\n+- SubqueryAlias spark_catalog.default.basic_pays\n+- HiveTableRelation [`default`.`employees`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [name#0, dept#1, salary#2, age#3], Partition Cols: []]\n```\nIn this case, when convert GlobalAggregate, should skip UnresolvedWindowExpression\nNow the error message is\n```\nWindow specification w is not defined in the WINDOW clause.\n```\n\n### Why are the changes needed?\nProvide proper error message\n\n### Does this PR introduce _any_ user-facing change?\nYes, error messages are improved as described in desc\n\n### How was this patch tested?\nAdded UT\n\nCloses #33892 from AngersZhuuuu/SPARK-36637.\n\nAuthored-by: Angerszhuuuu <angers.zhu@gmail.com>\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>"", 'distinct': False, 'url': 'https://api.github.com/repos/warrenzhu25/spark/commits/568ad6aa4435ce76ca3b5d9966e64259ea1f9b38'}, {'sha': 'b72fa5ef1c06b128011cc72d36f7bc02450ee675', 'author': {'email': 'william@apache.org', 'name': 'William Hyun'}, 'message': ""[SPARK-36657][SQL] Update comment in 'gen-sql-config-docs.py'\n\n### What changes were proposed in this pull request?\nThis PR aims to update comments in `gen-sql-config-docs.py`.\n\n### Why are the changes needed?\nTo make it up to date according to Spark version 3.2.0 release.\n\n### Does this PR introduce _any_ user-facing change?\nNo.\n\n### How was this patch tested?\nN/A.\n\nCloses #33902 from williamhyun/fixtool.\n\nAuthored-by: William Hyun <william@apache.org>\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>"", 'distinct': False, 'url': 'https://api.github.com/repos/warrenzhu25/spark/commits/b72fa5ef1c06b128011cc72d36f7bc02450ee675'}, {'sha': '38b6fbd9b8c621dc2de447d4a3ef65ea28510e5e', 'author': {'email': 'huaxin_gao@apple.com', 'name': 'Huaxin Gao'}, 'message': ""[SPARK-36351][SQL] Refactor filter push down in file source v2\n\n### What changes were proposed in this pull request?\n\nCurrently in `V2ScanRelationPushDown`, we push the filters (partition filters + data filters) to file source, and then pass all the filters (partition filters + data filters) as post scan filters to v2 Scan, and later in `PruneFileSourcePartitions`, we separate partition filters and data filters, set them in the format of `Expression` to file source.\n\nChanges in this PR:\nWhen we push filters to file sources in `V2ScanRelationPushDown`, since we already have the information about partition column , we want to separate partition filter and data filter there.\n\nThe benefit of doing this:\n- we can handle all the filter related work for v2 file source at one place instead of two (`V2ScanRelationPushDown` and `PruneFileSourcePartitions`), so the code will be cleaner and easier to maintain.\n- we actually have to separate partition filters and data filters at `V2ScanRelationPushDown`, otherwise, there is no way to find out which filters are partition filters, and we can't push down aggregate for parquet even if we only have partition filter.\n- By separating the filters early at `V2ScanRelationPushDown`, we only needs to check data filters to find out which one needs to be converted to data source filters (e.g. Parquet predicates, ORC predicates) and pushed down to file source, right now we are checking all the filters (both partition filters and data filters)\n- Similarly, we can only pass data filters as post scan filters to v2 Scan, because partition filters are used for partition pruning only, no need to pass them as post scan filters.\n\nIn order to do this, we will have the following changes\n\n-  add `pushFilters` in file source v2. In this method:\n    - push both Expression partition filter and Expression data filter to file source. Have to use Expression filters because we need these for partition pruning.\n    - data filters are used for filter push down. If file source needs to push down data filters, it translates the data filters from `Expression` to `Sources.Filer`, and then decides which filters to push down.\n    - partition filters are used for partition pruning.\n- file source v2 no need to implement `SupportsPushdownFilters` any more, because when we separating the two types of filters, we have already set them on file data sources. It's redundant to use `SupportsPushdownFilters` to set the filters again on file data sources.\n\n### Why are the changes needed?\n\nsee section one\n\n### Does this PR introduce _any_ user-facing change?\nNo\n\n### How was this patch tested?\nExisting tests\n\nCloses #33650 from huaxingao/partition_filter.\n\nAuthored-by: Huaxin Gao <huaxin_gao@apple.com>\nSigned-off-by: Liang-Chi Hsieh <viirya@gmail.com>"", 'distinct': False, 'url': 'https://api.github.com/repos/warrenzhu25/spark/commits/38b6fbd9b8c621dc2de447d4a3ef65ea28510e5e'}, {'sha': '4d9e577694f5232d808adf8b1ca35681216bd3d4', 'author': {'email': 'angers.zhu@gmail.com', 'name': 'Angerszhuuuu'}, 'message': ""[SPARK-36650][YARN] ApplicationMaster shutdown hook should catch timeout exception\n\n### What changes were proposed in this pull request?\nMeet a case in yarn-cluster mode, after stop SparkContext call ApplicationMaster's Shutdown hook.\nThrow timeout exception  then cause program throw exit code 1. But actually job success.\n```\n21/09/02 12:36:55 WARN ShutdownHookManager: ShutdownHook '$anon$2' timeout, java.util.concurrent.TimeoutException\njava.util.concurrent.TimeoutException\n\tat java.util.concurrent.FutureTask.get(FutureTask.java:205)\n\tat org.apache.hadoop.util.ShutdownHookManager.executeShutdown(ShutdownHookManager.java:124)\n\tat org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:95)\n21/09/02 12:36:55 ERROR Utils: Uncaught exception in thread shutdown-hook-0\njava.io.InterruptedIOException: Call interrupted\n\tat org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1569)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1521)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1418)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:251)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:130)\n\tat com.sun.proxy.$Proxy21.finishApplicationMaster(Unknown Source)\n\tat org.apache.hadoop.yarn.api.impl.pb.client.ApplicationMasterProtocolPBClientImpl.finishApplicationMaster(ApplicationMasterProtocolPBClientImpl.java:92)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)\n\tat com.sun.proxy.$Proxy22.finishApplicationMaster(Unknown Source)\n\tat org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl.unregisterApplicationMaster(AMRMClientImpl.java:479)\n\tat org.apache.spark.deploy.yarn.YarnRMClient.unregister(YarnRMClient.scala:90)\n\tat org.apache.spark.deploy.yarn.ApplicationMaster.unregister(ApplicationMaster.scala:384)\n\tat org.apache.spark.deploy.yarn.ApplicationMaster$$anonfun$org$apache$spark$deploy$yarn$ApplicationMaster$$runImpl$1.apply$mcV$sp(ApplicationMaster.scala:313)\n\tat org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:216)\n\tat org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:188)\n\tat org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)\n\tat org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)\n\tat org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:188)\n\tat org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)\n\tat org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)\n\tat scala.util.Try$.apply(Try.scala:192)\n\tat org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)\n\tat org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\n```\n\n### Why are the changes needed?\nReturn right exit code\n\n### Does this PR introduce _any_ user-facing change?\nNo\n\n### How was this patch tested?\nNot need\n\nCloses #33897 from AngersZhuuuu/SPARK-36650.\n\nAuthored-by: Angerszhuuuu <angers.zhu@gmail.com>\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>"", 'distinct': False, 'url': 'https://api.github.com/repos/warrenzhu25/spark/commits/4d9e577694f5232d808adf8b1ca35681216bd3d4'}, {'sha': '9054a6ac00b4fc094140c5b62ab38eb2e0c51d93', 'author': {'email': 'chengsu@fb.com', 'name': 'Cheng Su'}, 'message': '[SPARK-36652][SQL] AQE dynamic join selection should not apply to non-equi join\n\n### What changes were proposed in this pull request?\n\nCurrently `DynamicJoinSelection` has two features: 1.demote broadcast hash join, and 2.promote shuffled hash join. Both are achieved by adding join hint in query plan, and only works for equi join. However [the rule is matching with `Join` operator now](https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/DynamicJoinSelection.scala#L71), so it would add hint for non-equi join by mistake (See added test query in `JoinHintSuite.scala` for an example).\n\nThis PR is to fix `DynamicJoinSelection` to only apply to equi-join, and improve `checkHintNonEquiJoin` to check we should not add `PREFER_SHUFFLE_HASH` for non-equi join.\n\n### Why are the changes needed?\n\nImprove the logic of codebase to be better.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo.\n\n### How was this patch tested?\n\nAdded unit test in `JoinHintSuite.scala`.\n\nCloses #33899 from c21/aqe-test.\n\nAuthored-by: Cheng Su <chengsu@fb.com>\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>', 'distinct': False, 'url': 'https://api.github.com/repos/warrenzhu25/spark/commits/9054a6ac00b4fc094140c5b62ab38eb2e0c51d93'}, {'sha': 'd3e3df17aac577e163cab7e085624f94f07c748e', 'author': {'email': 'ktanimura@apple.com', 'name': 'Kazuyuki Tanimura'}, 'message': '[SPARK-36644][SQL] Push down boolean column filter\n\n### What changes were proposed in this pull request?\nThis PR proposes to improve `DataSourceStrategy` to be able to push down boolean column filters. Currently boolean column filters do not get pushed down and may cause unnecessary IO.\n\n### Why are the changes needed?\nThe following query does not push down the filter in the current implementation\n```\nSELECT * FROM t WHERE boolean_field\n```\nalthough the following query pushes down the filter as expected.\n```\nSELECT * FROM t WHERE boolean_field = true\n```\nThis is because the Physical Planner (`DataSourceStrategy`) currently only pushes down limited expression patterns like`EqualTo`.\nIt is fair for Spark SQL users to expect `boolean_field` performs the same as `boolean_field = true`.\n\n### Does this PR introduce _any_ user-facing change?\nNo\n\n### How was this patch tested?\nAdded unit tests\n```\nbuild/sbt ""core/testOnly *DataSourceStrategySuite   -- -z SPARK-36644""\n```\n\nCloses #33898 from kazuyukitanimura/SPARK-36644.\n\nAuthored-by: Kazuyuki Tanimura <ktanimura@apple.com>\nSigned-off-by: DB Tsai <d_tsai@apple.com>', 'distinct': False, 'url': 'https://api.github.com/repos/warrenzhu25/spark/commits/d3e3df17aac577e163cab7e085624f94f07c748e'}, {'sha': '7f1ad7be18b07d880ba92c470e64bf7458b8366a', 'author': {'email': 'yao@apache.org', 'name': 'Kent Yao'}, 'message': ""[SPARK-36659][SQL] Promote spark.sql.execution.topKSortFallbackThreshold to a user-facing config\n\n### What changes were proposed in this pull request?\n\nPromote spark.sql.execution.topKSortFallbackThreshold to a user-facing config\n\n### Why are the changes needed?\n\nspark.sql.execution.topKSortFallbackThreshold now is an internal config hidden from users Integer.MAX_VALUE - 15 as its default. In many real-world cases, if the K is very big,  there would be performance issues.\n\nIt's better to leave this choice to users\n\n### Does this PR introduce _any_ user-facing change?\n\n spark.sql.execution.topKSortFallbackThreshold is now user-facing\n\n### How was this patch tested?\n\npassing GA\n\nCloses #33904 from yaooqinn/SPARK-36659.\n\nAuthored-by: Kent Yao <yao@apache.org>\nSigned-off-by: Kent Yao <yao@apache.org>"", 'distinct': False, 'url': 'https://api.github.com/repos/warrenzhu25/spark/commits/7f1ad7be18b07d880ba92c470e64bf7458b8366a'}, {'sha': '61b3223f475c9cb8265807fbbe51563db699f7ca', 'author': {'email': 'haejoon.lee@databricks.com', 'name': 'itholic'}, 'message': ""[SPARK-36609][PYTHON] Add `errors` argument for `ps.to_numeric`\n\n### What changes were proposed in this pull request?\n\nThis PR proposes to support `errors` argument for `ps.to_numeric` such as pandas does.\n\nNote that we don't support the `ignore` when the `arg` is pandas-on-Spark Series for now.\n\n### Why are the changes needed?\n\nWe should match the behavior to pandas' as much as possible.\n\nAlso in the [recent blog post](https://medium.com/chuck.connell.3/pandas-on-databricks-via-koalas-a-review-9876b0a92541), the author pointed out we're missing this feature.\n\nSeems like it's the kind of feature that commonly used in data science.\n\n### Does this PR introduce _any_ user-facing change?\n\nNow the `errors` argument is available for `ps.to_numeric`.\n\n### How was this patch tested?\n\nUnittests.\n\nCloses #33882 from itholic/SPARK-36609.\n\nLead-authored-by: itholic <haejoon.lee@databricks.com>\nCo-authored-by: Hyukjin Kwon <gurwls223@gmail.com>\nCo-authored-by: Haejoon Lee <44108233+itholic@users.noreply.github.com>\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>"", 'distinct': False, 'url': 'https://api.github.com/repos/warrenzhu25/spark/commits/61b3223f475c9cb8265807fbbe51563db699f7ca'}, {'sha': 'cf3bc65e69dcb0f8ba3dee89642d082265edab31', 'author': {'email': 'sarutak@oss.nttdata.com', 'name': 'Kousuke Saruta'}, 'message': ""[SPARK-36639][SQL] Fix an issue that sequence builtin function causes ArrayIndexOutOfBoundsException if the arguments are under the condition of start == stop && step < 0\n\n### What changes were proposed in this pull request?\n\nThis PR fixes an issue that `sequence` builtin function causes `ArrayIndexOutOfBoundsException` if the arguments are under the condition of `start == stop && step < 0`.\nThis is an example.\n```\nSELECT sequence(timestamp'2021-08-31', timestamp'2021-08-31', -INTERVAL 1 month);\n21/09/02 04:14:42 ERROR SparkSQLDriver: Failed in [SELECT sequence(timestamp'2021-08-31', timestamp'2021-08-31', -INTERVAL 1 month)]\njava.lang.ArrayIndexOutOfBoundsException: 1\n```\nActually, this example succeeded before SPARK-31980 (#28819) was merged.\n\n### Why are the changes needed?\n\nBug fix.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo.\n\n### How was this patch tested?\n\nNew tests.\n\nCloses #33895 from sarutak/fix-sequence-issue.\n\nAuthored-by: Kousuke Saruta <sarutak@oss.nttdata.com>\nSigned-off-by: Kousuke Saruta <sarutak@oss.nttdata.com>"", 'distinct': False, 'url': 'https://api.github.com/repos/warrenzhu25/spark/commits/cf3bc65e69dcb0f8ba3dee89642d082265edab31'}, {'sha': '9b262e722d717bebf7d2aa19af0658cf3b8e1e85', 'author': {'email': 'dgd_contributor@viettel.com.vn', 'name': 'dgd-contributor'}, 'message': '[SPARK-36401][PYTHON] Implement Series.cov\n\n### What changes were proposed in this pull request?\n\nImplement Series.cov\n\n### Why are the changes needed?\n\nThat is supported in pandas. We should support that as well.\n\n### Does this PR introduce _any_ user-facing change?\n\nYes. Series.cov can be used.\n\n```python\n>>> from pyspark.pandas.config import set_option, reset_option\n>>> set_option(""compute.ops_on_diff_frames"", True)\n>>> s1 = ps.Series([0.90010907, 0.13484424, 0.62036035])\n>>> s2 = ps.Series([0.12528585, 0.26962463, 0.51111198])\n>>> s1.cov(s2)\n-0.016857626527158744\n>>> reset_option(""compute.ops_on_diff_frames"")\n```\n\n### How was this patch tested?\n\nUnit tests\n\nCloses #33752 from dgd-contributor/SPARK-36401_Implement_Series.cov.\n\nAuthored-by: dgd-contributor <dgd_contributor@viettel.com.vn>\nSigned-off-by: Takuya UESHIN <ueshin@databricks.com>', 'distinct': False, 'url': 'https://api.github.com/repos/warrenzhu25/spark/commits/9b262e722d717bebf7d2aa19af0658cf3b8e1e85'}, {'sha': '019203caec75b96d937ce416dfabcb3dbe440768', 'author': {'email': 'xinrong.meng@databricks.com', 'name': 'Xinrong Meng'}, 'message': ""[SPARK-36655][PYTHON] Add `versionadded` for API added in Spark 3.3.0\n\n### What changes were proposed in this pull request?\n\nAdd `versionadded` for API added in Spark 3.3.0: DataFrame.combine_first.\n\n### Why are the changes needed?\nThat documents the version of Spark which added the described API.\n\n### Does this PR introduce _any_ user-facing change?\nNo user-facing behavior change. Only the document of the affected API shows when it's introduced.\n\n### How was this patch tested?\nManual test.\n\nCloses #33901 from xinrong-databricks/version.\n\nAuthored-by: Xinrong Meng <xinrong.meng@databricks.com>\nSigned-off-by: Takuya UESHIN <ueshin@databricks.com>"", 'distinct': False, 'url': 'https://api.github.com/repos/warrenzhu25/spark/commits/019203caec75b96d937ce416dfabcb3dbe440768'}, {'sha': '6bd491ecb806ad53849593728bcdb133c572f8d2', 'author': {'email': 'senthh@gmail.com', 'name': 'Senthil Kumar'}, 'message': '[SPARK-36643][SQL] Add more information in ERROR log while SparkConf is modified when spark.sql.legacy.setCommandRejectsSparkCoreConfs is set\n\n### What changes were proposed in this pull request?\n\nThis PR adds additional information to ERROR log while SparkConf is modified when spark.sql.legacy.setCommandRejectsSparkCoreConfs is set\n\n### Why are the changes needed?\n\nRight now, by default sql.legacy.setCommandRejectsSparkCoreConfs is set as true in Spark 3.* versions int order to avoid changing Spark Confs. But from the error message we get confused if we can not modify/change Spark conf in Spark 3.* or not.\n\n### Does this PR introduce any user-facing change?\n\nYes. Trivial change in the error messages is included\n\n### How was this patch tested?\n\nNew Test added - SPARK-36643: Show migration guide when attempting SparkConf\n\nCloses #33894 from senthh/1st_Sept_2021.\n\nLead-authored-by: Senthil Kumar <senthh@gmail.com>\nCo-authored-by: Dongjoon Hyun <dongjoon@apache.org>\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>', 'distinct': False, 'url': 'https://api.github.com/repos/warrenzhu25/spark/commits/6bd491ecb806ad53849593728bcdb133c572f8d2'}]}",,Scala
20256821055,IssueCommentEvent,apache/spark,0.0,2022-02-15T00:38:18Z,"{'action': 'created', 'issue': {'url': 'https://api.github.com/repos/apache/spark/issues/35501', 'repository_url': 'https://api.github.com/repos/apache/spark', 'labels_url': 'https://api.github.com/repos/apache/spark/issues/35501/labels{/name}', 'comments_url': 'https://api.github.com/repos/apache/spark/issues/35501/comments', 'events_url': 'https://api.github.com/repos/apache/spark/issues/35501/events', 'html_url': 'https://github.com/apache/spark/pull/35501', 'id': 1135105379, 'node_id': 'PR_kwDOAQXtWs4yqegu', 'number': 35501, 'title': '[SPARK-36793][K8S] Support write container stdout/stderr to file', 'user': {'login': 'warrenzhu25', 'id': 1633312, 'node_id': 'MDQ6VXNlcjE2MzMzMTI=', 'avatar_url': 'https://avatars.githubusercontent.com/u/1633312?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/warrenzhu25', 'html_url': 'https://github.com/warrenzhu25', 'followers_url': 'https://api.github.com/users/warrenzhu25/followers', 'following_url': 'https://api.github.com/users/warrenzhu25/following{/other_user}', 'gists_url': 'https://api.github.com/users/warrenzhu25/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/warrenzhu25/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/warrenzhu25/subscriptions', 'organizations_url': 'https://api.github.com/users/warrenzhu25/orgs', 'repos_url': 'https://api.github.com/users/warrenzhu25/repos', 'events_url': 'https://api.github.com/users/warrenzhu25/events{/privacy}', 'received_events_url': 'https://api.github.com/users/warrenzhu25/received_events', 'type': 'User', 'site_admin': False}, 'labels': [{'id': 1406605057, 'node_id': 'MDU6TGFiZWwxNDA2NjA1MDU3', 'url': 'https://api.github.com/repos/apache/spark/labels/KUBERNETES', 'name': 'KUBERNETES', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981471081, 'node_id': 'MDU6TGFiZWwxOTgxNDcxMDgx', 'url': 'https://api.github.com/repos/apache/spark/labels/DOCS', 'name': 'DOCS', 'color': 'ededed', 'default': False, 'description': None}], 'state': 'open', 'locked': False, 'assignee': None, 'assignees': [], 'milestone': None, 'comments': 2, 'created_at': '2022-02-13T03:39:45Z', 'updated_at': '2022-02-15T00:38:17Z', 'closed_at': None, 'author_association': 'CONTRIBUTOR', 'active_lock_reason': None, 'draft': False, 'pull_request': {'url': 'https://api.github.com/repos/apache/spark/pulls/35501', 'html_url': 'https://github.com/apache/spark/pull/35501', 'diff_url': 'https://github.com/apache/spark/pull/35501.diff', 'patch_url': 'https://github.com/apache/spark/pull/35501.patch', 'merged_at': None}, 'body': '### What changes were proposed in this pull request?\r\nSupport write container stdout/stderr to file\r\n\r\n### Why are the changes needed?\r\nIf users want to sidecar logging agent to send stdout/stderr to external log storage,  only way is to change entrypoint.sh, which might break compatibility with community version.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nYes. User can enable this feature by spark config.\r\n\r\n### How was this patch tested?\r\nAdded UT in BasicDriverFeatureStepSuite and BasicExecutorFeatureStepSuite\r\n', 'reactions': {'url': 'https://api.github.com/repos/apache/spark/issues/35501/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/apache/spark/issues/35501/timeline', 'performed_via_github_app': None}, 'comment': {'url': 'https://api.github.com/repos/apache/spark/issues/comments/1039721538', 'html_url': 'https://github.com/apache/spark/pull/35501#issuecomment-1039721538', 'issue_url': 'https://api.github.com/repos/apache/spark/issues/35501', 'id': 1039721538, 'node_id': 'IC_kwDOAQXtWs49-ORC', 'user': {'login': 'warrenzhu25', 'id': 1633312, 'node_id': 'MDQ6VXNlcjE2MzMzMTI=', 'avatar_url': 'https://avatars.githubusercontent.com/u/1633312?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/warrenzhu25', 'html_url': 'https://github.com/warrenzhu25', 'followers_url': 'https://api.github.com/users/warrenzhu25/followers', 'following_url': 'https://api.github.com/users/warrenzhu25/following{/other_user}', 'gists_url': 'https://api.github.com/users/warrenzhu25/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/warrenzhu25/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/warrenzhu25/subscriptions', 'organizations_url': 'https://api.github.com/users/warrenzhu25/orgs', 'repos_url': 'https://api.github.com/users/warrenzhu25/repos', 'events_url': 'https://api.github.com/users/warrenzhu25/events{/privacy}', 'received_events_url': 'https://api.github.com/users/warrenzhu25/received_events', 'type': 'User', 'site_admin': False}, 'created_at': '2022-02-15T00:38:17Z', 'updated_at': '2022-02-15T00:38:17Z', 'author_association': 'CONTRIBUTOR', 'body': '> Thank you for making a PR, @warrenzhu25 . However, without a rolling feature, this will kill the executor due to OutOfDisk.\r\n\r\n@dongjoon-hyun thanks for reviewing this. For log rolling feature, could we support this in the future? I can create a jira ticket to track this. This is similar issue as shuffle output, which also might run out of disk.', 'reactions': {'url': 'https://api.github.com/repos/apache/spark/issues/comments/1039721538/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'performed_via_github_app': None}}",created,Scala
20256803493,PushEvent,warrenzhu25/spark,0.0,2022-02-15T00:36:29Z,"{'push_id': 9102480593, 'size': 1, 'distinct_size': 1, 'ref': 'refs/heads/k8s-log', 'head': '725aec80fcb806d43d2c1d509a39325054df192b', 'before': '56c6f86d518f67a865758a6630a652cc0af53cc6', 'commits': [{'sha': '725aec80fcb806d43d2c1d509a39325054df192b', 'author': {'email': 'zhonzh@microsoft.com', 'name': 'Warren Zhu'}, 'message': 'resolve comment', 'distinct': True, 'url': 'https://api.github.com/repos/warrenzhu25/spark/commits/725aec80fcb806d43d2c1d509a39325054df192b'}]}",,Scala
20250458556,PushEvent,warrenzhu25/spark,0.0,2022-02-14T17:21:49Z,"{'push_id': 9099428914, 'size': 1, 'distinct_size': 1, 'ref': 'refs/heads/k8s-log', 'head': '56c6f86d518f67a865758a6630a652cc0af53cc6', 'before': '77cdb7026713f53bf8bb40bfe9a5b42a9898e3cf', 'commits': [{'sha': '56c6f86d518f67a865758a6630a652cc0af53cc6', 'author': {'email': 'warren.zhu25@gmail.com', 'name': 'Warren Zhu'}, 'message': '[SPARK-36793][K8S] Support write container stdout/stderr to file', 'distinct': True, 'url': 'https://api.github.com/repos/warrenzhu25/spark/commits/56c6f86d518f67a865758a6630a652cc0af53cc6'}]}",,Scala
20231933570,PushEvent,warrenzhu25/spark,0.0,2022-02-13T18:14:40Z,"{'push_id': 9090382977, 'size': 1, 'distinct_size': 1, 'ref': 'refs/heads/33274', 'head': 'b9aa86e41656cb1db605c69758a048bca7386052', 'before': '2a3d5b6c96d199f958ad0a38b9e808d23dcc7a3e', 'commits': [{'sha': 'b9aa86e41656cb1db605c69758a048bca7386052', 'author': {'email': 'Zhongwei.Zhu@microsoft.com', 'name': 'Warren Zhu'}, 'message': '[SPARK-33274][SS] Fix job hang in cp mode when total cores less than total kafka partition', 'distinct': True, 'url': 'https://api.github.com/repos/warrenzhu25/spark/commits/b9aa86e41656cb1db605c69758a048bca7386052'}]}",,Scala
20223366906,PullRequestEvent,apache/spark,0.0,2022-02-13T03:39:45Z,"{'action': 'opened', 'number': 35501, 'pull_request': {'url': 'https://api.github.com/repos/apache/spark/pulls/35501', 'id': 849995822, 'node_id': 'PR_kwDOAQXtWs4yqegu', 'html_url': 'https://github.com/apache/spark/pull/35501', 'diff_url': 'https://github.com/apache/spark/pull/35501.diff', 'patch_url': 'https://github.com/apache/spark/pull/35501.patch', 'issue_url': 'https://api.github.com/repos/apache/spark/issues/35501', 'number': 35501, 'state': 'open', 'locked': False, 'title': '[SPARK-36793][K8S] Support write container stdout/stderr to file', 'user': {'login': 'warrenzhu25', 'id': 1633312, 'node_id': 'MDQ6VXNlcjE2MzMzMTI=', 'avatar_url': 'https://avatars.githubusercontent.com/u/1633312?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/warrenzhu25', 'html_url': 'https://github.com/warrenzhu25', 'followers_url': 'https://api.github.com/users/warrenzhu25/followers', 'following_url': 'https://api.github.com/users/warrenzhu25/following{/other_user}', 'gists_url': 'https://api.github.com/users/warrenzhu25/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/warrenzhu25/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/warrenzhu25/subscriptions', 'organizations_url': 'https://api.github.com/users/warrenzhu25/orgs', 'repos_url': 'https://api.github.com/users/warrenzhu25/repos', 'events_url': 'https://api.github.com/users/warrenzhu25/events{/privacy}', 'received_events_url': 'https://api.github.com/users/warrenzhu25/received_events', 'type': 'User', 'site_admin': False}, 'body': '### What changes were proposed in this pull request?\r\nSupport write container stdout/stderr to file\r\n\r\n### Why are the changes needed?\r\nIf users want to sidecar logging agent to send stdout/stderr to external log storage,  only way is to change entrypoint.sh, which might break compatibility with community version.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nYes. User can enable this feature by spark config.\r\n\r\n### How was this patch tested?\r\nAdded UT in BasicDriverFeatureStepSuite and BasicExecutorFeatureStepSuite\r\n', 'created_at': '2022-02-13T03:39:45Z', 'updated_at': '2022-02-13T03:39:45Z', 'closed_at': None, 'merged_at': None, 'merge_commit_sha': None, 'assignee': None, 'assignees': [], 'requested_reviewers': [], 'requested_teams': [], 'labels': [], 'milestone': None, 'draft': False, 'commits_url': 'https://api.github.com/repos/apache/spark/pulls/35501/commits', 'review_comments_url': 'https://api.github.com/repos/apache/spark/pulls/35501/comments', 'review_comment_url': 'https://api.github.com/repos/apache/spark/pulls/comments{/number}', 'comments_url': 'https://api.github.com/repos/apache/spark/issues/35501/comments', 'statuses_url': 'https://api.github.com/repos/apache/spark/statuses/77cdb7026713f53bf8bb40bfe9a5b42a9898e3cf', 'head': {'label': 'warrenzhu25:k8s-log', 'ref': 'k8s-log', 'sha': '77cdb7026713f53bf8bb40bfe9a5b42a9898e3cf', 'user': {'login': 'warrenzhu25', 'id': 1633312, 'node_id': 'MDQ6VXNlcjE2MzMzMTI=', 'avatar_url': 'https://avatars.githubusercontent.com/u/1633312?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/warrenzhu25', 'html_url': 'https://github.com/warrenzhu25', 'followers_url': 'https://api.github.com/users/warrenzhu25/followers', 'following_url': 'https://api.github.com/users/warrenzhu25/following{/other_user}', 'gists_url': 'https://api.github.com/users/warrenzhu25/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/warrenzhu25/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/warrenzhu25/subscriptions', 'organizations_url': 'https://api.github.com/users/warrenzhu25/orgs', 'repos_url': 'https://api.github.com/users/warrenzhu25/repos', 'events_url': 'https://api.github.com/users/warrenzhu25/events{/privacy}', 'received_events_url': 'https://api.github.com/users/warrenzhu25/received_events', 'type': 'User', 'site_admin': False}, 'repo': {'id': 217627454, 'node_id': 'MDEwOlJlcG9zaXRvcnkyMTc2Mjc0NTQ=', 'name': 'spark', 'full_name': 'warrenzhu25/spark', 'private': False, 'owner': {'login': 'warrenzhu25', 'id': 1633312, 'node_id': 'MDQ6VXNlcjE2MzMzMTI=', 'avatar_url': 'https://avatars.githubusercontent.com/u/1633312?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/warrenzhu25', 'html_url': 'https://github.com/warrenzhu25', 'followers_url': 'https://api.github.com/users/warrenzhu25/followers', 'following_url': 'https://api.github.com/users/warrenzhu25/following{/other_user}', 'gists_url': 'https://api.github.com/users/warrenzhu25/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/warrenzhu25/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/warrenzhu25/subscriptions', 'organizations_url': 'https://api.github.com/users/warrenzhu25/orgs', 'repos_url': 'https://api.github.com/users/warrenzhu25/repos', 'events_url': 'https://api.github.com/users/warrenzhu25/events{/privacy}', 'received_events_url': 'https://api.github.com/users/warrenzhu25/received_events', 'type': 'User', 'site_admin': False}, 'html_url': 'https://github.com/warrenzhu25/spark', 'description': 'Apache Spark', 'fork': True, 'url': 'https://api.github.com/repos/warrenzhu25/spark', 'forks_url': 'https://api.github.com/repos/warrenzhu25/spark/forks', 'keys_url': 'https://api.github.com/repos/warrenzhu25/spark/keys{/key_id}', 'collaborators_url': 'https://api.github.com/repos/warrenzhu25/spark/collaborators{/collaborator}', 'teams_url': 'https://api.github.com/repos/warrenzhu25/spark/teams', 'hooks_url': 'https://api.github.com/repos/warrenzhu25/spark/hooks', 'issue_events_url': 'https://api.github.com/repos/warrenzhu25/spark/issues/events{/number}', 'events_url': 'https://api.github.com/repos/warrenzhu25/spark/events', 'assignees_url': 'https://api.github.com/repos/warrenzhu25/spark/assignees{/user}', 'branches_url': 'https://api.github.com/repos/warrenzhu25/spark/branches{/branch}', 'tags_url': 'https://api.github.com/repos/warrenzhu25/spark/tags', 'blobs_url': 'https://api.github.com/repos/warrenzhu25/spark/git/blobs{/sha}', 'git_tags_url': 'https://api.github.com/repos/warrenzhu25/spark/git/tags{/sha}', 'git_refs_url': 'https://api.github.com/repos/warrenzhu25/spark/git/refs{/sha}', 'trees_url': 'https://api.github.com/repos/warrenzhu25/spark/git/trees{/sha}', 'statuses_url': 'https://api.github.com/repos/warrenzhu25/spark/statuses/{sha}', 'languages_url': 'https://api.github.com/repos/warrenzhu25/spark/languages', 'stargazers_url': 'https://api.github.com/repos/warrenzhu25/spark/stargazers', 'contributors_url': 'https://api.github.com/repos/warrenzhu25/spark/contributors', 'subscribers_url': 'https://api.github.com/repos/warrenzhu25/spark/subscribers', 'subscription_url': 'https://api.github.com/repos/warrenzhu25/spark/subscription', 'commits_url': 'https://api.github.com/repos/warrenzhu25/spark/commits{/sha}', 'git_commits_url': 'https://api.github.com/repos/warrenzhu25/spark/git/commits{/sha}', 'comments_url': 'https://api.github.com/repos/warrenzhu25/spark/comments{/number}', 'issue_comment_url': 'https://api.github.com/repos/warrenzhu25/spark/issues/comments{/number}', 'contents_url': 'https://api.github.com/repos/warrenzhu25/spark/contents/{+path}', 'compare_url': 'https://api.github.com/repos/warrenzhu25/spark/compare/{base}...{head}', 'merges_url': 'https://api.github.com/repos/warrenzhu25/spark/merges', 'archive_url': 'https://api.github.com/repos/warrenzhu25/spark/{archive_format}{/ref}', 'downloads_url': 'https://api.github.com/repos/warrenzhu25/spark/downloads', 'issues_url': 'https://api.github.com/repos/warrenzhu25/spark/issues{/number}', 'pulls_url': 'https://api.github.com/repos/warrenzhu25/spark/pulls{/number}', 'milestones_url': 'https://api.github.com/repos/warrenzhu25/spark/milestones{/number}', 'notifications_url': 'https://api.github.com/repos/warrenzhu25/spark/notifications{?since,all,participating}', 'labels_url': 'https://api.github.com/repos/warrenzhu25/spark/labels{/name}', 'releases_url': 'https://api.github.com/repos/warrenzhu25/spark/releases{/id}', 'deployments_url': 'https://api.github.com/repos/warrenzhu25/spark/deployments', 'created_at': '2019-10-25T23:12:23Z', 'updated_at': '2020-07-10T17:08:49Z', 'pushed_at': '2022-02-13T03:38:37Z', 'git_url': 'git://github.com/warrenzhu25/spark.git', 'ssh_url': 'git@github.com:warrenzhu25/spark.git', 'clone_url': 'https://github.com/warrenzhu25/spark.git', 'svn_url': 'https://github.com/warrenzhu25/spark', 'homepage': '', 'size': 397856, 'stargazers_count': 0, 'watchers_count': 0, 'language': 'Scala', 'has_issues': False, 'has_projects': True, 'has_downloads': True, 'has_wiki': False, 'has_pages': False, 'forks_count': 0, 'mirror_url': None, 'archived': False, 'disabled': False, 'open_issues_count': 0, 'license': {'key': 'apache-2.0', 'name': 'Apache License 2.0', 'spdx_id': 'Apache-2.0', 'url': 'https://api.github.com/licenses/apache-2.0', 'node_id': 'MDc6TGljZW5zZTI='}, 'allow_forking': True, 'is_template': False, 'topics': [], 'visibility': 'public', 'forks': 0, 'open_issues': 0, 'watchers': 0, 'default_branch': 'master'}}, 'base': {'label': 'apache:master', 'ref': 'master', 'sha': '25dd4254fed71923731fd59838875c0dd1ff665a', 'user': {'login': 'apache', 'id': 47359, 'node_id': 'MDEyOk9yZ2FuaXphdGlvbjQ3MzU5', 'avatar_url': 'https://avatars.githubusercontent.com/u/47359?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/apache', 'html_url': 'https://github.com/apache', 'followers_url': 'https://api.github.com/users/apache/followers', 'following_url': 'https://api.github.com/users/apache/following{/other_user}', 'gists_url': 'https://api.github.com/users/apache/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/apache/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/apache/subscriptions', 'organizations_url': 'https://api.github.com/users/apache/orgs', 'repos_url': 'https://api.github.com/users/apache/repos', 'events_url': 'https://api.github.com/users/apache/events{/privacy}', 'received_events_url': 'https://api.github.com/users/apache/received_events', 'type': 'Organization', 'site_admin': False}, 'repo': {'id': 17165658, 'node_id': 'MDEwOlJlcG9zaXRvcnkxNzE2NTY1OA==', 'name': 'spark', 'full_name': 'apache/spark', 'private': False, 'owner': {'login': 'apache', 'id': 47359, 'node_id': 'MDEyOk9yZ2FuaXphdGlvbjQ3MzU5', 'avatar_url': 'https://avatars.githubusercontent.com/u/47359?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/apache', 'html_url': 'https://github.com/apache', 'followers_url': 'https://api.github.com/users/apache/followers', 'following_url': 'https://api.github.com/users/apache/following{/other_user}', 'gists_url': 'https://api.github.com/users/apache/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/apache/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/apache/subscriptions', 'organizations_url': 'https://api.github.com/users/apache/orgs', 'repos_url': 'https://api.github.com/users/apache/repos', 'events_url': 'https://api.github.com/users/apache/events{/privacy}', 'received_events_url': 'https://api.github.com/users/apache/received_events', 'type': 'Organization', 'site_admin': False}, 'html_url': 'https://github.com/apache/spark', 'description': 'Apache Spark - A unified analytics engine for large-scale data processing', 'fork': False, 'url': 'https://api.github.com/repos/apache/spark', 'forks_url': 'https://api.github.com/repos/apache/spark/forks', 'keys_url': 'https://api.github.com/repos/apache/spark/keys{/key_id}', 'collaborators_url': 'https://api.github.com/repos/apache/spark/collaborators{/collaborator}', 'teams_url': 'https://api.github.com/repos/apache/spark/teams', 'hooks_url': 'https://api.github.com/repos/apache/spark/hooks', 'issue_events_url': 'https://api.github.com/repos/apache/spark/issues/events{/number}', 'events_url': 'https://api.github.com/repos/apache/spark/events', 'assignees_url': 'https://api.github.com/repos/apache/spark/assignees{/user}', 'branches_url': 'https://api.github.com/repos/apache/spark/branches{/branch}', 'tags_url': 'https://api.github.com/repos/apache/spark/tags', 'blobs_url': 'https://api.github.com/repos/apache/spark/git/blobs{/sha}', 'git_tags_url': 'https://api.github.com/repos/apache/spark/git/tags{/sha}', 'git_refs_url': 'https://api.github.com/repos/apache/spark/git/refs{/sha}', 'trees_url': 'https://api.github.com/repos/apache/spark/git/trees{/sha}', 'statuses_url': 'https://api.github.com/repos/apache/spark/statuses/{sha}', 'languages_url': 'https://api.github.com/repos/apache/spark/languages', 'stargazers_url': 'https://api.github.com/repos/apache/spark/stargazers', 'contributors_url': 'https://api.github.com/repos/apache/spark/contributors', 'subscribers_url': 'https://api.github.com/repos/apache/spark/subscribers', 'subscription_url': 'https://api.github.com/repos/apache/spark/subscription', 'commits_url': 'https://api.github.com/repos/apache/spark/commits{/sha}', 'git_commits_url': 'https://api.github.com/repos/apache/spark/git/commits{/sha}', 'comments_url': 'https://api.github.com/repos/apache/spark/comments{/number}', 'issue_comment_url': 'https://api.github.com/repos/apache/spark/issues/comments{/number}', 'contents_url': 'https://api.github.com/repos/apache/spark/contents/{+path}', 'compare_url': 'https://api.github.com/repos/apache/spark/compare/{base}...{head}', 'merges_url': 'https://api.github.com/repos/apache/spark/merges', 'archive_url': 'https://api.github.com/repos/apache/spark/{archive_format}{/ref}', 'downloads_url': 'https://api.github.com/repos/apache/spark/downloads', 'issues_url': 'https://api.github.com/repos/apache/spark/issues{/number}', 'pulls_url': 'https://api.github.com/repos/apache/spark/pulls{/number}', 'milestones_url': 'https://api.github.com/repos/apache/spark/milestones{/number}', 'notifications_url': 'https://api.github.com/repos/apache/spark/notifications{?since,all,participating}', 'labels_url': 'https://api.github.com/repos/apache/spark/labels{/name}', 'releases_url': 'https://api.github.com/repos/apache/spark/releases{/id}', 'deployments_url': 'https://api.github.com/repos/apache/spark/deployments', 'created_at': '2014-02-25T08:00:08Z', 'updated_at': '2022-02-12T19:17:52Z', 'pushed_at': '2022-02-13T03:39:45Z', 'git_url': 'git://github.com/apache/spark.git', 'ssh_url': 'git@github.com:apache/spark.git', 'clone_url': 'https://github.com/apache/spark.git', 'svn_url': 'https://github.com/apache/spark', 'homepage': 'https://spark.apache.org/', 'size': 406848, 'stargazers_count': 32048, 'watchers_count': 32048, 'language': 'Scala', 'has_issues': False, 'has_projects': True, 'has_downloads': True, 'has_wiki': False, 'has_pages': False, 'forks_count': 25214, 'mirror_url': None, 'archived': False, 'disabled': False, 'open_issues_count': 246, 'license': {'key': 'apache-2.0', 'name': 'Apache License 2.0', 'spdx_id': 'Apache-2.0', 'url': 'https://api.github.com/licenses/apache-2.0', 'node_id': 'MDc6TGljZW5zZTI='}, 'allow_forking': True, 'is_template': False, 'topics': ['big-data', 'java', 'jdbc', 'python', 'r', 'scala', 'spark', 'sql'], 'visibility': 'public', 'forks': 25214, 'open_issues': 246, 'watchers': 32048, 'default_branch': 'master'}}, '_links': {'self': {'href': 'https://api.github.com/repos/apache/spark/pulls/35501'}, 'html': {'href': 'https://github.com/apache/spark/pull/35501'}, 'issue': {'href': 'https://api.github.com/repos/apache/spark/issues/35501'}, 'comments': {'href': 'https://api.github.com/repos/apache/spark/issues/35501/comments'}, 'review_comments': {'href': 'https://api.github.com/repos/apache/spark/pulls/35501/comments'}, 'review_comment': {'href': 'https://api.github.com/repos/apache/spark/pulls/comments{/number}'}, 'commits': {'href': 'https://api.github.com/repos/apache/spark/pulls/35501/commits'}, 'statuses': {'href': 'https://api.github.com/repos/apache/spark/statuses/77cdb7026713f53bf8bb40bfe9a5b42a9898e3cf'}}, 'author_association': 'CONTRIBUTOR', 'auto_merge': None, 'active_lock_reason': None, 'merged': False, 'mergeable': None, 'rebaseable': None, 'mergeable_state': 'unknown', 'merged_by': None, 'comments': 0, 'review_comments': 0, 'maintainer_can_modify': True, 'commits': 1, 'additions': 75, 'deletions': 3, 'changed_files': 8}}",opened,Scala
20223357925,PushEvent,warrenzhu25/spark,0.0,2022-02-13T03:38:40Z,"{'push_id': 9086146024, 'size': 1, 'distinct_size': 1, 'ref': 'refs/heads/k8s-log', 'head': '77cdb7026713f53bf8bb40bfe9a5b42a9898e3cf', 'before': 'a0d994c247866d079a3d02abc801c3758b56ea5b', 'commits': [{'sha': '77cdb7026713f53bf8bb40bfe9a5b42a9898e3cf', 'author': {'email': 'warren.zhu25@gmail.com', 'name': 'Warren Zhu'}, 'message': '[SPARK-36793][K8S] Support write container stdout/stderr to file', 'distinct': True, 'url': 'https://api.github.com/repos/warrenzhu25/spark/commits/77cdb7026713f53bf8bb40bfe9a5b42a9898e3cf'}]}",,Scala
20223353474,PushEvent,warrenzhu25/spark,0.0,2022-02-13T03:38:08Z,"{'push_id': 9086143874, 'size': 23, 'distinct_size': 2, 'ref': 'refs/heads/k8s-log', 'head': 'a0d994c247866d079a3d02abc801c3758b56ea5b', 'before': '41d58298a219f068630a2bb1f56405651d81b49d', 'commits': [{'sha': '7688d839ccc78972b4590be7b30758dd5eb7fc9f', 'author': {'email': 'tengfei.h@gmail.com', 'name': 'Tengfei Huang'}, 'message': '[SPARK-38113][SQL] Use error classes in the execution errors of pivoting\n\n### What changes were proposed in this pull request?\nMigrate the following errors in QueryExecutionErrors onto use error classes:\n1. repeatedPivotsUnsupportedError => UNSUPPORTED_FEATURE\n2. pivotNotAfterGroupByUnsupportedError => UNSUPPORTED_FEATURE\n\n### Why are the changes needed?\nPorting pivot execute errors to new error framework.\n\n### Does this PR introduce _any_ user-facing change?\nNo\n\n### How was this patch tested?\nUT added.\n\nCloses #35466 from ivoson/SPARK-38113.\n\nAuthored-by: Tengfei Huang <tengfei.h@gmail.com>\nSigned-off-by: Max Gekk <max.gekk@gmail.com>', 'distinct': False, 'url': 'https://api.github.com/repos/warrenzhu25/spark/commits/7688d839ccc78972b4590be7b30758dd5eb7fc9f'}, {'sha': '53ba6e2affc0d96e45a50ba9f4bdd07b359dcd7c', 'author': {'email': 'max.gekk@gmail.com', 'name': 'Max Gekk'}, 'message': ""[SPARK-38131][SQL] Use error classes in user-facing exceptions only\n\n### What changes were proposed in this pull request?\nIn the PR, I propose to remove the error class `ROW_FROM_CSV_PARSER_NOT_EXPECTED` and don't use the error class `UNSUPPORTED_FEATURE` in an up-cast exception which is an internal one.\n\n### Why are the changes needed?\nThe error classes are supposed to be used in user-facing errors/exceptions only. It doesn't make sense to introduce/use them in internal errors.\n\n### Does this PR introduce _any_ user-facing change?\nNo.\n\n### How was this patch tested?\nBy running existing test suites.\n\nCloses #35445 from MaxGekk/keep-only-user-facing-error-classes.\n\nAuthored-by: Max Gekk <max.gekk@gmail.com>\nSigned-off-by: Max Gekk <max.gekk@gmail.com>"", 'distinct': False, 'url': 'https://api.github.com/repos/warrenzhu25/spark/commits/53ba6e2affc0d96e45a50ba9f4bdd07b359dcd7c'}, {'sha': '17653fbbb5fa98e73932b702641f50671579d431', 'author': {'email': 'mszymkiewicz@gmail.com', 'name': 'zero323'}, 'message': '[SPARK-37401][PYTHON][ML] Inline typehints for pyspark.ml.clustering\n\n### What changes were proposed in this pull request?\n\nThis PR migrates type `pyspark.ml.stat clustering` from stub file to inline type hints.\n\n### Why are the changes needed?\n\nPart of ongoing migration of type hints.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo.\n\n### How was this patch tested?\n\nExisting tests + new data test.\n\nCloses #35439 from zero323/SPARK-37401.\n\nAuthored-by: zero323 <mszymkiewicz@gmail.com>\nSigned-off-by: zero323 <mszymkiewicz@gmail.com>', 'distinct': False, 'url': 'https://api.github.com/repos/warrenzhu25/spark/commits/17653fbbb5fa98e73932b702641f50671579d431'}, {'sha': '3d285c11b611e63d6ebb0b209f52d6ec7a61debe', 'author': {'email': 'yangjie01@baidu.com', 'name': 'yangjie01'}, 'message': '[SPARK-38123][SQL] Unified use `DataType` as `targetType` of `QueryExecutionErrors#castingCauseOverflowError`\n\n### What changes were proposed in this pull request?\nSPARK-33541 introduces `QueryExecutionErrors#castingCauseOverflowError` and there are 2 ways for input parameter `targetType` in Spark code now:\n\n- Use `DataType.catalogString` as `targetType`, such as use in `Cast.scala` and `IntervalUtils.scala`\n- Use custom literal such as `short`,`int` and `long` in `Decimal.scala` and `numberics.scala`\n\nThis pr change to unified use `DataType` as the `targetType`.\n\nAnother change of this pr is to change the `targetType` from `int` to `LongType.catalogString` when `FloatExactNumeric#toLong` method throw castingCauseOverflowError, this seems to be a issue left over from history.\n\n### Why are the changes needed?\nUnified use `DataType.catalogString` as `targetType` when throwing castingCauseOverflowError and bug fix\n\n### Does this PR introduce _any_ user-facing change?\nNo\n\n### How was this patch tested?\nPass GA\n\nCloses #35412 from LuciferYang/use-catalogString.\n\nAuthored-by: yangjie01 <yangjie01@baidu.com>\nSigned-off-by: Max Gekk <max.gekk@gmail.com>', 'distinct': False, 'url': 'https://api.github.com/repos/warrenzhu25/spark/commits/3d285c11b611e63d6ebb0b209f52d6ec7a61debe'}, {'sha': 'fab8d43fac18ee91ab3369918c636ad0239475d0', 'author': {'email': 'beliefer@163.com', 'name': 'Jiaan Geng'}, 'message': '[SPARK-37960][SQL] A new framework to represent catalyst expressions in DS v2 APIs\n\n### What changes were proposed in this pull request?\nThis PR provides a new framework to represent catalyst expressions in DS v2 APIs.\n`GeneralSQLExpression` is a general SQL expression to represent catalyst expression in DS v2 API.\n`ExpressionSQLBuilder` is a builder to generate `GeneralSQLExpression` from catalyst expressions.\n`CASE ... WHEN ... ELSE ... END` is just the first use case.\n\nThis PR also supports aggregate push down with `CASE ... WHEN ... ELSE ... END`.\n\n### Why are the changes needed?\nSupport aggregate push down with `CASE ... WHEN ... ELSE ... END`.\n\n### Does this PR introduce _any_ user-facing change?\nYes. Users could use `CASE ... WHEN ... ELSE ... END` with aggregate push down.\n\n### How was this patch tested?\nNew tests.\n\nCloses #35248 from beliefer/SPARK-37960.\n\nAuthored-by: Jiaan Geng <beliefer@163.com>\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>', 'distinct': False, 'url': 'https://api.github.com/repos/warrenzhu25/spark/commits/fab8d43fac18ee91ab3369918c636ad0239475d0'}, {'sha': '50256bde9bdf217413545a6d2945d6c61bf4cfff', 'author': {'email': 'beliefer@163.com', 'name': 'Jiaan Geng'}, 'message': ""[SPARK-38054][SQL] Supports list namespaces in JDBC v2 MySQL dialect\n\n### What changes were proposed in this pull request?\nCurrently, `JDBCTableCatalog.scala` query namespaces show below.\n```\n      val schemaBuilder = ArrayBuilder.make[Array[String]]\n      val rs = conn.getMetaData.getSchemas()\n      while (rs.next()) {\n        schemaBuilder += Array(rs.getString(1))\n      }\n      schemaBuilder.result\n```\n\nBut the code cannot get any information when using MySQL JDBC driver.\nThis PR uses `SHOW SCHEMAS` to query namespaces of MySQL.\nThis PR also fix other issues below:\n\n- Release the docker tests in `MySQLNamespaceSuite.scala`.\n- Because MySQL doesn't support create comment of schema, let's throws `SQLFeatureNotSupportedException`.\n- Because MySQL doesn't support `DROP SCHEMA` in `RESTRICT` mode, let's throws `SQLFeatureNotSupportedException`.\n- Reactor `JdbcUtils.executeQuery` to avoid `java.sql.SQLException: Operation not allowed after ResultSet closed`.\n\n### Why are the changes needed?\nMySQL dialect supports query namespaces.\n\n### Does this PR introduce _any_ user-facing change?\n'Yes'.\nSome API changed.\n\n### How was this patch tested?\nNew tests.\n\nCloses #35355 from beliefer/SPARK-38054.\n\nAuthored-by: Jiaan Geng <beliefer@163.com>\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>"", 'distinct': False, 'url': 'https://api.github.com/repos/warrenzhu25/spark/commits/50256bde9bdf217413545a6d2945d6c61bf4cfff'}, {'sha': '344fd5c0b3958da881aa2775689f620aa25c1de4', 'author': {'email': 'weixiuli@jd.com', 'name': 'weixiuli'}, 'message': '[MINOR][CORE] Fix the method description of refill\n\n### What changes were proposed in this pull request?\n\nFix the method description of refill.\n\n### Why are the changes needed?\nEasy to understand.\n### Does this PR introduce _any_ user-facing change?\nNo.\n\n### How was this patch tested?\n\nExisting tests.\n\nCloses #35307 from weixiuli/SPARK-38008.\n\nAuthored-by: weixiuli <weixiuli@jd.com>\nSigned-off-by: Sean Owen <srowen@gmail.com>', 'distinct': False, 'url': 'https://api.github.com/repos/warrenzhu25/spark/commits/344fd5c0b3958da881aa2775689f620aa25c1de4'}, {'sha': '75122f3518b8355b5f59c06954faa1834347af87', 'author': {'email': 'dongjoon@apache.org', 'name': 'Dongjoon Hyun'}, 'message': '[SPARK-38171][BUILD][SQL] Upgrade ORC to 1.7.3\n\n### What changes were proposed in this pull request?\n\nThis PR aims to upgrade Apache ORC dependency to 1.7.3.\n\n### Why are the changes needed?\n\nApache ORC 1.7.3 is the 3rd maintenance release.\n- https://orc.apache.org/news/2022/02/09/ORC-1.7.3/\n- https://github.com/apache/orc/releases/tag/v1.7.3\n\n### Does this PR introduce _any_ user-facing change?\n\nNo.\n\n### How was this patch tested?\n\nPass the CIs.\n\nCloses #35474 from dongjoon-hyun/SPARK-38171.\n\nAuthored-by: Dongjoon Hyun <dongjoon@apache.org>\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>', 'distinct': False, 'url': 'https://api.github.com/repos/warrenzhu25/spark/commits/75122f3518b8355b5f59c06954faa1834347af87'}, {'sha': '0ae8c39da4917fdb07d0a019732b644406307525', 'author': {'email': 'sunchao@apple.com', 'name': 'Chao Sun'}, 'message': '[SPARK-38134][BUILD] Upgrade Apache Arrow to 7.0.0\n\n### What changes were proposed in this pull request?\n\nUpgrade Apache Arrow to 7.0.0.\n\n### Why are the changes needed?\n\nTo pick up new improvements & bug fixes from the latest release.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo.\n\n### How was this patch tested?\n\nExisting tests.\n\nCloses #35449 from sunchao/SPARK-38134.\n\nAuthored-by: Chao Sun <sunchao@apple.com>\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>', 'distinct': False, 'url': 'https://api.github.com/repos/warrenzhu25/spark/commits/0ae8c39da4917fdb07d0a019732b644406307525'}, {'sha': '93251ed77ea1c5d037c64d2292b8760b03c8e181', 'author': {'email': '7163127+lzm0@users.noreply.github.com', 'name': 'Zimo Li'}, 'message': ""[MINOR][K8S][DOCS] Fix typo in K8s conf `deleteOnTermination`\n\n### What changes were proposed in this pull request?\n\nThere is a grammatical mistake in the doc for the config `spark.kubernetes.driver.service.deleteOnTermination`\n\n### Why are the changes needed?\n\nFix typo\n\n### Does this PR introduce _any_ user-facing change?\n\nyes\npreviously:\n> If true, driver service will be deleted on Spark application termination. If false, it will be cleaned up when the driver pod is **deletion**.\n\ncorrected:\n> If true, driver service will be deleted on Spark application termination. If false, it will be cleaned up when the driver pod is **deleted**.\n\n### How was this patch tested?\n\nNo tests are needed since it's only documentation changes.\n\nCloses #35482 from lzm0/patch-1.\n\nAuthored-by: Zimo Li <7163127+lzm0@users.noreply.github.com>\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>"", 'distinct': False, 'url': 'https://api.github.com/repos/warrenzhu25/spark/commits/93251ed77ea1c5d037c64d2292b8760b03c8e181'}, {'sha': '7ed51bbbe5fd571bcbd95c78f66532eda83e2d8a', 'author': {'email': 'yaohua.zhao@databricks.com', 'name': 'yaohua'}, 'message': '[SPARK-38159][SQL] Add a new FileSourceMetadataAttribute for the Hidden File Metadata\n\n### What changes were proposed in this pull request?\nAdd a new `FileSourceMetadataAttribute` object with an `apply` method to create a `FileSourceMetadataAttribute`, and an `unapply` method to match only file source metadata attribute.\n\n### Why are the changes needed?\nExtra safeguard to make sure it matches file source metadata attribute.\n\n### Does this PR introduce _any_ user-facing change?\nNo\n\n### How was this patch tested?\nExisting UTs\n\nCloses #35459 from Yaohua628/spark-38159.\n\nAuthored-by: yaohua <yaohua.zhao@databricks.com>\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>', 'distinct': False, 'url': 'https://api.github.com/repos/warrenzhu25/spark/commits/7ed51bbbe5fd571bcbd95c78f66532eda83e2d8a'}, {'sha': '23cd3190f7453010a0311b8639158e8b586997e2', 'author': {'email': 'dongjoon@apache.org', 'name': 'Dongjoon Hyun'}, 'message': '[SPARK-38125][BUILD][SQL] Use static factory methods instead of the deprecated `Byte/Short/Integer/Long` constructors\n\n### What changes were proposed in this pull request?\n\nThis PR aims to use static factor methods instead of the deprecated `Integer` constructors and add Java/Scala linter rules to enforce new styles.\n\n### Why are the changes needed?\n\n`Byte/Short/Integer/Long` constructors are deprecated in Java 9.\n- https://docs.oracle.com/javase/9/docs/api/java/lang/Byte.html#Byte-byte-\n- https://docs.oracle.com/javase/9/docs/api/java/lang/Byte.html#Byte-java.lang.String-\n- https://docs.oracle.com/javase/9/docs/api/java/lang/Short.html#Short-short-\n- https://docs.oracle.com/javase/9/docs/api/java/lang/Short.html#Short-java.lang.String-\n- https://docs.oracle.com/javase/9/docs/api/java/lang/Integer.html#Integer-int-\n- https://docs.oracle.com/javase/9/docs/api/java/lang/Integer.html#Integer-java.lang.String-\n- https://docs.oracle.com/javase/9/docs/api/java/lang/Long.html#Long-long-\n- https://docs.oracle.com/javase/9/docs/api/java/lang/Long.html#Long-java.lang.String-\n\n### Does this PR introduce _any_ user-facing change?\n\nNo.\n\n### How was this patch tested?\n\nPass the CIs with the newly added Scalastyle and Java Checkstyle.\n\nCloses #35414 from dongjoon-hyun/SPARK-38125.\n\nAuthored-by: Dongjoon Hyun <dongjoon@apache.org>\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>', 'distinct': False, 'url': 'https://api.github.com/repos/warrenzhu25/spark/commits/23cd3190f7453010a0311b8639158e8b586997e2'}, {'sha': '93a9464b6814b36dd9ff056800143bc700976abf', 'author': {'email': 'myasuka@live.com', 'name': 'Yun Tang'}, 'message': '[SPARK-38178][SS] Correct the logic to measure the memory usage of RocksDB\n\n### What changes were proposed in this pull request?\n\nCorrect the logic to measure the memory usage of RocksDB to include the memory used by block cache.\nAs ""block-cache-pinned-usage"" is included in ""block-cache-usage"", we don\'t need to sum the pinned usage separately.\n\n### Why are the changes needed?\n\nCurrent reported metrics of RocksDB memory usage is not correct.\n\n### Does this PR introduce _any_ user-facing change?\n\nNO\n\n### How was this patch tested?\n\nThe information could refer to https://github.com/facebook/rocksdb/wiki/Memory-usage-in-RocksDB\n\nCloses #35480 from Myasuka/rocksdb-mem-usage.\n\nAuthored-by: Yun Tang <myasuka@live.com>\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>', 'distinct': False, 'url': 'https://api.github.com/repos/warrenzhu25/spark/commits/93a9464b6814b36dd9ff056800143bc700976abf'}, {'sha': 'a2d7a23be2004cca7346b43785f35a2088ab83ea', 'author': {'email': 'gengliang@apache.org', 'name': 'Gengliang Wang'}, 'message': ""[SPARK-38176][SQL] ANSI mode: allow implicitly casting String to other simple types\n\n### What changes were proposed in this pull request?\n\nCompared to the default behavior, the current ANSI type coercion rules don't allow the following cases:\n\n- Comparing String with other simple types, e.g. `str_col > date'2022-01-01'`\n- Arithmetic operation containing String and other simple types\n- Union/Intersect/Except containing String and other simple types\n- SQL function expects non-string types but got  string input\n- other SQL operators..\n\nThis PR is to remove the limitation. After changes, the String type can be implicit cast as Long/Double/Date/Timestamp/Boolean/Binary.\n\nNote that Byte/Short/Int is not on the precedent list of String: `str_col > 1` will become `cast(str_col as long) > 1L`. So that we can avoid string parsing error if the string is out of the range of Byte/Short/Int in comparison/arithmetic/union operations.\nThe design applies to Float/Decimal (especially Decimal), for SQL operators containing Float/Decimal and String, the type coercion system will convert both as Double.\n![image](https://user-images.githubusercontent.com/1097932/153430898-1f4eca1e-d72b-4714-831f-ff697a046f93.png)\n\n### Why are the changes needed?\n\nThe purpose of the current limitation is to prevent potential String parsing errors under ANSI mode. However, after doing research among real-world Spark SQL queries, I find that **many users are actually using String as Date/Timestamp/Numeric/etc in their queries**.  For example, the purpose of query `where str_col > date'2022-01-01'` is quite obvious, but users have to rewrite it as `where cast(str_col as date) > date'2022-01-01'` under ANSI mode.\nTo make the migration to ANSI mode easier, I suggest removing this limitation. Let's treat it as an extension in our SQL dialect.\n\n### Does this PR introduce _any_ user-facing change?\n\nYes, allow implicitly casting String to other simple types under ANSI mode\n\n### How was this patch tested?\n\nUnit tests\n\nCloses #35478 from gengliangwang/allowStringCoercion.\n\nLead-authored-by: Gengliang Wang <gengliang@apache.org>\nCo-authored-by: Gengliang Wang <ltnwgl@gmail.com>\nSigned-off-by: Gengliang Wang <gengliang@apache.org>"", 'distinct': False, 'url': 'https://api.github.com/repos/warrenzhu25/spark/commits/a2d7a23be2004cca7346b43785f35a2088ab83ea'}, {'sha': 'a62ae9f64fbc5f9f472bc01dbd97caa65927baef', 'author': {'email': 'ulyssesyou18@gmail.com', 'name': 'ulysses-you'}, 'message': '[SPARK-38177][SQL] Fix wrong transformExpressions in Optimizer\n\n### What changes were proposed in this pull request?\n\n- `EliminateDistinct`: change `transformExpressions` to `transformAllExpressionsWithPruning `\n- `EliminateAggregateFilter `: change `transformExpressionsWithPruning` to `transformAllExpressionsWithPruning`\n\n### Why are the changes needed?\n\n`transformExpressions` can only traverse all expressions in this current query plan, so the rule `EliminateDistinct` and `EliminateAggregateFilter` can not optimize the non-root node. We should use `transformAllExpressions` rather than `transformExpressions`.\n\n### Does this PR introduce _any_ user-facing change?\n\nno, only change plan\n\n### How was this patch tested?\n\nadd new test for  `EliminateDistinct` and `EliminateAggregateFilter`\n\nCloses #35479 from ulysses-you/SPARK-38177.\n\nAuthored-by: ulysses-you <ulyssesyou18@gmail.com>\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>', 'distinct': False, 'url': 'https://api.github.com/repos/warrenzhu25/spark/commits/a62ae9f64fbc5f9f472bc01dbd97caa65927baef'}, {'sha': 'e0bc977dd0dd378665da431d4a2497e05ae90cf7', 'author': {'email': 'kabhwan.opensource@gmail.com', 'name': 'Jungtaek Lim'}, 'message': '[SPARK-37970][SS] Introduce AcceptsLatestSeenOffset to indicate latest seen offset to streaming source\n\n### What changes were proposed in this pull request?\n\nThis PR introduces a new interface on streaming data source `AcceptsLatestSeenOffset`, which notifies Spark to provide latest seen offset to the sources implementing the interface at every restart of the query. Spark will provide the latest seen offset before fetching the offset or data.\n\nWorth noting that the interface only support DSv2 streaming sources; the usage of DSv1 streaming source is limited to internal and it has different method call flow, so we would like to focus on DSv2. Spark will throw error if the DSv1 streaming source implements the interface.\n\n### Why are the changes needed?\n\nThis could be useful for the source if source needs to prepare based on the latest seen offset before fetching anything. More specifically, we found this very useful and handy for the data source which needs to track the offset by itself, since the external storage does not provide the offset for the latest available data.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo, the change is limited to the data source developers.\n\n### How was this patch tested?\n\nNew unit tests.\n\nCloses #35259 from HeartSaVioR/SPARK-37970.\n\nAuthored-by: Jungtaek Lim <kabhwan.opensource@gmail.com>\nSigned-off-by: Yuanjian Li <yuanjian.li@databricks.com>', 'distinct': False, 'url': 'https://api.github.com/repos/warrenzhu25/spark/commits/e0bc977dd0dd378665da431d4a2497e05ae90cf7'}, {'sha': 'e81f6118693b9f624c54b52ca92fd80d6c9d4432', 'author': {'email': 'yangjie01@baidu.com', 'name': 'yangjie01'}, 'message': '[SPARK-38036][SQL][TESTS] Refactor `VersionsSuite` to `HiveClientSuite` and make it a subclass of `HiveVersionSuite`\n\n### What changes were proposed in this pull request?\nThere is a TODO in `VersionsSuite`:\n\n- TODO: Refactor this to `HiveClientSuite` and make it a subclass of `HiveVersionSuite`\n\nthis pr completed this TODO, the main change as follows:\n\n- copy all test cases in `versions.foreach` scope of `VersionsSuite` to `HiveClientSuite`\n- override `nestedSuites` function in `HiveClientSuites` to use each hive version to test the cases in `HiveClientSuite` similar as `HiveClientUserNameSuites` and `HivePartitionFilteringSuites`\n- move other cases to `HiveClientSuites`\n\n### Why are the changes needed?\nMake `VersionsSuite` as a subclass of `HiveVersionSuite`  to unify the test mode of multi version hive\n\n### Does this PR introduce _any_ user-facing change?\nNo.\n\n### How was this patch tested?\n- Pass GA\n- Manual test:\n\n**Before**\n\n```\nmvn clean install -DskipTests -pl sql/hive -am\nmvn test -pl sql/hive -Dtest=none -DwildcardSuites=org.apache.spark.sql.hive.client.HiveClientSuites\n\nRun completed in 13 minutes, 10 seconds.\nTotal number of tests run: 867\nSuites: completed 2, aborted 0\nTests: succeeded 867, failed 0, canceled 0, ignored 1, pending 0\nAll tests passed.\n```\n\n**After**\n\n```\nmvn clean install -DskipTests -pl sql/hive -am\nmvn test -pl sql/hive -Dtest=none -DwildcardSuites=org.apache.spark.sql.hive.client.HiveClientSuites\n\nRun completed in 3 minutes, 8 seconds.\nTotal number of tests run: 867\nSuites: completed 14, aborted 0\nTests: succeeded 867, failed 0, canceled 0, ignored 1, pending 0\nAll tests passed\n```\n\nThe number of test cases is the same, and Suites changed from 2 to 14\n\nCloses #35335 from LuciferYang/SPARK-38036.\n\nAuthored-by: yangjie01 <yangjie01@baidu.com>\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>', 'distinct': False, 'url': 'https://api.github.com/repos/warrenzhu25/spark/commits/e81f6118693b9f624c54b52ca92fd80d6c9d4432'}, {'sha': 'f741c0a82a3bb9d01e1cab26c79622880d7dd1ba', 'author': {'email': 'gengliang@apache.org', 'name': 'Gengliang Wang'}, 'message': '[SPARK-38186][SQL] Improve the README of Spark docs\n\n### What changes were proposed in this pull request?\n\nImprove https://github.com/apache/spark/blob/master/docs/README.md, mark some of the setup steps as optional.\nAlso, recommend developers to use `SKIP_API=1` if no API docs are needed.\n\n### Why are the changes needed?\n\n Developers usually need to generate HTML Docs without API Docs. Marking the setup for SQL/Python/R doc as optional can help them avoid unnecessary efforts.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo\n\n### How was this patch tested?\n\nManual test\n\nCloses #35491 from gengliangwang/updateDoc.\n\nAuthored-by: Gengliang Wang <gengliang@apache.org>\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>', 'distinct': False, 'url': 'https://api.github.com/repos/warrenzhu25/spark/commits/f741c0a82a3bb9d01e1cab26c79622880d7dd1ba'}, {'sha': 'd4a2e5c55d127218f6ae42925443f7d0588d5875', 'author': {'email': 'xinrong.meng@databricks.com', 'name': 'Xinrong Meng'}, 'message': '[SPARK-38184][SQL][DOCS] Fix malformatted ExpressionDescription of decode\n\n### What changes were proposed in this pull request?\nFix malformatted ExpressionDescription of decode.\n\n### Why are the changes needed?\nCurrently, https://spark.apache.org/docs/latest/api/sql/#decode is malformed because its ExpressionDescription uses the pipe symbol.\n\n### Does this PR introduce _any_ user-facing change?\nDoc change only.\n\n### How was this patch tested?\nManual test.\n\nUpdated HTML looks as below:\n![image](https://user-images.githubusercontent.com/47337188/153554869-b2b16250-6a9a-4f84-9774-96c0894edcd7.png)\n\nCloses #35489 from xinrong-databricks/fixExpressionDescription.\n\nAuthored-by: Xinrong Meng <xinrong.meng@databricks.com>\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>', 'distinct': False, 'url': 'https://api.github.com/repos/warrenzhu25/spark/commits/d4a2e5c55d127218f6ae42925443f7d0588d5875'}, {'sha': '25a4c5fa84d64e37cf5c27c7b2f0f29867330bf2', 'author': {'email': 'ulyssesyou18@gmail.com', 'name': 'ulysses-you'}, 'message': '[SPARK-38185][SQL] Fix data incorrect if aggregate function is empty\n\n### What changes were proposed in this pull request?\n\nAdd `aggregateExpressions.nonEmpty` check in `groupOnly` function.\n\n### Why are the changes needed?\n\nThe group only condition should check if the aggregate expression is empty.\n\nIn DataFrame api, it is allowed to make a empty aggregations.\n\nSo the following query should return 1 rather than 0 because it\'s a global aggregate.\n```scala\nval emptyAgg = Map.empty[String, String]\nspark.range(2).where(""id > 2"").agg(emptyAgg).limit(1).count\n```\n\n### Does this PR introduce _any_ user-facing change?\n\nyes, bug fix\n\n### How was this patch tested?\n\nAdd test\n\nCloses #35490 from ulysses-you/SPARK-38185.\n\nAuthored-by: ulysses-you <ulyssesyou18@gmail.com>\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>', 'distinct': False, 'url': 'https://api.github.com/repos/warrenzhu25/spark/commits/25a4c5fa84d64e37cf5c27c7b2f0f29867330bf2'}]}",,Scala
20223336050,PullRequestEvent,apache/spark,0.0,2022-02-13T03:36:05Z,"{'action': 'opened', 'number': 35500, 'pull_request': {'url': 'https://api.github.com/repos/apache/spark/pulls/35500', 'id': 849991495, 'node_id': 'PR_kwDOAQXtWs4yqddH', 'html_url': 'https://github.com/apache/spark/pull/35500', 'diff_url': 'https://github.com/apache/spark/pull/35500.diff', 'patch_url': 'https://github.com/apache/spark/pull/35500.patch', 'issue_url': 'https://api.github.com/repos/apache/spark/issues/35500', 'number': 35500, 'state': 'open', 'locked': False, 'title': '[SPARK-33274][SS] Stop query in cp mode when total cores less than total kafka partition', 'user': {'login': 'warrenzhu25', 'id': 1633312, 'node_id': 'MDQ6VXNlcjE2MzMzMTI=', 'avatar_url': 'https://avatars.githubusercontent.com/u/1633312?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/warrenzhu25', 'html_url': 'https://github.com/warrenzhu25', 'followers_url': 'https://api.github.com/users/warrenzhu25/followers', 'following_url': 'https://api.github.com/users/warrenzhu25/following{/other_user}', 'gists_url': 'https://api.github.com/users/warrenzhu25/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/warrenzhu25/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/warrenzhu25/subscriptions', 'organizations_url': 'https://api.github.com/users/warrenzhu25/orgs', 'repos_url': 'https://api.github.com/users/warrenzhu25/repos', 'events_url': 'https://api.github.com/users/warrenzhu25/events{/privacy}', 'received_events_url': 'https://api.github.com/users/warrenzhu25/received_events', 'type': 'User', 'site_admin': False}, 'body': ""### What changes were proposed in this pull request?\r\nAdd check for total executor cores when `SetReaderPartitions` message received.\r\n\r\n### Why are the changes needed?\r\nIn continuous processing mode, EpochCoordinator won't add offsets to query until got ReportPartitionOffset from all partitions. Normally, each kafka topic partition will be handled by one core, if total cores is smaller than total kafka topic partition counts, the job will hang without any error message.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nYes, if total executor cores is smaller than total kafka partition count, the exception with below error will be thrown:\r\n`Total %s (kafka partitions) * %s (cpus per task) = %s needed, \r\nbut only have %s (executors) * %s (cores per executor) = %s (total cores).\r\nPlease increase total number of executor cores to at least %s.`\r\n\r\n### How was this patch tested?\r\nAdded test in EpochCoordinatorSuite\r\n"", 'created_at': '2022-02-13T03:36:04Z', 'updated_at': '2022-02-13T03:36:04Z', 'closed_at': None, 'merged_at': None, 'merge_commit_sha': None, 'assignee': None, 'assignees': [], 'requested_reviewers': [], 'requested_teams': [], 'labels': [], 'milestone': None, 'draft': False, 'commits_url': 'https://api.github.com/repos/apache/spark/pulls/35500/commits', 'review_comments_url': 'https://api.github.com/repos/apache/spark/pulls/35500/comments', 'review_comment_url': 'https://api.github.com/repos/apache/spark/pulls/comments{/number}', 'comments_url': 'https://api.github.com/repos/apache/spark/issues/35500/comments', 'statuses_url': 'https://api.github.com/repos/apache/spark/statuses/2a3d5b6c96d199f958ad0a38b9e808d23dcc7a3e', 'head': {'label': 'warrenzhu25:33274', 'ref': '33274', 'sha': '2a3d5b6c96d199f958ad0a38b9e808d23dcc7a3e', 'user': {'login': 'warrenzhu25', 'id': 1633312, 'node_id': 'MDQ6VXNlcjE2MzMzMTI=', 'avatar_url': 'https://avatars.githubusercontent.com/u/1633312?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/warrenzhu25', 'html_url': 'https://github.com/warrenzhu25', 'followers_url': 'https://api.github.com/users/warrenzhu25/followers', 'following_url': 'https://api.github.com/users/warrenzhu25/following{/other_user}', 'gists_url': 'https://api.github.com/users/warrenzhu25/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/warrenzhu25/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/warrenzhu25/subscriptions', 'organizations_url': 'https://api.github.com/users/warrenzhu25/orgs', 'repos_url': 'https://api.github.com/users/warrenzhu25/repos', 'events_url': 'https://api.github.com/users/warrenzhu25/events{/privacy}', 'received_events_url': 'https://api.github.com/users/warrenzhu25/received_events', 'type': 'User', 'site_admin': False}, 'repo': {'id': 217627454, 'node_id': 'MDEwOlJlcG9zaXRvcnkyMTc2Mjc0NTQ=', 'name': 'spark', 'full_name': 'warrenzhu25/spark', 'private': False, 'owner': {'login': 'warrenzhu25', 'id': 1633312, 'node_id': 'MDQ6VXNlcjE2MzMzMTI=', 'avatar_url': 'https://avatars.githubusercontent.com/u/1633312?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/warrenzhu25', 'html_url': 'https://github.com/warrenzhu25', 'followers_url': 'https://api.github.com/users/warrenzhu25/followers', 'following_url': 'https://api.github.com/users/warrenzhu25/following{/other_user}', 'gists_url': 'https://api.github.com/users/warrenzhu25/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/warrenzhu25/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/warrenzhu25/subscriptions', 'organizations_url': 'https://api.github.com/users/warrenzhu25/orgs', 'repos_url': 'https://api.github.com/users/warrenzhu25/repos', 'events_url': 'https://api.github.com/users/warrenzhu25/events{/privacy}', 'received_events_url': 'https://api.github.com/users/warrenzhu25/received_events', 'type': 'User', 'site_admin': False}, 'html_url': 'https://github.com/warrenzhu25/spark', 'description': 'Apache Spark', 'fork': True, 'url': 'https://api.github.com/repos/warrenzhu25/spark', 'forks_url': 'https://api.github.com/repos/warrenzhu25/spark/forks', 'keys_url': 'https://api.github.com/repos/warrenzhu25/spark/keys{/key_id}', 'collaborators_url': 'https://api.github.com/repos/warrenzhu25/spark/collaborators{/collaborator}', 'teams_url': 'https://api.github.com/repos/warrenzhu25/spark/teams', 'hooks_url': 'https://api.github.com/repos/warrenzhu25/spark/hooks', 'issue_events_url': 'https://api.github.com/repos/warrenzhu25/spark/issues/events{/number}', 'events_url': 'https://api.github.com/repos/warrenzhu25/spark/events', 'assignees_url': 'https://api.github.com/repos/warrenzhu25/spark/assignees{/user}', 'branches_url': 'https://api.github.com/repos/warrenzhu25/spark/branches{/branch}', 'tags_url': 'https://api.github.com/repos/warrenzhu25/spark/tags', 'blobs_url': 'https://api.github.com/repos/warrenzhu25/spark/git/blobs{/sha}', 'git_tags_url': 'https://api.github.com/repos/warrenzhu25/spark/git/tags{/sha}', 'git_refs_url': 'https://api.github.com/repos/warrenzhu25/spark/git/refs{/sha}', 'trees_url': 'https://api.github.com/repos/warrenzhu25/spark/git/trees{/sha}', 'statuses_url': 'https://api.github.com/repos/warrenzhu25/spark/statuses/{sha}', 'languages_url': 'https://api.github.com/repos/warrenzhu25/spark/languages', 'stargazers_url': 'https://api.github.com/repos/warrenzhu25/spark/stargazers', 'contributors_url': 'https://api.github.com/repos/warrenzhu25/spark/contributors', 'subscribers_url': 'https://api.github.com/repos/warrenzhu25/spark/subscribers', 'subscription_url': 'https://api.github.com/repos/warrenzhu25/spark/subscription', 'commits_url': 'https://api.github.com/repos/warrenzhu25/spark/commits{/sha}', 'git_commits_url': 'https://api.github.com/repos/warrenzhu25/spark/git/commits{/sha}', 'comments_url': 'https://api.github.com/repos/warrenzhu25/spark/comments{/number}', 'issue_comment_url': 'https://api.github.com/repos/warrenzhu25/spark/issues/comments{/number}', 'contents_url': 'https://api.github.com/repos/warrenzhu25/spark/contents/{+path}', 'compare_url': 'https://api.github.com/repos/warrenzhu25/spark/compare/{base}...{head}', 'merges_url': 'https://api.github.com/repos/warrenzhu25/spark/merges', 'archive_url': 'https://api.github.com/repos/warrenzhu25/spark/{archive_format}{/ref}', 'downloads_url': 'https://api.github.com/repos/warrenzhu25/spark/downloads', 'issues_url': 'https://api.github.com/repos/warrenzhu25/spark/issues{/number}', 'pulls_url': 'https://api.github.com/repos/warrenzhu25/spark/pulls{/number}', 'milestones_url': 'https://api.github.com/repos/warrenzhu25/spark/milestones{/number}', 'notifications_url': 'https://api.github.com/repos/warrenzhu25/spark/notifications{?since,all,participating}', 'labels_url': 'https://api.github.com/repos/warrenzhu25/spark/labels{/name}', 'releases_url': 'https://api.github.com/repos/warrenzhu25/spark/releases{/id}', 'deployments_url': 'https://api.github.com/repos/warrenzhu25/spark/deployments', 'created_at': '2019-10-25T23:12:23Z', 'updated_at': '2020-07-10T17:08:49Z', 'pushed_at': '2022-02-13T03:34:28Z', 'git_url': 'git://github.com/warrenzhu25/spark.git', 'ssh_url': 'git@github.com:warrenzhu25/spark.git', 'clone_url': 'https://github.com/warrenzhu25/spark.git', 'svn_url': 'https://github.com/warrenzhu25/spark', 'homepage': '', 'size': 397856, 'stargazers_count': 0, 'watchers_count': 0, 'language': 'Scala', 'has_issues': False, 'has_projects': True, 'has_downloads': True, 'has_wiki': False, 'has_pages': False, 'forks_count': 0, 'mirror_url': None, 'archived': False, 'disabled': False, 'open_issues_count': 0, 'license': {'key': 'apache-2.0', 'name': 'Apache License 2.0', 'spdx_id': 'Apache-2.0', 'url': 'https://api.github.com/licenses/apache-2.0', 'node_id': 'MDc6TGljZW5zZTI='}, 'allow_forking': True, 'is_template': False, 'topics': [], 'visibility': 'public', 'forks': 0, 'open_issues': 0, 'watchers': 0, 'default_branch': 'master'}}, 'base': {'label': 'apache:master', 'ref': 'master', 'sha': '25dd4254fed71923731fd59838875c0dd1ff665a', 'user': {'login': 'apache', 'id': 47359, 'node_id': 'MDEyOk9yZ2FuaXphdGlvbjQ3MzU5', 'avatar_url': 'https://avatars.githubusercontent.com/u/47359?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/apache', 'html_url': 'https://github.com/apache', 'followers_url': 'https://api.github.com/users/apache/followers', 'following_url': 'https://api.github.com/users/apache/following{/other_user}', 'gists_url': 'https://api.github.com/users/apache/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/apache/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/apache/subscriptions', 'organizations_url': 'https://api.github.com/users/apache/orgs', 'repos_url': 'https://api.github.com/users/apache/repos', 'events_url': 'https://api.github.com/users/apache/events{/privacy}', 'received_events_url': 'https://api.github.com/users/apache/received_events', 'type': 'Organization', 'site_admin': False}, 'repo': {'id': 17165658, 'node_id': 'MDEwOlJlcG9zaXRvcnkxNzE2NTY1OA==', 'name': 'spark', 'full_name': 'apache/spark', 'private': False, 'owner': {'login': 'apache', 'id': 47359, 'node_id': 'MDEyOk9yZ2FuaXphdGlvbjQ3MzU5', 'avatar_url': 'https://avatars.githubusercontent.com/u/47359?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/apache', 'html_url': 'https://github.com/apache', 'followers_url': 'https://api.github.com/users/apache/followers', 'following_url': 'https://api.github.com/users/apache/following{/other_user}', 'gists_url': 'https://api.github.com/users/apache/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/apache/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/apache/subscriptions', 'organizations_url': 'https://api.github.com/users/apache/orgs', 'repos_url': 'https://api.github.com/users/apache/repos', 'events_url': 'https://api.github.com/users/apache/events{/privacy}', 'received_events_url': 'https://api.github.com/users/apache/received_events', 'type': 'Organization', 'site_admin': False}, 'html_url': 'https://github.com/apache/spark', 'description': 'Apache Spark - A unified analytics engine for large-scale data processing', 'fork': False, 'url': 'https://api.github.com/repos/apache/spark', 'forks_url': 'https://api.github.com/repos/apache/spark/forks', 'keys_url': 'https://api.github.com/repos/apache/spark/keys{/key_id}', 'collaborators_url': 'https://api.github.com/repos/apache/spark/collaborators{/collaborator}', 'teams_url': 'https://api.github.com/repos/apache/spark/teams', 'hooks_url': 'https://api.github.com/repos/apache/spark/hooks', 'issue_events_url': 'https://api.github.com/repos/apache/spark/issues/events{/number}', 'events_url': 'https://api.github.com/repos/apache/spark/events', 'assignees_url': 'https://api.github.com/repos/apache/spark/assignees{/user}', 'branches_url': 'https://api.github.com/repos/apache/spark/branches{/branch}', 'tags_url': 'https://api.github.com/repos/apache/spark/tags', 'blobs_url': 'https://api.github.com/repos/apache/spark/git/blobs{/sha}', 'git_tags_url': 'https://api.github.com/repos/apache/spark/git/tags{/sha}', 'git_refs_url': 'https://api.github.com/repos/apache/spark/git/refs{/sha}', 'trees_url': 'https://api.github.com/repos/apache/spark/git/trees{/sha}', 'statuses_url': 'https://api.github.com/repos/apache/spark/statuses/{sha}', 'languages_url': 'https://api.github.com/repos/apache/spark/languages', 'stargazers_url': 'https://api.github.com/repos/apache/spark/stargazers', 'contributors_url': 'https://api.github.com/repos/apache/spark/contributors', 'subscribers_url': 'https://api.github.com/repos/apache/spark/subscribers', 'subscription_url': 'https://api.github.com/repos/apache/spark/subscription', 'commits_url': 'https://api.github.com/repos/apache/spark/commits{/sha}', 'git_commits_url': 'https://api.github.com/repos/apache/spark/git/commits{/sha}', 'comments_url': 'https://api.github.com/repos/apache/spark/comments{/number}', 'issue_comment_url': 'https://api.github.com/repos/apache/spark/issues/comments{/number}', 'contents_url': 'https://api.github.com/repos/apache/spark/contents/{+path}', 'compare_url': 'https://api.github.com/repos/apache/spark/compare/{base}...{head}', 'merges_url': 'https://api.github.com/repos/apache/spark/merges', 'archive_url': 'https://api.github.com/repos/apache/spark/{archive_format}{/ref}', 'downloads_url': 'https://api.github.com/repos/apache/spark/downloads', 'issues_url': 'https://api.github.com/repos/apache/spark/issues{/number}', 'pulls_url': 'https://api.github.com/repos/apache/spark/pulls{/number}', 'milestones_url': 'https://api.github.com/repos/apache/spark/milestones{/number}', 'notifications_url': 'https://api.github.com/repos/apache/spark/notifications{?since,all,participating}', 'labels_url': 'https://api.github.com/repos/apache/spark/labels{/name}', 'releases_url': 'https://api.github.com/repos/apache/spark/releases{/id}', 'deployments_url': 'https://api.github.com/repos/apache/spark/deployments', 'created_at': '2014-02-25T08:00:08Z', 'updated_at': '2022-02-12T19:17:52Z', 'pushed_at': '2022-02-13T03:36:05Z', 'git_url': 'git://github.com/apache/spark.git', 'ssh_url': 'git@github.com:apache/spark.git', 'clone_url': 'https://github.com/apache/spark.git', 'svn_url': 'https://github.com/apache/spark', 'homepage': 'https://spark.apache.org/', 'size': 406848, 'stargazers_count': 32048, 'watchers_count': 32048, 'language': 'Scala', 'has_issues': False, 'has_projects': True, 'has_downloads': True, 'has_wiki': False, 'has_pages': False, 'forks_count': 25214, 'mirror_url': None, 'archived': False, 'disabled': False, 'open_issues_count': 245, 'license': {'key': 'apache-2.0', 'name': 'Apache License 2.0', 'spdx_id': 'Apache-2.0', 'url': 'https://api.github.com/licenses/apache-2.0', 'node_id': 'MDc6TGljZW5zZTI='}, 'allow_forking': True, 'is_template': False, 'topics': ['big-data', 'java', 'jdbc', 'python', 'r', 'scala', 'spark', 'sql'], 'visibility': 'public', 'forks': 25214, 'open_issues': 245, 'watchers': 32048, 'default_branch': 'master'}}, '_links': {'self': {'href': 'https://api.github.com/repos/apache/spark/pulls/35500'}, 'html': {'href': 'https://github.com/apache/spark/pull/35500'}, 'issue': {'href': 'https://api.github.com/repos/apache/spark/issues/35500'}, 'comments': {'href': 'https://api.github.com/repos/apache/spark/issues/35500/comments'}, 'review_comments': {'href': 'https://api.github.com/repos/apache/spark/pulls/35500/comments'}, 'review_comment': {'href': 'https://api.github.com/repos/apache/spark/pulls/comments{/number}'}, 'commits': {'href': 'https://api.github.com/repos/apache/spark/pulls/35500/commits'}, 'statuses': {'href': 'https://api.github.com/repos/apache/spark/statuses/2a3d5b6c96d199f958ad0a38b9e808d23dcc7a3e'}}, 'author_association': 'CONTRIBUTOR', 'auto_merge': None, 'active_lock_reason': None, 'merged': False, 'mergeable': None, 'rebaseable': None, 'mergeable_state': 'unknown', 'merged_by': None, 'comments': 0, 'review_comments': 0, 'maintainer_can_modify': True, 'commits': 1, 'additions': 52, 'deletions': 3, 'changed_files': 6}}",opened,Scala
20223323404,PushEvent,warrenzhu25/spark,0.0,2022-02-13T03:34:36Z,"{'push_id': 9086129344, 'size': 1000, 'distinct_size': 2, 'ref': 'refs/heads/33274', 'head': '2a3d5b6c96d199f958ad0a38b9e808d23dcc7a3e', 'before': '103836b38651169aac701a371bfd0d33b1631be4', 'commits': [{'sha': '3ac0382759d33a46cf5d590c6f3915be7fa75351', 'author': {'email': 'max.gekk@gmail.com', 'name': 'Max Gekk'}, 'message': ""[SPARK-36920][SQL][FOLLOWUP] Fix input types of `ABS()`: numeric and ANSI intervals\n\n### What changes were proposed in this pull request?\nChange allowed input types of `Abs()` from:\n```\nNumericType + CalendarIntervalType + YearMonthIntervalType + DayTimeIntervalType\n```\nto\n```\nNumericType + YearMonthIntervalType + DayTimeIntervalType\n```\n\n### Why are the changes needed?\nThe changes make the error message more clear.\n\nBefore changes:\n```sql\nspark-sql> set spark.sql.legacy.interval.enabled=true;\nspark.sql.legacy.interval.enabled\ttrue\nspark-sql> select abs(interval -10 days -20 minutes);\n21/10/05 09:11:30 ERROR SparkSQLDriver: Failed in [select abs(interval -10 days -20 minutes)]\njava.lang.ClassCastException: org.apache.spark.sql.types.CalendarIntervalType$ cannot be cast to org.apache.spark.sql.types.NumericType\n\tat org.apache.spark.sql.catalyst.util.TypeUtils$.getNumeric(TypeUtils.scala:77)\n\tat org.apache.spark.sql.catalyst.expressions.Abs.numeric$lzycompute(arithmetic.scala:172)\n\tat org.apache.spark.sql.catalyst.expressions.Abs.numeric(arithmetic.scala:169)\n```\n\nAfter:\n```sql\nspark.sql.legacy.interval.enabled\ttrue\nspark-sql> select abs(interval -10 days -20 minutes);\nError in query: cannot resolve 'abs(INTERVAL '-10 days -20 minutes')' due to data type mismatch: argument 1 requires (numeric or interval day to second or interval year to month) type, however, 'INTERVAL '-10 days -20 minutes'' is of interval type.; line 1 pos 7;\n'Project [unresolvedalias(abs(-10 days -20 minutes, false), None)]\n+- OneRowRelation\n```\n\n### Does this PR introduce _any_ user-facing change?\nNo, because the original changes of https://github.com/apache/spark/pull/34169 haven't released yet.\n\n### How was this patch tested?\nManually checked in the command line, see examples above.\n\nCloses #34183 from MaxGekk/fix-abs-input-types.\n\nAuthored-by: Max Gekk <max.gekk@gmail.com>\nSigned-off-by: Max Gekk <max.gekk@gmail.com>"", 'distinct': False, 'url': 'https://api.github.com/repos/warrenzhu25/spark/commits/3ac0382759d33a46cf5d590c6f3915be7fa75351'}, {'sha': 'e5b01cd823990d71c3ff32061c2998a076166ba8', 'author': {'email': 'minyang@minyang-mn3.linkedin.biz', 'name': 'Minchu Yang'}, 'message': '[SPARK-36705][FOLLOW-UP] Support the case when user\'s classes need to register for Kryo serialization\n\n### What changes were proposed in this pull request?\n\n- Make the val lazy wherever `isPushBasedShuffleEnabled` is invoked when it is a class instance variable, so it can happen after user-defined jars/classes in `spark.kryo.classesToRegister` are downloaded and available on executor-side, as part of the fix for the exception mentioned below.\n\n- Add a flag `checkSerializer` to control whether we need to check a serializer is `supportsRelocationOfSerializedObjects` or not within `isPushBasedShuffleEnabled` as part of the fix for the exception mentioned below. Specifically, we don\'t check this in `registerWithExternalShuffleServer()` in `BlockManager` and `createLocalDirsForMergedShuffleBlocks()` in `DiskBlockManager.scala` as the same issue would raise otherwise.\n\n- Move `instantiateClassFromConf` and `instantiateClass` from `SparkEnv` into `Utils`, in order to let `isPushBasedShuffleEnabled` to leverage them for instantiating serializer instances.\n\n### Why are the changes needed?\n\nWhen user tries to set classes for Kryo Serialization by `spark.kryo.classesToRegister`, below exception(or similar) would be encountered in `isPushBasedShuffleEnabled` as indicated below.\nReproduced the issue in our internal branch by launching spark-shell as:\n```\nspark-shell --spark-version 3.1.1 --packages ml.dmlc:xgboost4j_2.12:1.3.1 --conf spark.kryo.classesToRegister=ml.dmlc.xgboost4j.scala.Booster\n```\n\n```\nException in thread ""main"" java.lang.reflect.UndeclaredThrowableException\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1911)\n\tat org.apache.spark.deploy.SparkHadoopUtil.runAsSparkUser(SparkHadoopUtil.scala:61)\n\tat org.apache.spark.executor.CoarseGrainedExecutorBackend$.run(CoarseGrainedExecutorBackend.scala:393)\n\tat org.apache.spark.executor.YarnCoarseGrainedExecutorBackend$.main(YarnCoarseGrainedExecutorBackend.scala:83)\n\tat org.apache.spark.executor.YarnCoarseGrainedExecutorBackend.main(YarnCoarseGrainedExecutorBackend.scala)\nCaused by: org.apache.spark.SparkException: Failed to register classes with Kryo\n\tat org.apache.spark.serializer.KryoSerializer.$anonfun$newKryo$5(KryoSerializer.scala:183)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.util.Utils$.withContextClassLoader(Utils.scala:230)\n\tat org.apache.spark.serializer.KryoSerializer.newKryo(KryoSerializer.scala:171)\n\tat org.apache.spark.serializer.KryoSerializer$$anon$1.create(KryoSerializer.scala:102)\n\tat com.esotericsoftware.kryo.pool.KryoPoolQueueImpl.borrow(KryoPoolQueueImpl.java:48)\n\tat org.apache.spark.serializer.KryoSerializer$PoolWrapper.borrow(KryoSerializer.scala:109)\n\tat org.apache.spark.serializer.KryoSerializerInstance.borrowKryo(KryoSerializer.scala:346)\n\tat org.apache.spark.serializer.KryoSerializerInstance.getAutoReset(KryoSerializer.scala:446)\n\tat org.apache.spark.serializer.KryoSerializer.supportsRelocationOfSerializedObjects$lzycompute(KryoSerializer.scala:253)\n\tat org.apache.spark.serializer.KryoSerializer.supportsRelocationOfSerializedObjects(KryoSerializer.scala:249)\n\tat org.apache.spark.util.Utils$.isPushBasedShuffleEnabled(Utils.scala:2584)\n\tat org.apache.spark.MapOutputTrackerWorker.<init>(MapOutputTracker.scala:1109)\n\tat org.apache.spark.SparkEnv$.create(SparkEnv.scala:322)\n\tat org.apache.spark.SparkEnv$.createExecutorEnv(SparkEnv.scala:205)\n\tat org.apache.spark.executor.CoarseGrainedExecutorBackend$.$anonfun$run$7(CoarseGrainedExecutorBackend.scala:442)\n\tat org.apache.spark.deploy.SparkHadoopUtil$$anon$1.run(SparkHadoopUtil.scala:62)\n\tat org.apache.spark.deploy.SparkHadoopUtil$$anon$1.run(SparkHadoopUtil.scala:61)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1893)\n\t... 4 more\nCaused by: java.lang.ClassNotFoundException: ml.dmlc.xgboost4j.scala.Booster\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:381)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:424)\n\tat sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:357)\n\tat java.lang.Class.forName0(Native Method)\n\tat java.lang.Class.forName(Class.java:348)\n\tat org.apache.spark.util.Utils$.classForName(Utils.scala:217)\n\tat org.apache.spark.serializer.KryoSerializer.$anonfun$newKryo$6(KryoSerializer.scala:174)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.serializer.KryoSerializer.$anonfun$newKryo$5(KryoSerializer.scala:173)\n\t... 24 more\n```\nRegistering user class for kryo serialization is happening after serializer creation in SparkEnv. Serializer creation can happen in `isPushBasedShuffleEnabled`, which can be called in some places prior to SparkEnv is created. Also, as per analysis by JoshRosen, this is probably due to Kryo instantiation was failing because added packages hadn\'t been downloaded to the executor yet (because this code is running during executor startup, not task startup). The proposed change helps fix this issue.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo.\n\n### How was this patch tested?\n\nPassed existing tests.\nTested this patch in our internal branch where user reported the issue. Issue is now not reproducible with this patch.\n\nCloses #34158 from rmcyang/SPARK-33781-bugFix.\n\nLead-authored-by: Minchu Yang <minyang@minyang-mn3.linkedin.biz>\nCo-authored-by: Minchu Yang <31781684+rmcyang@users.noreply.github.com>\nSigned-off-by: Mridul Muralidharan <mridul<at>gmail.com>', 'distinct': False, 'url': 'https://api.github.com/repos/warrenzhu25/spark/commits/e5b01cd823990d71c3ff32061c2998a076166ba8'}, {'sha': '6a2452fb5cd776dc1f292704e6b86bbec0ff24e7', 'author': {'email': 'gurwls223@apache.org', 'name': 'Hyukjin Kwon'}, 'message': '[SPARK-36711][PYTHON][FOLLOW-UP] Refactor typing logic for multi-index support\n\n### What changes were proposed in this pull request?\n\nThis PR proposes to refactor typing logic for multi-index support that was mostly introduced in https://github.com/apache/spark/pull/34176\n\nAt a high level, the below functions were introduced\n\n```bash\n_extract_types # renamed from `extract_types`\n```\n\n```\n_is_named_params\n_address_named_type_hoders\n_to_tuple_of_params\n_convert_tuples_to_zip\n_address_unnamed_type_holders\n```\n\nIn this PR, they become as below with simplification:\n\n```bash\n_to_type_holders # renamed from `_extract_types`\n```\n\n```bash\n_new_type_holders # merged from `_is_named_params`, etc.\n```\n\n### Why are the changes needed?\n\nTo make the codes easier to read.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo, dev-only.\n\n### How was this patch tested?\n\nExisting tests should cover them.\n\nCloses #34181 from HyukjinKwon/SPARK-36711.\n\nAuthored-by: Hyukjin Kwon <gurwls223@apache.org>\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>', 'distinct': False, 'url': 'https://api.github.com/repos/warrenzhu25/spark/commits/6a2452fb5cd776dc1f292704e6b86bbec0ff24e7'}, {'sha': '5c6f0b9263f29f805f386237448f671dea3ad6c5', 'author': {'email': 'dgd_contributor@viettel.com.vn', 'name': 'dchvn nguyen'}, 'message': '[SPARK-36930][PYTHON] Support ps.MultiIndex.dtypes\n\n### What changes were proposed in this pull request?\nAdd dtypes for MultiIndex\n\n### Why are the changes needed?\nAdd dtypes for MultiIndex\n\nBefore this PR:\n\n```python\n>>> idx = pd.MultiIndex.from_arrays([[0, 1, 2, 3, 4, 5, 6, 7, 8], [1, 2, 3, 4, 5, 6, 7, 8, 9]], names=(""zero"", ""one""))\n>>> pdf = pd.DataFrame(\n...     {""a"": [1, 2, 3, 4, 5, 6, 7, 8, 9], ""b"": [4, 5, 6, 3, 2, 1, 0, 0, 0]},\n...     index=idx,\n... )\n>>> psdf = ps.from_pandas(pdf)\n>>>\n>>> ps.DataFrame[psdf.index.dtypes, psdf.dtypes]\nTraceback (most recent call last):\n  File ""<stdin>"", line 1, in <module>\n  File ""/u02/spark/python/pyspark/pandas/indexes/multi.py"", line 917, in __getattr__\n    raise AttributeError(""\'MultiIndex\' object has no attribute \'{}\'"".format(item))\nAttributeError: \'MultiIndex\' object has no attribute \'dtypes\'\n>>>\n```\n\n### Does this PR introduce _any_ user-facing change?\nAfter this PR user can use ```MultiIndex.dtypes``` for:\n\n``` python\n>>> ps.DataFrame[psdf.index.dtypes, psdf.dtypes]\ntyping.Tuple[pyspark.pandas.typedef.typehints.IndexNameType, pyspark.pandas.typedef.typehints.IndexNameType, pyspark.pandas.typedef.typehints.NameType, pyspark.pandas.typedef.typehints.NameType]\n```\n\n### How was this patch tested?\nunit tests.\n\nCloses #34179 from dchvn/add_multiindex_dtypes.\n\nLead-authored-by: dchvn nguyen <dgd_contributor@viettel.com.vn>\nCo-authored-by: dch nguyen <dgd_contributor@viettel.com.vn>\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>', 'distinct': False, 'url': 'https://api.github.com/repos/warrenzhu25/spark/commits/5c6f0b9263f29f805f386237448f671dea3ad6c5'}, {'sha': '31b6f614d3173c8a5852243bf7d0b6200788432d', 'author': {'email': 'yezhou@linkedin.com', 'name': 'Ye Zhou'}, 'message': '[SPARK-36892][CORE] Disable batch fetch for a shuffle when push based shuffle is enabled\n\nWe found an issue where user configured both AQE and push based shuffle, but the job started to hang after running some  stages. We took the thread dump from the Executors, which showed the task is still waiting to fetch shuffle blocks.\nProposed changes in the PR to fix the issue.\n\n### What changes were proposed in this pull request?\nDisabled Batch fetch when push based shuffle is enabled.\n\n### Why are the changes needed?\nWithout this patch, enabling AQE and Push based shuffle will have a chance to hang the tasks.\n\n### Does this PR introduce _any_ user-facing change?\nNo\n\n### How was this patch tested?\nTested the PR within our PR, with Spark shell and the queries are:\n\nsql(""""""SELECT CASE WHEN rand() < 0.8 THEN 100 ELSE CAST(rand() * 30000000 AS INT) END AS s_item_id, CAST(rand() * 100 AS INT) AS s_quantity, DATE_ADD(current_date(), - CAST(rand() * 360 AS INT)) AS s_date FROM RANGE(1000000000)"""""").createOrReplaceTempView(""sales"")\n// Dynamically coalesce partitions\nsql(""""""SELECT s_date, sum(s_quantity) AS q FROM sales GROUP BY s_date ORDER BY q DESC"""""").collect\n\nUnit tests to be added.\n\nCloses #34156 from zhouyejoe/SPARK-36892.\n\nAuthored-by: Ye Zhou <yezhou@linkedin.com>\nSigned-off-by: Gengliang Wang <gengliang@apache.org>', 'distinct': False, 'url': 'https://api.github.com/repos/warrenzhu25/spark/commits/31b6f614d3173c8a5852243bf7d0b6200788432d'}, {'sha': 'f6013c8ce5b4efccd64cd900c694e40612c81d9c', 'author': {'email': 'xinrong.meng@databricks.com', 'name': 'Xinrong Meng'}, 'message': '[SPARK-36927][PYTHON] Inline type hints for python/pyspark/sql/window.py\n\n### What changes were proposed in this pull request?\nInline type hints for python/pyspark/sql/window.py\n\n### Why are the changes needed?\nCurrently, stub files are used for type hints. However, statements within functions cannot be type-checked.\nThe PR is proposed to inline type hints for python/pyspark/sql/window.py to type check statements within functions.\n\n### Does this PR introduce _any_ user-facing change?\nNo.\n\n### How was this patch tested?\nExisting tests.\n\nCloses #34173 from xinrong-databricks/inline_window.\n\nAuthored-by: Xinrong Meng <xinrong.meng@databricks.com>\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>', 'distinct': False, 'url': 'https://api.github.com/repos/warrenzhu25/spark/commits/f6013c8ce5b4efccd64cd900c694e40612c81d9c'}, {'sha': 'aed977c4682b6f378a26050ffab51b9b2075cae4', 'author': {'email': 'adrianhu96@gmail.com', 'name': 'tianhanhu'}, 'message': '[SPARK-36919][SQL] Make BadRecordException fields transient\n\n### What changes were proposed in this pull request?\nMigrating a Spark application from 2.4.x to 3.1.x and finding a difference in the exception chaining behavior. In a case of parsing a malformed CSV, where the root cause exception should be Caused by: java.lang.RuntimeException: Malformed CSV record, only the top level exception is kept, and all lower level exceptions and root cause are lost. Thus, when we call ExceptionUtils.getRootCause on the exception, we still get itself.\nThe reason for the difference is that RuntimeException is wrapped in BadRecordException, which has unserializable fields. When we try to serialize the exception from tasks and deserialize from scheduler, the exception is lost.\nThis PR makes unserializable fields of BadRecordException transient, so the rest of the exception could be serialized and deserialized properly.\n\n### Why are the changes needed?\nMake BadRecordException serializable\n\n### Does this PR introduce _any_ user-facing change?\nUser could get root cause of BadRecordException\n\n### How was this patch tested?\nUnit testing\n\nCloses #34167 from tianhanhu/master.\n\nAuthored-by: tianhanhu <adrianhu96@gmail.com>\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>', 'distinct': False, 'url': 'https://api.github.com/repos/warrenzhu25/spark/commits/aed977c4682b6f378a26050ffab51b9b2075cae4'}, {'sha': '2953d4f5005705d0e08f457156fcfe4e7e4ffa17', 'author': {'email': 'gurwls223@apache.org', 'name': 'Hyukjin Kwon'}, 'message': '[SPARK-36751][PYTHON][DOCS][FOLLOW-UP] Fix unexpected section title for Examples in docstring\n\n### What changes were proposed in this pull request?\n\nThis PR is a minor followup of https://github.com/apache/spark/pull/33992 to fix the warnings during PySpark documentation build:\n\n```\n/.../spark/python/pyspark/sql/functions.py:docstring of pyspark.sql.functions.bit_length:19: WARNING: Unexpected section title or transition.\n\n-------\n/.../spark/python/pyspark/sql/functions.py:docstring of pyspark.sql.functions.octet_length:19: WARNING: Unexpected section title or transition.\n\n-------\n```\n\nWe should always have the same length of hyphens with the title.\n\n### Why are the changes needed?\n\nTo remove warnings during the documentation build and show the HTML pages correctly.\n\n### Does this PR introduce _any_ user-facing change?\n\nThis is not released yet, and only in master branch. So, no to end users.\n\n### How was this patch tested?\n\nManually built the docs via `make clean html` at `python/docs` directory.\n\nCloses #34196 from HyukjinKwon/SPARK-36751.\n\nAuthored-by: Hyukjin Kwon <gurwls223@apache.org>\nSigned-off-by: Kousuke Saruta <sarutak@oss.nttdata.com>', 'distinct': False, 'url': 'https://api.github.com/repos/warrenzhu25/spark/commits/2953d4f5005705d0e08f457156fcfe4e7e4ffa17'}, {'sha': '090c9bf73843c4741fcdaa26a7b3b63b9e2e01b8', 'author': {'email': 'sarutak@oss.nttdata.com', 'name': 'Kousuke Saruta'}, 'message': '[SPARK-36937][SQL][TESTS] Change OrcSourceSuite to test both V1 and V2 sources\n\n### What changes were proposed in this pull request?\n\nThis PR changes the existing `OrcSourceSuite` as abstract class and adds `OrcSource{V1,V2}Suite` to test both V1 and V2 ORC sources.\n\n### Why are the changes needed?\n\nFor the better test coverage.\nThere is no V2 test for the ORC source which implements `CommonFileDataSourceSuite` while the corresponding ones exist for all other built-in file-based datasources.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo.\n\n### How was this patch tested?\n\nExisting tests.\n\nCloses #34194 from sarutak/restructure-OrcSourceSuite.\n\nAuthored-by: Kousuke Saruta <sarutak@oss.nttdata.com>\nSigned-off-by: Max Gekk <max.gekk@gmail.com>', 'distinct': False, 'url': 'https://api.github.com/repos/warrenzhu25/spark/commits/090c9bf73843c4741fcdaa26a7b3b63b9e2e01b8'}, {'sha': '0902b47a7529d036afdaf181d96abdb2fbfb8d77', 'author': {'email': 'gurwls223@apache.org', 'name': 'Hyukjin Kwon'}, 'message': ""[SPARK-36939][PYTHON][DOCS] Add orphan migration page into list in PySpark documentation\n\n### What changes were proposed in this pull request?\n\nThis PR fixes the warning below during PySpark documentation build:\n\n```\nchecking consistency... /.../spark/python/docs/source/migration_guide/pyspark_3.2_to_3.3.rst: WARNING: document isn't included in any toctree\ndone\n```\n\nSPARK-36618 added a new migration guide page but that's mistakenly not added to `spark/python/docs/source/migration_guideindex.rst` resulting in not being shown.\n\n### Why are the changes needed?\n\nTo show the migration guides to end users.\n\n### Does this PR introduce _any_ user-facing change?\n\nIt's not yet released but we should better backport to branch-3.2.\nIt's a followup of the new page in PySpark documentation (branch-3.2).\n\n### How was this patch tested?\n\nManually built the docs\n\nCloses #34195 from HyukjinKwon/SPARK-36939.\n\nAuthored-by: Hyukjin Kwon <gurwls223@apache.org>\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>"", 'distinct': False, 'url': 'https://api.github.com/repos/warrenzhu25/spark/commits/0902b47a7529d036afdaf181d96abdb2fbfb8d77'}, {'sha': 'd99edacb7a5b66581f98b26be6b1a775b794594a', 'author': {'email': 'ueshin@databricks.com', 'name': 'Takuya UESHIN'}, 'message': '[SPARK-36884][PYTHON] Inline type hints for pyspark.sql.session\n\n### What changes were proposed in this pull request?\n\nInline type hints from `python/pyspark/sql/session.pyi` to `python/pyspark/sql/session.py`.\n\n### Why are the changes needed?\n\nCurrently, there is type hint stub files `python/pyspark/sql/session.pyi` to show the expected types for functions, but we can also take advantage of static type checking within the functions by inlining the type hints.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo.\n\n### How was this patch tested?\n\nExisting test.\n\nCloses #34136 from ueshin/issues/SPARK-36884/inline_typehints.\n\nAuthored-by: Takuya UESHIN <ueshin@databricks.com>\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>', 'distinct': False, 'url': 'https://api.github.com/repos/warrenzhu25/spark/commits/d99edacb7a5b66581f98b26be6b1a775b794594a'}, {'sha': '1eb6ea396da83dca5eb3ec50e4c744c77f8fe33e', 'author': {'email': 'adamq43@gmail.com', 'name': 'Adam Binford'}, 'message': ""[SPARK-36918][SQL] Ignore types when comparing structs for unionByName\n\n### What changes were proposed in this pull request?\n\nRather than using `DataType.sameType` for comparing structs when unioning by name, only compare the names and orders of the fields, since all we are doing is determining if we need to recreate the struct in a different order. After ResolveUnion is done, the normal union will handle if the types are incompatible or not.\n\nAdditionally, adds a check to the recursive struct handling as well, so we don't have to recreate a nested struct if only one of its parents or siblings are different.\n\n### Why are the changes needed?\n\nPerformance improvement, especially with nested nullable structs, which get wrapped in an If(IsNull()). Unioning three or more structs can explode the plan due to the multiple structs created extracting values from If(IsNull()) values when the projections are collapsed.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo, just a performance improvement.\n\n### How was this patch tested?\n\nNew unit test for the helper method, and existing tests still pass.\n\nCloses #34166 from Kimahriman/union-by-name-ignore-types.\n\nAuthored-by: Adam Binford <adamq43@gmail.com>\nSigned-off-by: Liang-Chi Hsieh <viirya@gmail.com>"", 'distinct': False, 'url': 'https://api.github.com/repos/warrenzhu25/spark/commits/1eb6ea396da83dca5eb3ec50e4c744c77f8fe33e'}, {'sha': '218da86b8d682ddce3208e0c57b6df7055449130', 'author': {'email': 'dgd_contributor@viettel.com.vn', 'name': 'dch nguyen'}, 'message': '[SPARK-36742][PYTHON] Fix ps.to_datetime with plurals of keys like years, months, days\n\n### What changes were proposed in this pull request?\nFix ps.to_datetime with plurals of keys like years, months, days.\n\n### Why are the changes needed?\nFix ps.to_datetime with plurals of keys like years, months, days\nBefore this PR\n``` python\n# pandas\ndf_test = pd.DataFrame({\'years\': [2015, 2016], \'months\': [2, 3], \'days\': [4, 5]})\ndf_test[\'date\'] = pd.to_datetime(df_test[[\'years\', \'months\', \'days\']])\ndf_test\n\n   years  months  days       date\n0   2015       2     4 2015-02-04\n1   2016       3     5 2016-03-05\n\n# pandas on spark\ndf_test = ps.DataFrame({\'years\': [2015, 2016], \'months\': [2, 3], \'days\': [4, 5]})\ndf_test[\'date\'] = ps.to_datetime(df_test[[\'years\', \'months\', \'days\']])\n\nTraceback (most recent call last):\n  File ""<stdin>"", line 1, in <module>\n  File ""/u02/spark/python/pyspark/pandas/namespace.py"", line 1643, in to_datetime\n    psdf = arg[[""year"", ""month"", ""day""]]\n  File ""/u02/spark/python/pyspark/pandas/frame.py"", line 11888, in __getitem__\n    return self.loc[:, list(key)]\n  File ""/u02/spark/python/pyspark/pandas/indexing.py"", line 480, in __getitem__\n    ) = self._select_cols(cols_sel)\n  File ""/u02/spark/python/pyspark/pandas/indexing.py"", line 325, in _select_cols\n    return self._select_cols_by_iterable(cols_sel, missing_keys)\n  File ""/u02/spark/python/pyspark/pandas/indexing.py"", line 1356, in _select_cols_by_iterable\n    raise KeyError(""[\'{}\'] not in index"".format(name_like_string(key)))\nKeyError: ""[\'year\'] not in index""\n```\n\n### Does this PR introduce _any_ user-facing change?\nAfter this PR :\n``` python\ndf_test = ps.DataFrame({\'years\': [2015, 2016], \'months\': [2, 3], \'days\': [4, 5]})\ndf_test[\'date\'] = ps.to_datetime(df_test[[\'years\', \'months\', \'days\']])\ndf_test\n\n   years  months  days       date\n0   2015       2     4 2015-02-04\n1   2016       3     5 2016-03-05\n```\n\n### How was this patch tested?\nUnit tests\n\nCloses #34182 from dchvn/SPARK-36742.\n\nAuthored-by: dch nguyen <dgd_contributor@viettel.com.vn>\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>', 'distinct': False, 'url': 'https://api.github.com/repos/warrenzhu25/spark/commits/218da86b8d682ddce3208e0c57b6df7055449130'}, {'sha': '4a143f0208f2a1ba9cea1a8b9307a28a78bdf64c', 'author': {'email': 'max.gekk@gmail.com', 'name': 'Max Gekk'}, 'message': '[SPARK-36941][SQL][TESTS] Check saving/loading of ANSI intervals to Hive Parquet table\n\n### What changes were proposed in this pull request?\nIn the PR, I propose to add new test which checks saving/loading of ANSI intervals as columns of a dataframe to/from a table using Hive External catalog and the Parquet datasource.\n\nSince Hive Metastore/Serde doesn\'t support interval types natively, Spark fallbacks to Spark specific format for schema w/ ANSI intervals. And it outputs the warning:\n```\n23:35:46.289 WARN org.apache.spark.sql.hive.test.TestHiveExternalCatalog: Could not persist `default`.`tbl_ansi_intervals` in a Hive compatible way. Persisting it into Hive metastore in Spark SQL specific format.\norg.apache.hadoop.hive.ql.metadata.HiveException: java.lang.IllegalArgumentException: Error: type expected at the position 0 of \'interval year to month:interval day to second\' but \'interval year to month\' is found.\n\tat org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:869)\n```\n\n### Why are the changes needed?\nTo improve test coverage.\n\n### Does this PR introduce _any_ user-facing change?\nNo.\n\n### How was this patch tested?\nBy running new test:\n```\n$ ./build/sbt -Phive-2.3 -Phive-thriftserver ""test:testOnly *HiveParquetSourceSuite""\n```\n\nCloses #34201 from MaxGekk/create-table-ansi-intervals.\n\nAuthored-by: Max Gekk <max.gekk@gmail.com>\nSigned-off-by: Max Gekk <max.gekk@gmail.com>', 'distinct': False, 'url': 'https://api.github.com/repos/warrenzhu25/spark/commits/4a143f0208f2a1ba9cea1a8b9307a28a78bdf64c'}, {'sha': 'b8e7e0d06447b02c30616078145eb01b1186f5db', 'author': {'email': 'dgd_contributor@viettel.com.vn', 'name': 'dch nguyen'}, 'message': '[SPARK-36930][PYTHON][FOLLOW-UP] Fix test case for MultiIndex.dtypes with pandas version < 1.3\n\n### What changes were proposed in this pull request?\n\nThis PR is a minor followup of https://github.com/apache/spark/pull/34179 to fix test case for ```MultiIndex.dtypes``` with pandas version < 1.3.\n\n### Why are the changes needed?\n\nFix test case for ```MultiIndex.dtypes``` with pandas version < 1.3.\n\n### Does this PR introduce _any_ user-facing change?\nNo\n\n### How was this patch tested?\nmanual test\n\nCloses #34206 from dchvn/SPARK-36930-FU.\n\nAuthored-by: dch nguyen <dgd_contributor@viettel.com.vn>\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>', 'distinct': False, 'url': 'https://api.github.com/repos/warrenzhu25/spark/commits/b8e7e0d06447b02c30616078145eb01b1186f5db'}, {'sha': '148dcd9e4ae178126be4c20a9a524430d886dd33', 'author': {'email': 'dgd_contributor@viettel.com.vn', 'name': 'dch nguyen'}, 'message': '[SPARK-36711][PYTHON][FOLLOW-UP] Skip docs tests having MultiIndex.dtypes\n\n### What changes were proposed in this pull request?\n\nThis PR is a minor followup of #34176 to skip docs test which use ```pd.MultiIndex.dtypes``` which is only supported in pandas version 1.3+.\n\n### Why are the changes needed?\nSkip docs test which use ```pd.MultiIndex.dtypes``` which is only supported in pandas version 1.3+.\n\n### Does this PR introduce _any_ user-facing change?\nNo\n\n### How was this patch tested?\nManual test\n\nCloses #34207 from dchvn/SPARK-36711-FU.\n\nAuthored-by: dch nguyen <dgd_contributor@viettel.com.vn>\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>', 'distinct': False, 'url': 'https://api.github.com/repos/warrenzhu25/spark/commits/148dcd9e4ae178126be4c20a9a524430d886dd33'}, {'sha': 'aa393cdafff5c9f20d41133631a4efcdee3ccdc7', 'author': {'email': 'viirya@gmail.com', 'name': 'Liang-Chi Hsieh'}, 'message': '[MINOR][TEST] GzipCodec should be set with Configuration before using\n\n### What changes were proposed in this pull request?\n\nThis is a minor fix to the usage of `GzipCodec` in `WholeTextFileRecordReaderSuite` by setting `Configuration` on it.\n\n### Why are the changes needed?\n\nAs `Configurable` class, `GzipCodec` should be set with `Configuration` before using as an initialization. It has a `conf` member variable and many methods use it to access hadoop configurations. This is how it is used in Hadoop codebase.\n\nWe added an internal Hadoop configuration recently and found `WholeTextFileRecordReaderSuite` failed due to this issue in the test suite.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo.\n\n### How was this patch tested?\n\nExisting tests.\n\nCloses #34209 from viirya/fix-test.\n\nAuthored-by: Liang-Chi Hsieh <viirya@gmail.com>\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>', 'distinct': False, 'url': 'https://api.github.com/repos/warrenzhu25/spark/commits/aa393cdafff5c9f20d41133631a4efcdee3ccdc7'}, {'sha': '2e9b698d3100f18595dadbf35abb06502c3d6123', 'author': {'email': 'gurwls223@apache.org', 'name': 'Hyukjin Kwon'}, 'message': '[SPARK-36713][PYTHON][DOCS] Document new syntax of type hints with index (pandas-on-Spark)\n\n### What changes were proposed in this pull request?\n\nThis PR proposes to document the new syntax of type hints with index. Self-contained.\n\n### Why are the changes needed?\n\nTo guide users about the new ways of typing to avoid creating default index.\n\n### Does this PR introduce _any_ user-facing change?\n\nYes, it adds new sections in the pandas-on-Spark documentation.\n\n### How was this patch tested?\n\nManually built the docs and verified the output HTMLs. Also manually ran the example codes.\n\n![Screen Shot 2021-10-07 at 2 19 41 PM](https://user-images.githubusercontent.com/6477701/136324614-a9eafaa9-79b6-42fb-be65-ac43e12017b7.png)\n\n![Screen Shot 2021-10-07 at 2 19 38 PM](https://user-images.githubusercontent.com/6477701/136324609-8da68d45-259e-441d-9226-b97fe7b0d63f.png)\n\nCloses #34210 from HyukjinKwon/SPARK-36713.\n\nAuthored-by: Hyukjin Kwon <gurwls223@apache.org>\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>', 'distinct': False, 'url': 'https://api.github.com/repos/warrenzhu25/spark/commits/2e9b698d3100f18595dadbf35abb06502c3d6123'}, {'sha': 'd758ae3164933039c9c4766d8085ba3003df7b49', 'author': {'email': 'panchal.harsh18@gmail.com', 'name': 'BOOTMGR'}, 'message': '[SPARK-36798][CORE] Wait for listeners to finish before flushing metrics\n\n### What changes were proposed in this pull request?\nWhen `SparkContext` is shutting down, wait for listener bus to finish and then only flush `MetricsSystem`.\n\n### Why are the changes needed?\nIn current implementation, when `SparkContext.stop()` is called, `metricsSystem.report()` is called before `listenerBus.stop()`. In this case, if some listener is producing some metrics, they would never reach sink.\n\n### Does this PR introduce _any_ user-facing change?\nNo\n\n### How was this patch tested?\nNA\n\nCloses #34039 from BOOTMGR/SPARK-36798.\n\nAuthored-by: BOOTMGR <panchal.harsh18@gmail.com>\nSigned-off-by: Mridul Muralidharan <mridul<at>gmail.com>', 'distinct': False, 'url': 'https://api.github.com/repos/warrenzhu25/spark/commits/d758ae3164933039c9c4766d8085ba3003df7b49'}, {'sha': '255d86f854773c8134dc2c12e1cdcfe12aed55c1', 'author': {'email': 'xinrong.meng@databricks.com', 'name': 'Xinrong Meng'}, 'message': ""[SPARK-36940][PYTHON] Inline type hints for python/pyspark/sql/avro/functions.py\n\n### What changes were proposed in this pull request?\nInline type hints for python/pyspark/sql/avro/functions.py.\n\n### Why are the changes needed?\nCurrently, we use stub files for type annotations, which don't support type checks within function bodies. So we propose to inline the type hints to support that.\n\n### Does this PR introduce _any_ user-facing change?\nNo.\n\n### How was this patch tested?\nExisting tests.\n\nCloses #34200 from xinrong-databricks/inline_avro_func.\n\nAuthored-by: Xinrong Meng <xinrong.meng@databricks.com>\nSigned-off-by: Takuya UESHIN <ueshin@databricks.com>"", 'distinct': False, 'url': 'https://api.github.com/repos/warrenzhu25/spark/commits/255d86f854773c8134dc2c12e1cdcfe12aed55c1'}]}",,Scala
20223148839,PushEvent,warrenzhu25/spark,0.0,2022-02-13T03:14:30Z,"{'push_id': 9086045082, 'size': 1000, 'distinct_size': 21, 'ref': 'refs/heads/input', 'head': 'a5de42381ce129db609007ea07a7097c39056f14', 'before': '67eb28f353ba367cf98a3918002ed49362b83dcc', 'commits': [{'sha': '6e8a4626117f0cb5535875f7181f56350ad4f195', 'author': {'email': 'peng.8lei@gmail.com', 'name': 'PengLei'}, 'message': '[SPARK-36841][SQL] Add ansi syntax `set catalog xxx` to change the current catalog\n\n### What changes were proposed in this pull request?\n1Add the statement of `set catalog xxx` to change the current catalog\n2Retain the `USE` statement to change the current catalog\n3Forcible loading the new catalog when change the new catalog.\n\n### Why are the changes needed?\nAnsi SQL use `SET CATALOG XXX` statement to change the catalog.\n\n[DISCUSS](https://github.com/apache/spark/pull/34030#issuecomment-925936538)\n\n<img width=""521"" alt=""set-catalog"" src=""https://user-images.githubusercontent.com/41178002/134658562-4e4dd879-b6e5-484c-9461-6345c3faaf2e.png"">\n\n### Does this PR introduce _any_ user-facing change?\nYes, User can use `SET CATALOG XXX` to change the current catalog\n\n### How was this patch tested?\nAdd ut testcase\n\nCloses #34096 from Peng-Lei/set-catalog-statement.\n\nAuthored-by: PengLei <peng.8lei@gmail.com>\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>', 'distinct': False, 'url': 'https://api.github.com/repos/warrenzhu25/spark/commits/6e8a4626117f0cb5535875f7181f56350ad4f195'}, {'sha': '3ac0382759d33a46cf5d590c6f3915be7fa75351', 'author': {'email': 'max.gekk@gmail.com', 'name': 'Max Gekk'}, 'message': ""[SPARK-36920][SQL][FOLLOWUP] Fix input types of `ABS()`: numeric and ANSI intervals\n\n### What changes were proposed in this pull request?\nChange allowed input types of `Abs()` from:\n```\nNumericType + CalendarIntervalType + YearMonthIntervalType + DayTimeIntervalType\n```\nto\n```\nNumericType + YearMonthIntervalType + DayTimeIntervalType\n```\n\n### Why are the changes needed?\nThe changes make the error message more clear.\n\nBefore changes:\n```sql\nspark-sql> set spark.sql.legacy.interval.enabled=true;\nspark.sql.legacy.interval.enabled\ttrue\nspark-sql> select abs(interval -10 days -20 minutes);\n21/10/05 09:11:30 ERROR SparkSQLDriver: Failed in [select abs(interval -10 days -20 minutes)]\njava.lang.ClassCastException: org.apache.spark.sql.types.CalendarIntervalType$ cannot be cast to org.apache.spark.sql.types.NumericType\n\tat org.apache.spark.sql.catalyst.util.TypeUtils$.getNumeric(TypeUtils.scala:77)\n\tat org.apache.spark.sql.catalyst.expressions.Abs.numeric$lzycompute(arithmetic.scala:172)\n\tat org.apache.spark.sql.catalyst.expressions.Abs.numeric(arithmetic.scala:169)\n```\n\nAfter:\n```sql\nspark.sql.legacy.interval.enabled\ttrue\nspark-sql> select abs(interval -10 days -20 minutes);\nError in query: cannot resolve 'abs(INTERVAL '-10 days -20 minutes')' due to data type mismatch: argument 1 requires (numeric or interval day to second or interval year to month) type, however, 'INTERVAL '-10 days -20 minutes'' is of interval type.; line 1 pos 7;\n'Project [unresolvedalias(abs(-10 days -20 minutes, false), None)]\n+- OneRowRelation\n```\n\n### Does this PR introduce _any_ user-facing change?\nNo, because the original changes of https://github.com/apache/spark/pull/34169 haven't released yet.\n\n### How was this patch tested?\nManually checked in the command line, see examples above.\n\nCloses #34183 from MaxGekk/fix-abs-input-types.\n\nAuthored-by: Max Gekk <max.gekk@gmail.com>\nSigned-off-by: Max Gekk <max.gekk@gmail.com>"", 'distinct': False, 'url': 'https://api.github.com/repos/warrenzhu25/spark/commits/3ac0382759d33a46cf5d590c6f3915be7fa75351'}, {'sha': 'e5b01cd823990d71c3ff32061c2998a076166ba8', 'author': {'email': 'minyang@minyang-mn3.linkedin.biz', 'name': 'Minchu Yang'}, 'message': '[SPARK-36705][FOLLOW-UP] Support the case when user\'s classes need to register for Kryo serialization\n\n### What changes were proposed in this pull request?\n\n- Make the val lazy wherever `isPushBasedShuffleEnabled` is invoked when it is a class instance variable, so it can happen after user-defined jars/classes in `spark.kryo.classesToRegister` are downloaded and available on executor-side, as part of the fix for the exception mentioned below.\n\n- Add a flag `checkSerializer` to control whether we need to check a serializer is `supportsRelocationOfSerializedObjects` or not within `isPushBasedShuffleEnabled` as part of the fix for the exception mentioned below. Specifically, we don\'t check this in `registerWithExternalShuffleServer()` in `BlockManager` and `createLocalDirsForMergedShuffleBlocks()` in `DiskBlockManager.scala` as the same issue would raise otherwise.\n\n- Move `instantiateClassFromConf` and `instantiateClass` from `SparkEnv` into `Utils`, in order to let `isPushBasedShuffleEnabled` to leverage them for instantiating serializer instances.\n\n### Why are the changes needed?\n\nWhen user tries to set classes for Kryo Serialization by `spark.kryo.classesToRegister`, below exception(or similar) would be encountered in `isPushBasedShuffleEnabled` as indicated below.\nReproduced the issue in our internal branch by launching spark-shell as:\n```\nspark-shell --spark-version 3.1.1 --packages ml.dmlc:xgboost4j_2.12:1.3.1 --conf spark.kryo.classesToRegister=ml.dmlc.xgboost4j.scala.Booster\n```\n\n```\nException in thread ""main"" java.lang.reflect.UndeclaredThrowableException\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1911)\n\tat org.apache.spark.deploy.SparkHadoopUtil.runAsSparkUser(SparkHadoopUtil.scala:61)\n\tat org.apache.spark.executor.CoarseGrainedExecutorBackend$.run(CoarseGrainedExecutorBackend.scala:393)\n\tat org.apache.spark.executor.YarnCoarseGrainedExecutorBackend$.main(YarnCoarseGrainedExecutorBackend.scala:83)\n\tat org.apache.spark.executor.YarnCoarseGrainedExecutorBackend.main(YarnCoarseGrainedExecutorBackend.scala)\nCaused by: org.apache.spark.SparkException: Failed to register classes with Kryo\n\tat org.apache.spark.serializer.KryoSerializer.$anonfun$newKryo$5(KryoSerializer.scala:183)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.util.Utils$.withContextClassLoader(Utils.scala:230)\n\tat org.apache.spark.serializer.KryoSerializer.newKryo(KryoSerializer.scala:171)\n\tat org.apache.spark.serializer.KryoSerializer$$anon$1.create(KryoSerializer.scala:102)\n\tat com.esotericsoftware.kryo.pool.KryoPoolQueueImpl.borrow(KryoPoolQueueImpl.java:48)\n\tat org.apache.spark.serializer.KryoSerializer$PoolWrapper.borrow(KryoSerializer.scala:109)\n\tat org.apache.spark.serializer.KryoSerializerInstance.borrowKryo(KryoSerializer.scala:346)\n\tat org.apache.spark.serializer.KryoSerializerInstance.getAutoReset(KryoSerializer.scala:446)\n\tat org.apache.spark.serializer.KryoSerializer.supportsRelocationOfSerializedObjects$lzycompute(KryoSerializer.scala:253)\n\tat org.apache.spark.serializer.KryoSerializer.supportsRelocationOfSerializedObjects(KryoSerializer.scala:249)\n\tat org.apache.spark.util.Utils$.isPushBasedShuffleEnabled(Utils.scala:2584)\n\tat org.apache.spark.MapOutputTrackerWorker.<init>(MapOutputTracker.scala:1109)\n\tat org.apache.spark.SparkEnv$.create(SparkEnv.scala:322)\n\tat org.apache.spark.SparkEnv$.createExecutorEnv(SparkEnv.scala:205)\n\tat org.apache.spark.executor.CoarseGrainedExecutorBackend$.$anonfun$run$7(CoarseGrainedExecutorBackend.scala:442)\n\tat org.apache.spark.deploy.SparkHadoopUtil$$anon$1.run(SparkHadoopUtil.scala:62)\n\tat org.apache.spark.deploy.SparkHadoopUtil$$anon$1.run(SparkHadoopUtil.scala:61)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1893)\n\t... 4 more\nCaused by: java.lang.ClassNotFoundException: ml.dmlc.xgboost4j.scala.Booster\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:381)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:424)\n\tat sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:357)\n\tat java.lang.Class.forName0(Native Method)\n\tat java.lang.Class.forName(Class.java:348)\n\tat org.apache.spark.util.Utils$.classForName(Utils.scala:217)\n\tat org.apache.spark.serializer.KryoSerializer.$anonfun$newKryo$6(KryoSerializer.scala:174)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.serializer.KryoSerializer.$anonfun$newKryo$5(KryoSerializer.scala:173)\n\t... 24 more\n```\nRegistering user class for kryo serialization is happening after serializer creation in SparkEnv. Serializer creation can happen in `isPushBasedShuffleEnabled`, which can be called in some places prior to SparkEnv is created. Also, as per analysis by JoshRosen, this is probably due to Kryo instantiation was failing because added packages hadn\'t been downloaded to the executor yet (because this code is running during executor startup, not task startup). The proposed change helps fix this issue.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo.\n\n### How was this patch tested?\n\nPassed existing tests.\nTested this patch in our internal branch where user reported the issue. Issue is now not reproducible with this patch.\n\nCloses #34158 from rmcyang/SPARK-33781-bugFix.\n\nLead-authored-by: Minchu Yang <minyang@minyang-mn3.linkedin.biz>\nCo-authored-by: Minchu Yang <31781684+rmcyang@users.noreply.github.com>\nSigned-off-by: Mridul Muralidharan <mridul<at>gmail.com>', 'distinct': False, 'url': 'https://api.github.com/repos/warrenzhu25/spark/commits/e5b01cd823990d71c3ff32061c2998a076166ba8'}, {'sha': '6a2452fb5cd776dc1f292704e6b86bbec0ff24e7', 'author': {'email': 'gurwls223@apache.org', 'name': 'Hyukjin Kwon'}, 'message': '[SPARK-36711][PYTHON][FOLLOW-UP] Refactor typing logic for multi-index support\n\n### What changes were proposed in this pull request?\n\nThis PR proposes to refactor typing logic for multi-index support that was mostly introduced in https://github.com/apache/spark/pull/34176\n\nAt a high level, the below functions were introduced\n\n```bash\n_extract_types # renamed from `extract_types`\n```\n\n```\n_is_named_params\n_address_named_type_hoders\n_to_tuple_of_params\n_convert_tuples_to_zip\n_address_unnamed_type_holders\n```\n\nIn this PR, they become as below with simplification:\n\n```bash\n_to_type_holders # renamed from `_extract_types`\n```\n\n```bash\n_new_type_holders # merged from `_is_named_params`, etc.\n```\n\n### Why are the changes needed?\n\nTo make the codes easier to read.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo, dev-only.\n\n### How was this patch tested?\n\nExisting tests should cover them.\n\nCloses #34181 from HyukjinKwon/SPARK-36711.\n\nAuthored-by: Hyukjin Kwon <gurwls223@apache.org>\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>', 'distinct': False, 'url': 'https://api.github.com/repos/warrenzhu25/spark/commits/6a2452fb5cd776dc1f292704e6b86bbec0ff24e7'}, {'sha': '5c6f0b9263f29f805f386237448f671dea3ad6c5', 'author': {'email': 'dgd_contributor@viettel.com.vn', 'name': 'dchvn nguyen'}, 'message': '[SPARK-36930][PYTHON] Support ps.MultiIndex.dtypes\n\n### What changes were proposed in this pull request?\nAdd dtypes for MultiIndex\n\n### Why are the changes needed?\nAdd dtypes for MultiIndex\n\nBefore this PR:\n\n```python\n>>> idx = pd.MultiIndex.from_arrays([[0, 1, 2, 3, 4, 5, 6, 7, 8], [1, 2, 3, 4, 5, 6, 7, 8, 9]], names=(""zero"", ""one""))\n>>> pdf = pd.DataFrame(\n...     {""a"": [1, 2, 3, 4, 5, 6, 7, 8, 9], ""b"": [4, 5, 6, 3, 2, 1, 0, 0, 0]},\n...     index=idx,\n... )\n>>> psdf = ps.from_pandas(pdf)\n>>>\n>>> ps.DataFrame[psdf.index.dtypes, psdf.dtypes]\nTraceback (most recent call last):\n  File ""<stdin>"", line 1, in <module>\n  File ""/u02/spark/python/pyspark/pandas/indexes/multi.py"", line 917, in __getattr__\n    raise AttributeError(""\'MultiIndex\' object has no attribute \'{}\'"".format(item))\nAttributeError: \'MultiIndex\' object has no attribute \'dtypes\'\n>>>\n```\n\n### Does this PR introduce _any_ user-facing change?\nAfter this PR user can use ```MultiIndex.dtypes``` for:\n\n``` python\n>>> ps.DataFrame[psdf.index.dtypes, psdf.dtypes]\ntyping.Tuple[pyspark.pandas.typedef.typehints.IndexNameType, pyspark.pandas.typedef.typehints.IndexNameType, pyspark.pandas.typedef.typehints.NameType, pyspark.pandas.typedef.typehints.NameType]\n```\n\n### How was this patch tested?\nunit tests.\n\nCloses #34179 from dchvn/add_multiindex_dtypes.\n\nLead-authored-by: dchvn nguyen <dgd_contributor@viettel.com.vn>\nCo-authored-by: dch nguyen <dgd_contributor@viettel.com.vn>\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>', 'distinct': False, 'url': 'https://api.github.com/repos/warrenzhu25/spark/commits/5c6f0b9263f29f805f386237448f671dea3ad6c5'}, {'sha': '31b6f614d3173c8a5852243bf7d0b6200788432d', 'author': {'email': 'yezhou@linkedin.com', 'name': 'Ye Zhou'}, 'message': '[SPARK-36892][CORE] Disable batch fetch for a shuffle when push based shuffle is enabled\n\nWe found an issue where user configured both AQE and push based shuffle, but the job started to hang after running some  stages. We took the thread dump from the Executors, which showed the task is still waiting to fetch shuffle blocks.\nProposed changes in the PR to fix the issue.\n\n### What changes were proposed in this pull request?\nDisabled Batch fetch when push based shuffle is enabled.\n\n### Why are the changes needed?\nWithout this patch, enabling AQE and Push based shuffle will have a chance to hang the tasks.\n\n### Does this PR introduce _any_ user-facing change?\nNo\n\n### How was this patch tested?\nTested the PR within our PR, with Spark shell and the queries are:\n\nsql(""""""SELECT CASE WHEN rand() < 0.8 THEN 100 ELSE CAST(rand() * 30000000 AS INT) END AS s_item_id, CAST(rand() * 100 AS INT) AS s_quantity, DATE_ADD(current_date(), - CAST(rand() * 360 AS INT)) AS s_date FROM RANGE(1000000000)"""""").createOrReplaceTempView(""sales"")\n// Dynamically coalesce partitions\nsql(""""""SELECT s_date, sum(s_quantity) AS q FROM sales GROUP BY s_date ORDER BY q DESC"""""").collect\n\nUnit tests to be added.\n\nCloses #34156 from zhouyejoe/SPARK-36892.\n\nAuthored-by: Ye Zhou <yezhou@linkedin.com>\nSigned-off-by: Gengliang Wang <gengliang@apache.org>', 'distinct': False, 'url': 'https://api.github.com/repos/warrenzhu25/spark/commits/31b6f614d3173c8a5852243bf7d0b6200788432d'}, {'sha': 'f6013c8ce5b4efccd64cd900c694e40612c81d9c', 'author': {'email': 'xinrong.meng@databricks.com', 'name': 'Xinrong Meng'}, 'message': '[SPARK-36927][PYTHON] Inline type hints for python/pyspark/sql/window.py\n\n### What changes were proposed in this pull request?\nInline type hints for python/pyspark/sql/window.py\n\n### Why are the changes needed?\nCurrently, stub files are used for type hints. However, statements within functions cannot be type-checked.\nThe PR is proposed to inline type hints for python/pyspark/sql/window.py to type check statements within functions.\n\n### Does this PR introduce _any_ user-facing change?\nNo.\n\n### How was this patch tested?\nExisting tests.\n\nCloses #34173 from xinrong-databricks/inline_window.\n\nAuthored-by: Xinrong Meng <xinrong.meng@databricks.com>\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>', 'distinct': False, 'url': 'https://api.github.com/repos/warrenzhu25/spark/commits/f6013c8ce5b4efccd64cd900c694e40612c81d9c'}, {'sha': 'aed977c4682b6f378a26050ffab51b9b2075cae4', 'author': {'email': 'adrianhu96@gmail.com', 'name': 'tianhanhu'}, 'message': '[SPARK-36919][SQL] Make BadRecordException fields transient\n\n### What changes were proposed in this pull request?\nMigrating a Spark application from 2.4.x to 3.1.x and finding a difference in the exception chaining behavior. In a case of parsing a malformed CSV, where the root cause exception should be Caused by: java.lang.RuntimeException: Malformed CSV record, only the top level exception is kept, and all lower level exceptions and root cause are lost. Thus, when we call ExceptionUtils.getRootCause on the exception, we still get itself.\nThe reason for the difference is that RuntimeException is wrapped in BadRecordException, which has unserializable fields. When we try to serialize the exception from tasks and deserialize from scheduler, the exception is lost.\nThis PR makes unserializable fields of BadRecordException transient, so the rest of the exception could be serialized and deserialized properly.\n\n### Why are the changes needed?\nMake BadRecordException serializable\n\n### Does this PR introduce _any_ user-facing change?\nUser could get root cause of BadRecordException\n\n### How was this patch tested?\nUnit testing\n\nCloses #34167 from tianhanhu/master.\n\nAuthored-by: tianhanhu <adrianhu96@gmail.com>\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>', 'distinct': False, 'url': 'https://api.github.com/repos/warrenzhu25/spark/commits/aed977c4682b6f378a26050ffab51b9b2075cae4'}, {'sha': '2953d4f5005705d0e08f457156fcfe4e7e4ffa17', 'author': {'email': 'gurwls223@apache.org', 'name': 'Hyukjin Kwon'}, 'message': '[SPARK-36751][PYTHON][DOCS][FOLLOW-UP] Fix unexpected section title for Examples in docstring\n\n### What changes were proposed in this pull request?\n\nThis PR is a minor followup of https://github.com/apache/spark/pull/33992 to fix the warnings during PySpark documentation build:\n\n```\n/.../spark/python/pyspark/sql/functions.py:docstring of pyspark.sql.functions.bit_length:19: WARNING: Unexpected section title or transition.\n\n-------\n/.../spark/python/pyspark/sql/functions.py:docstring of pyspark.sql.functions.octet_length:19: WARNING: Unexpected section title or transition.\n\n-------\n```\n\nWe should always have the same length of hyphens with the title.\n\n### Why are the changes needed?\n\nTo remove warnings during the documentation build and show the HTML pages correctly.\n\n### Does this PR introduce _any_ user-facing change?\n\nThis is not released yet, and only in master branch. So, no to end users.\n\n### How was this patch tested?\n\nManually built the docs via `make clean html` at `python/docs` directory.\n\nCloses #34196 from HyukjinKwon/SPARK-36751.\n\nAuthored-by: Hyukjin Kwon <gurwls223@apache.org>\nSigned-off-by: Kousuke Saruta <sarutak@oss.nttdata.com>', 'distinct': False, 'url': 'https://api.github.com/repos/warrenzhu25/spark/commits/2953d4f5005705d0e08f457156fcfe4e7e4ffa17'}, {'sha': '090c9bf73843c4741fcdaa26a7b3b63b9e2e01b8', 'author': {'email': 'sarutak@oss.nttdata.com', 'name': 'Kousuke Saruta'}, 'message': '[SPARK-36937][SQL][TESTS] Change OrcSourceSuite to test both V1 and V2 sources\n\n### What changes were proposed in this pull request?\n\nThis PR changes the existing `OrcSourceSuite` as abstract class and adds `OrcSource{V1,V2}Suite` to test both V1 and V2 ORC sources.\n\n### Why are the changes needed?\n\nFor the better test coverage.\nThere is no V2 test for the ORC source which implements `CommonFileDataSourceSuite` while the corresponding ones exist for all other built-in file-based datasources.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo.\n\n### How was this patch tested?\n\nExisting tests.\n\nCloses #34194 from sarutak/restructure-OrcSourceSuite.\n\nAuthored-by: Kousuke Saruta <sarutak@oss.nttdata.com>\nSigned-off-by: Max Gekk <max.gekk@gmail.com>', 'distinct': False, 'url': 'https://api.github.com/repos/warrenzhu25/spark/commits/090c9bf73843c4741fcdaa26a7b3b63b9e2e01b8'}, {'sha': '0902b47a7529d036afdaf181d96abdb2fbfb8d77', 'author': {'email': 'gurwls223@apache.org', 'name': 'Hyukjin Kwon'}, 'message': ""[SPARK-36939][PYTHON][DOCS] Add orphan migration page into list in PySpark documentation\n\n### What changes were proposed in this pull request?\n\nThis PR fixes the warning below during PySpark documentation build:\n\n```\nchecking consistency... /.../spark/python/docs/source/migration_guide/pyspark_3.2_to_3.3.rst: WARNING: document isn't included in any toctree\ndone\n```\n\nSPARK-36618 added a new migration guide page but that's mistakenly not added to `spark/python/docs/source/migration_guideindex.rst` resulting in not being shown.\n\n### Why are the changes needed?\n\nTo show the migration guides to end users.\n\n### Does this PR introduce _any_ user-facing change?\n\nIt's not yet released but we should better backport to branch-3.2.\nIt's a followup of the new page in PySpark documentation (branch-3.2).\n\n### How was this patch tested?\n\nManually built the docs\n\nCloses #34195 from HyukjinKwon/SPARK-36939.\n\nAuthored-by: Hyukjin Kwon <gurwls223@apache.org>\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>"", 'distinct': False, 'url': 'https://api.github.com/repos/warrenzhu25/spark/commits/0902b47a7529d036afdaf181d96abdb2fbfb8d77'}, {'sha': 'd99edacb7a5b66581f98b26be6b1a775b794594a', 'author': {'email': 'ueshin@databricks.com', 'name': 'Takuya UESHIN'}, 'message': '[SPARK-36884][PYTHON] Inline type hints for pyspark.sql.session\n\n### What changes were proposed in this pull request?\n\nInline type hints from `python/pyspark/sql/session.pyi` to `python/pyspark/sql/session.py`.\n\n### Why are the changes needed?\n\nCurrently, there is type hint stub files `python/pyspark/sql/session.pyi` to show the expected types for functions, but we can also take advantage of static type checking within the functions by inlining the type hints.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo.\n\n### How was this patch tested?\n\nExisting test.\n\nCloses #34136 from ueshin/issues/SPARK-36884/inline_typehints.\n\nAuthored-by: Takuya UESHIN <ueshin@databricks.com>\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>', 'distinct': False, 'url': 'https://api.github.com/repos/warrenzhu25/spark/commits/d99edacb7a5b66581f98b26be6b1a775b794594a'}, {'sha': '1eb6ea396da83dca5eb3ec50e4c744c77f8fe33e', 'author': {'email': 'adamq43@gmail.com', 'name': 'Adam Binford'}, 'message': ""[SPARK-36918][SQL] Ignore types when comparing structs for unionByName\n\n### What changes were proposed in this pull request?\n\nRather than using `DataType.sameType` for comparing structs when unioning by name, only compare the names and orders of the fields, since all we are doing is determining if we need to recreate the struct in a different order. After ResolveUnion is done, the normal union will handle if the types are incompatible or not.\n\nAdditionally, adds a check to the recursive struct handling as well, so we don't have to recreate a nested struct if only one of its parents or siblings are different.\n\n### Why are the changes needed?\n\nPerformance improvement, especially with nested nullable structs, which get wrapped in an If(IsNull()). Unioning three or more structs can explode the plan due to the multiple structs created extracting values from If(IsNull()) values when the projections are collapsed.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo, just a performance improvement.\n\n### How was this patch tested?\n\nNew unit test for the helper method, and existing tests still pass.\n\nCloses #34166 from Kimahriman/union-by-name-ignore-types.\n\nAuthored-by: Adam Binford <adamq43@gmail.com>\nSigned-off-by: Liang-Chi Hsieh <viirya@gmail.com>"", 'distinct': False, 'url': 'https://api.github.com/repos/warrenzhu25/spark/commits/1eb6ea396da83dca5eb3ec50e4c744c77f8fe33e'}, {'sha': '218da86b8d682ddce3208e0c57b6df7055449130', 'author': {'email': 'dgd_contributor@viettel.com.vn', 'name': 'dch nguyen'}, 'message': '[SPARK-36742][PYTHON] Fix ps.to_datetime with plurals of keys like years, months, days\n\n### What changes were proposed in this pull request?\nFix ps.to_datetime with plurals of keys like years, months, days.\n\n### Why are the changes needed?\nFix ps.to_datetime with plurals of keys like years, months, days\nBefore this PR\n``` python\n# pandas\ndf_test = pd.DataFrame({\'years\': [2015, 2016], \'months\': [2, 3], \'days\': [4, 5]})\ndf_test[\'date\'] = pd.to_datetime(df_test[[\'years\', \'months\', \'days\']])\ndf_test\n\n   years  months  days       date\n0   2015       2     4 2015-02-04\n1   2016       3     5 2016-03-05\n\n# pandas on spark\ndf_test = ps.DataFrame({\'years\': [2015, 2016], \'months\': [2, 3], \'days\': [4, 5]})\ndf_test[\'date\'] = ps.to_datetime(df_test[[\'years\', \'months\', \'days\']])\n\nTraceback (most recent call last):\n  File ""<stdin>"", line 1, in <module>\n  File ""/u02/spark/python/pyspark/pandas/namespace.py"", line 1643, in to_datetime\n    psdf = arg[[""year"", ""month"", ""day""]]\n  File ""/u02/spark/python/pyspark/pandas/frame.py"", line 11888, in __getitem__\n    return self.loc[:, list(key)]\n  File ""/u02/spark/python/pyspark/pandas/indexing.py"", line 480, in __getitem__\n    ) = self._select_cols(cols_sel)\n  File ""/u02/spark/python/pyspark/pandas/indexing.py"", line 325, in _select_cols\n    return self._select_cols_by_iterable(cols_sel, missing_keys)\n  File ""/u02/spark/python/pyspark/pandas/indexing.py"", line 1356, in _select_cols_by_iterable\n    raise KeyError(""[\'{}\'] not in index"".format(name_like_string(key)))\nKeyError: ""[\'year\'] not in index""\n```\n\n### Does this PR introduce _any_ user-facing change?\nAfter this PR :\n``` python\ndf_test = ps.DataFrame({\'years\': [2015, 2016], \'months\': [2, 3], \'days\': [4, 5]})\ndf_test[\'date\'] = ps.to_datetime(df_test[[\'years\', \'months\', \'days\']])\ndf_test\n\n   years  months  days       date\n0   2015       2     4 2015-02-04\n1   2016       3     5 2016-03-05\n```\n\n### How was this patch tested?\nUnit tests\n\nCloses #34182 from dchvn/SPARK-36742.\n\nAuthored-by: dch nguyen <dgd_contributor@viettel.com.vn>\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>', 'distinct': False, 'url': 'https://api.github.com/repos/warrenzhu25/spark/commits/218da86b8d682ddce3208e0c57b6df7055449130'}, {'sha': '4a143f0208f2a1ba9cea1a8b9307a28a78bdf64c', 'author': {'email': 'max.gekk@gmail.com', 'name': 'Max Gekk'}, 'message': '[SPARK-36941][SQL][TESTS] Check saving/loading of ANSI intervals to Hive Parquet table\n\n### What changes were proposed in this pull request?\nIn the PR, I propose to add new test which checks saving/loading of ANSI intervals as columns of a dataframe to/from a table using Hive External catalog and the Parquet datasource.\n\nSince Hive Metastore/Serde doesn\'t support interval types natively, Spark fallbacks to Spark specific format for schema w/ ANSI intervals. And it outputs the warning:\n```\n23:35:46.289 WARN org.apache.spark.sql.hive.test.TestHiveExternalCatalog: Could not persist `default`.`tbl_ansi_intervals` in a Hive compatible way. Persisting it into Hive metastore in Spark SQL specific format.\norg.apache.hadoop.hive.ql.metadata.HiveException: java.lang.IllegalArgumentException: Error: type expected at the position 0 of \'interval year to month:interval day to second\' but \'interval year to month\' is found.\n\tat org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:869)\n```\n\n### Why are the changes needed?\nTo improve test coverage.\n\n### Does this PR introduce _any_ user-facing change?\nNo.\n\n### How was this patch tested?\nBy running new test:\n```\n$ ./build/sbt -Phive-2.3 -Phive-thriftserver ""test:testOnly *HiveParquetSourceSuite""\n```\n\nCloses #34201 from MaxGekk/create-table-ansi-intervals.\n\nAuthored-by: Max Gekk <max.gekk@gmail.com>\nSigned-off-by: Max Gekk <max.gekk@gmail.com>', 'distinct': False, 'url': 'https://api.github.com/repos/warrenzhu25/spark/commits/4a143f0208f2a1ba9cea1a8b9307a28a78bdf64c'}, {'sha': 'b8e7e0d06447b02c30616078145eb01b1186f5db', 'author': {'email': 'dgd_contributor@viettel.com.vn', 'name': 'dch nguyen'}, 'message': '[SPARK-36930][PYTHON][FOLLOW-UP] Fix test case for MultiIndex.dtypes with pandas version < 1.3\n\n### What changes were proposed in this pull request?\n\nThis PR is a minor followup of https://github.com/apache/spark/pull/34179 to fix test case for ```MultiIndex.dtypes``` with pandas version < 1.3.\n\n### Why are the changes needed?\n\nFix test case for ```MultiIndex.dtypes``` with pandas version < 1.3.\n\n### Does this PR introduce _any_ user-facing change?\nNo\n\n### How was this patch tested?\nmanual test\n\nCloses #34206 from dchvn/SPARK-36930-FU.\n\nAuthored-by: dch nguyen <dgd_contributor@viettel.com.vn>\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>', 'distinct': False, 'url': 'https://api.github.com/repos/warrenzhu25/spark/commits/b8e7e0d06447b02c30616078145eb01b1186f5db'}, {'sha': '148dcd9e4ae178126be4c20a9a524430d886dd33', 'author': {'email': 'dgd_contributor@viettel.com.vn', 'name': 'dch nguyen'}, 'message': '[SPARK-36711][PYTHON][FOLLOW-UP] Skip docs tests having MultiIndex.dtypes\n\n### What changes were proposed in this pull request?\n\nThis PR is a minor followup of #34176 to skip docs test which use ```pd.MultiIndex.dtypes``` which is only supported in pandas version 1.3+.\n\n### Why are the changes needed?\nSkip docs test which use ```pd.MultiIndex.dtypes``` which is only supported in pandas version 1.3+.\n\n### Does this PR introduce _any_ user-facing change?\nNo\n\n### How was this patch tested?\nManual test\n\nCloses #34207 from dchvn/SPARK-36711-FU.\n\nAuthored-by: dch nguyen <dgd_contributor@viettel.com.vn>\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>', 'distinct': False, 'url': 'https://api.github.com/repos/warrenzhu25/spark/commits/148dcd9e4ae178126be4c20a9a524430d886dd33'}, {'sha': 'aa393cdafff5c9f20d41133631a4efcdee3ccdc7', 'author': {'email': 'viirya@gmail.com', 'name': 'Liang-Chi Hsieh'}, 'message': '[MINOR][TEST] GzipCodec should be set with Configuration before using\n\n### What changes were proposed in this pull request?\n\nThis is a minor fix to the usage of `GzipCodec` in `WholeTextFileRecordReaderSuite` by setting `Configuration` on it.\n\n### Why are the changes needed?\n\nAs `Configurable` class, `GzipCodec` should be set with `Configuration` before using as an initialization. It has a `conf` member variable and many methods use it to access hadoop configurations. This is how it is used in Hadoop codebase.\n\nWe added an internal Hadoop configuration recently and found `WholeTextFileRecordReaderSuite` failed due to this issue in the test suite.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo.\n\n### How was this patch tested?\n\nExisting tests.\n\nCloses #34209 from viirya/fix-test.\n\nAuthored-by: Liang-Chi Hsieh <viirya@gmail.com>\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>', 'distinct': False, 'url': 'https://api.github.com/repos/warrenzhu25/spark/commits/aa393cdafff5c9f20d41133631a4efcdee3ccdc7'}, {'sha': '2e9b698d3100f18595dadbf35abb06502c3d6123', 'author': {'email': 'gurwls223@apache.org', 'name': 'Hyukjin Kwon'}, 'message': '[SPARK-36713][PYTHON][DOCS] Document new syntax of type hints with index (pandas-on-Spark)\n\n### What changes were proposed in this pull request?\n\nThis PR proposes to document the new syntax of type hints with index. Self-contained.\n\n### Why are the changes needed?\n\nTo guide users about the new ways of typing to avoid creating default index.\n\n### Does this PR introduce _any_ user-facing change?\n\nYes, it adds new sections in the pandas-on-Spark documentation.\n\n### How was this patch tested?\n\nManually built the docs and verified the output HTMLs. Also manually ran the example codes.\n\n![Screen Shot 2021-10-07 at 2 19 41 PM](https://user-images.githubusercontent.com/6477701/136324614-a9eafaa9-79b6-42fb-be65-ac43e12017b7.png)\n\n![Screen Shot 2021-10-07 at 2 19 38 PM](https://user-images.githubusercontent.com/6477701/136324609-8da68d45-259e-441d-9226-b97fe7b0d63f.png)\n\nCloses #34210 from HyukjinKwon/SPARK-36713.\n\nAuthored-by: Hyukjin Kwon <gurwls223@apache.org>\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>', 'distinct': False, 'url': 'https://api.github.com/repos/warrenzhu25/spark/commits/2e9b698d3100f18595dadbf35abb06502c3d6123'}, {'sha': 'd758ae3164933039c9c4766d8085ba3003df7b49', 'author': {'email': 'panchal.harsh18@gmail.com', 'name': 'BOOTMGR'}, 'message': '[SPARK-36798][CORE] Wait for listeners to finish before flushing metrics\n\n### What changes were proposed in this pull request?\nWhen `SparkContext` is shutting down, wait for listener bus to finish and then only flush `MetricsSystem`.\n\n### Why are the changes needed?\nIn current implementation, when `SparkContext.stop()` is called, `metricsSystem.report()` is called before `listenerBus.stop()`. In this case, if some listener is producing some metrics, they would never reach sink.\n\n### Does this PR introduce _any_ user-facing change?\nNo\n\n### How was this patch tested?\nNA\n\nCloses #34039 from BOOTMGR/SPARK-36798.\n\nAuthored-by: BOOTMGR <panchal.harsh18@gmail.com>\nSigned-off-by: Mridul Muralidharan <mridul<at>gmail.com>', 'distinct': False, 'url': 'https://api.github.com/repos/warrenzhu25/spark/commits/d758ae3164933039c9c4766d8085ba3003df7b49'}]}",,Scala
20223140547,PullRequestEvent,apache/spark,0.0,2022-02-13T03:13:33Z,"{'action': 'opened', 'number': 35498, 'pull_request': {'url': 'https://api.github.com/repos/apache/spark/pulls/35498', 'id': 849964721, 'node_id': 'PR_kwDOAQXtWs4yqW6x', 'html_url': 'https://github.com/apache/spark/pull/35498', 'diff_url': 'https://github.com/apache/spark/pull/35498.diff', 'patch_url': 'https://github.com/apache/spark/pull/35498.patch', 'issue_url': 'https://api.github.com/repos/apache/spark/issues/35498', 'number': 35498, 'state': 'open', 'locked': False, 'title': '[SPARK-34777][UI] StagePage input/output size records not show when r', 'user': {'login': 'warrenzhu25', 'id': 1633312, 'node_id': 'MDQ6VXNlcjE2MzMzMTI=', 'avatar_url': 'https://avatars.githubusercontent.com/u/1633312?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/warrenzhu25', 'html_url': 'https://github.com/warrenzhu25', 'followers_url': 'https://api.github.com/users/warrenzhu25/followers', 'following_url': 'https://api.github.com/users/warrenzhu25/following{/other_user}', 'gists_url': 'https://api.github.com/users/warrenzhu25/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/warrenzhu25/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/warrenzhu25/subscriptions', 'organizations_url': 'https://api.github.com/users/warrenzhu25/orgs', 'repos_url': 'https://api.github.com/users/warrenzhu25/repos', 'events_url': 'https://api.github.com/users/warrenzhu25/events{/privacy}', 'received_events_url': 'https://api.github.com/users/warrenzhu25/received_events', 'type': 'User', 'site_admin': False}, 'body': '### What changes were proposed in this pull request?\r\nDetermine whether show input/output size and records based on either has value, rather than only size.\r\n\r\nBefore:\r\n![image](https://user-images.githubusercontent.com/1633312/113449257-48d03b00-93b2-11eb-9f48-f473ca51cbf3.png)\r\n\r\nAfter:\r\n![image](https://user-images.githubusercontent.com/1633312/113449020-ce072000-93b1-11eb-9ecb-bf568f5ace48.png)\r\n\r\n\r\n### Why are the changes needed?\r\nStage page UI not show input/output size and records even when records greater than zero. This is common when data source only have records metrics updated.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nNo\r\n\r\n### How was this patch tested?\r\nManually\r\n', 'created_at': '2022-02-13T03:13:33Z', 'updated_at': '2022-02-13T03:13:33Z', 'closed_at': None, 'merged_at': None, 'merge_commit_sha': None, 'assignee': None, 'assignees': [], 'requested_reviewers': [], 'requested_teams': [], 'labels': [], 'milestone': None, 'draft': False, 'commits_url': 'https://api.github.com/repos/apache/spark/pulls/35498/commits', 'review_comments_url': 'https://api.github.com/repos/apache/spark/pulls/35498/comments', 'review_comment_url': 'https://api.github.com/repos/apache/spark/pulls/comments{/number}', 'comments_url': 'https://api.github.com/repos/apache/spark/issues/35498/comments', 'statuses_url': 'https://api.github.com/repos/apache/spark/statuses/67eb28f353ba367cf98a3918002ed49362b83dcc', 'head': {'label': 'warrenzhu25:input', 'ref': 'input', 'sha': '67eb28f353ba367cf98a3918002ed49362b83dcc', 'user': {'login': 'warrenzhu25', 'id': 1633312, 'node_id': 'MDQ6VXNlcjE2MzMzMTI=', 'avatar_url': 'https://avatars.githubusercontent.com/u/1633312?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/warrenzhu25', 'html_url': 'https://github.com/warrenzhu25', 'followers_url': 'https://api.github.com/users/warrenzhu25/followers', 'following_url': 'https://api.github.com/users/warrenzhu25/following{/other_user}', 'gists_url': 'https://api.github.com/users/warrenzhu25/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/warrenzhu25/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/warrenzhu25/subscriptions', 'organizations_url': 'https://api.github.com/users/warrenzhu25/orgs', 'repos_url': 'https://api.github.com/users/warrenzhu25/repos', 'events_url': 'https://api.github.com/users/warrenzhu25/events{/privacy}', 'received_events_url': 'https://api.github.com/users/warrenzhu25/received_events', 'type': 'User', 'site_admin': False}, 'repo': {'id': 217627454, 'node_id': 'MDEwOlJlcG9zaXRvcnkyMTc2Mjc0NTQ=', 'name': 'spark', 'full_name': 'warrenzhu25/spark', 'private': False, 'owner': {'login': 'warrenzhu25', 'id': 1633312, 'node_id': 'MDQ6VXNlcjE2MzMzMTI=', 'avatar_url': 'https://avatars.githubusercontent.com/u/1633312?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/warrenzhu25', 'html_url': 'https://github.com/warrenzhu25', 'followers_url': 'https://api.github.com/users/warrenzhu25/followers', 'following_url': 'https://api.github.com/users/warrenzhu25/following{/other_user}', 'gists_url': 'https://api.github.com/users/warrenzhu25/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/warrenzhu25/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/warrenzhu25/subscriptions', 'organizations_url': 'https://api.github.com/users/warrenzhu25/orgs', 'repos_url': 'https://api.github.com/users/warrenzhu25/repos', 'events_url': 'https://api.github.com/users/warrenzhu25/events{/privacy}', 'received_events_url': 'https://api.github.com/users/warrenzhu25/received_events', 'type': 'User', 'site_admin': False}, 'html_url': 'https://github.com/warrenzhu25/spark', 'description': 'Apache Spark', 'fork': True, 'url': 'https://api.github.com/repos/warrenzhu25/spark', 'forks_url': 'https://api.github.com/repos/warrenzhu25/spark/forks', 'keys_url': 'https://api.github.com/repos/warrenzhu25/spark/keys{/key_id}', 'collaborators_url': 'https://api.github.com/repos/warrenzhu25/spark/collaborators{/collaborator}', 'teams_url': 'https://api.github.com/repos/warrenzhu25/spark/teams', 'hooks_url': 'https://api.github.com/repos/warrenzhu25/spark/hooks', 'issue_events_url': 'https://api.github.com/repos/warrenzhu25/spark/issues/events{/number}', 'events_url': 'https://api.github.com/repos/warrenzhu25/spark/events', 'assignees_url': 'https://api.github.com/repos/warrenzhu25/spark/assignees{/user}', 'branches_url': 'https://api.github.com/repos/warrenzhu25/spark/branches{/branch}', 'tags_url': 'https://api.github.com/repos/warrenzhu25/spark/tags', 'blobs_url': 'https://api.github.com/repos/warrenzhu25/spark/git/blobs{/sha}', 'git_tags_url': 'https://api.github.com/repos/warrenzhu25/spark/git/tags{/sha}', 'git_refs_url': 'https://api.github.com/repos/warrenzhu25/spark/git/refs{/sha}', 'trees_url': 'https://api.github.com/repos/warrenzhu25/spark/git/trees{/sha}', 'statuses_url': 'https://api.github.com/repos/warrenzhu25/spark/statuses/{sha}', 'languages_url': 'https://api.github.com/repos/warrenzhu25/spark/languages', 'stargazers_url': 'https://api.github.com/repos/warrenzhu25/spark/stargazers', 'contributors_url': 'https://api.github.com/repos/warrenzhu25/spark/contributors', 'subscribers_url': 'https://api.github.com/repos/warrenzhu25/spark/subscribers', 'subscription_url': 'https://api.github.com/repos/warrenzhu25/spark/subscription', 'commits_url': 'https://api.github.com/repos/warrenzhu25/spark/commits{/sha}', 'git_commits_url': 'https://api.github.com/repos/warrenzhu25/spark/git/commits{/sha}', 'comments_url': 'https://api.github.com/repos/warrenzhu25/spark/comments{/number}', 'issue_comment_url': 'https://api.github.com/repos/warrenzhu25/spark/issues/comments{/number}', 'contents_url': 'https://api.github.com/repos/warrenzhu25/spark/contents/{+path}', 'compare_url': 'https://api.github.com/repos/warrenzhu25/spark/compare/{base}...{head}', 'merges_url': 'https://api.github.com/repos/warrenzhu25/spark/merges', 'archive_url': 'https://api.github.com/repos/warrenzhu25/spark/{archive_format}{/ref}', 'downloads_url': 'https://api.github.com/repos/warrenzhu25/spark/downloads', 'issues_url': 'https://api.github.com/repos/warrenzhu25/spark/issues{/number}', 'pulls_url': 'https://api.github.com/repos/warrenzhu25/spark/pulls{/number}', 'milestones_url': 'https://api.github.com/repos/warrenzhu25/spark/milestones{/number}', 'notifications_url': 'https://api.github.com/repos/warrenzhu25/spark/notifications{?since,all,participating}', 'labels_url': 'https://api.github.com/repos/warrenzhu25/spark/labels{/name}', 'releases_url': 'https://api.github.com/repos/warrenzhu25/spark/releases{/id}', 'deployments_url': 'https://api.github.com/repos/warrenzhu25/spark/deployments', 'created_at': '2019-10-25T23:12:23Z', 'updated_at': '2020-07-10T17:08:49Z', 'pushed_at': '2022-02-10T08:57:24Z', 'git_url': 'git://github.com/warrenzhu25/spark.git', 'ssh_url': 'git@github.com:warrenzhu25/spark.git', 'clone_url': 'https://github.com/warrenzhu25/spark.git', 'svn_url': 'https://github.com/warrenzhu25/spark', 'homepage': '', 'size': 397856, 'stargazers_count': 0, 'watchers_count': 0, 'language': 'Scala', 'has_issues': False, 'has_projects': True, 'has_downloads': True, 'has_wiki': False, 'has_pages': False, 'forks_count': 0, 'mirror_url': None, 'archived': False, 'disabled': False, 'open_issues_count': 0, 'license': {'key': 'apache-2.0', 'name': 'Apache License 2.0', 'spdx_id': 'Apache-2.0', 'url': 'https://api.github.com/licenses/apache-2.0', 'node_id': 'MDc6TGljZW5zZTI='}, 'allow_forking': True, 'is_template': False, 'topics': [], 'visibility': 'public', 'forks': 0, 'open_issues': 0, 'watchers': 0, 'default_branch': 'master'}}, 'base': {'label': 'apache:master', 'ref': 'master', 'sha': '25dd4254fed71923731fd59838875c0dd1ff665a', 'user': {'login': 'apache', 'id': 47359, 'node_id': 'MDEyOk9yZ2FuaXphdGlvbjQ3MzU5', 'avatar_url': 'https://avatars.githubusercontent.com/u/47359?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/apache', 'html_url': 'https://github.com/apache', 'followers_url': 'https://api.github.com/users/apache/followers', 'following_url': 'https://api.github.com/users/apache/following{/other_user}', 'gists_url': 'https://api.github.com/users/apache/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/apache/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/apache/subscriptions', 'organizations_url': 'https://api.github.com/users/apache/orgs', 'repos_url': 'https://api.github.com/users/apache/repos', 'events_url': 'https://api.github.com/users/apache/events{/privacy}', 'received_events_url': 'https://api.github.com/users/apache/received_events', 'type': 'Organization', 'site_admin': False}, 'repo': {'id': 17165658, 'node_id': 'MDEwOlJlcG9zaXRvcnkxNzE2NTY1OA==', 'name': 'spark', 'full_name': 'apache/spark', 'private': False, 'owner': {'login': 'apache', 'id': 47359, 'node_id': 'MDEyOk9yZ2FuaXphdGlvbjQ3MzU5', 'avatar_url': 'https://avatars.githubusercontent.com/u/47359?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/apache', 'html_url': 'https://github.com/apache', 'followers_url': 'https://api.github.com/users/apache/followers', 'following_url': 'https://api.github.com/users/apache/following{/other_user}', 'gists_url': 'https://api.github.com/users/apache/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/apache/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/apache/subscriptions', 'organizations_url': 'https://api.github.com/users/apache/orgs', 'repos_url': 'https://api.github.com/users/apache/repos', 'events_url': 'https://api.github.com/users/apache/events{/privacy}', 'received_events_url': 'https://api.github.com/users/apache/received_events', 'type': 'Organization', 'site_admin': False}, 'html_url': 'https://github.com/apache/spark', 'description': 'Apache Spark - A unified analytics engine for large-scale data processing', 'fork': False, 'url': 'https://api.github.com/repos/apache/spark', 'forks_url': 'https://api.github.com/repos/apache/spark/forks', 'keys_url': 'https://api.github.com/repos/apache/spark/keys{/key_id}', 'collaborators_url': 'https://api.github.com/repos/apache/spark/collaborators{/collaborator}', 'teams_url': 'https://api.github.com/repos/apache/spark/teams', 'hooks_url': 'https://api.github.com/repos/apache/spark/hooks', 'issue_events_url': 'https://api.github.com/repos/apache/spark/issues/events{/number}', 'events_url': 'https://api.github.com/repos/apache/spark/events', 'assignees_url': 'https://api.github.com/repos/apache/spark/assignees{/user}', 'branches_url': 'https://api.github.com/repos/apache/spark/branches{/branch}', 'tags_url': 'https://api.github.com/repos/apache/spark/tags', 'blobs_url': 'https://api.github.com/repos/apache/spark/git/blobs{/sha}', 'git_tags_url': 'https://api.github.com/repos/apache/spark/git/tags{/sha}', 'git_refs_url': 'https://api.github.com/repos/apache/spark/git/refs{/sha}', 'trees_url': 'https://api.github.com/repos/apache/spark/git/trees{/sha}', 'statuses_url': 'https://api.github.com/repos/apache/spark/statuses/{sha}', 'languages_url': 'https://api.github.com/repos/apache/spark/languages', 'stargazers_url': 'https://api.github.com/repos/apache/spark/stargazers', 'contributors_url': 'https://api.github.com/repos/apache/spark/contributors', 'subscribers_url': 'https://api.github.com/repos/apache/spark/subscribers', 'subscription_url': 'https://api.github.com/repos/apache/spark/subscription', 'commits_url': 'https://api.github.com/repos/apache/spark/commits{/sha}', 'git_commits_url': 'https://api.github.com/repos/apache/spark/git/commits{/sha}', 'comments_url': 'https://api.github.com/repos/apache/spark/comments{/number}', 'issue_comment_url': 'https://api.github.com/repos/apache/spark/issues/comments{/number}', 'contents_url': 'https://api.github.com/repos/apache/spark/contents/{+path}', 'compare_url': 'https://api.github.com/repos/apache/spark/compare/{base}...{head}', 'merges_url': 'https://api.github.com/repos/apache/spark/merges', 'archive_url': 'https://api.github.com/repos/apache/spark/{archive_format}{/ref}', 'downloads_url': 'https://api.github.com/repos/apache/spark/downloads', 'issues_url': 'https://api.github.com/repos/apache/spark/issues{/number}', 'pulls_url': 'https://api.github.com/repos/apache/spark/pulls{/number}', 'milestones_url': 'https://api.github.com/repos/apache/spark/milestones{/number}', 'notifications_url': 'https://api.github.com/repos/apache/spark/notifications{?since,all,participating}', 'labels_url': 'https://api.github.com/repos/apache/spark/labels{/name}', 'releases_url': 'https://api.github.com/repos/apache/spark/releases{/id}', 'deployments_url': 'https://api.github.com/repos/apache/spark/deployments', 'created_at': '2014-02-25T08:00:08Z', 'updated_at': '2022-02-12T19:17:52Z', 'pushed_at': '2022-02-13T03:08:57Z', 'git_url': 'git://github.com/apache/spark.git', 'ssh_url': 'git@github.com:apache/spark.git', 'clone_url': 'https://github.com/apache/spark.git', 'svn_url': 'https://github.com/apache/spark', 'homepage': 'https://spark.apache.org/', 'size': 406848, 'stargazers_count': 32048, 'watchers_count': 32048, 'language': 'Scala', 'has_issues': False, 'has_projects': True, 'has_downloads': True, 'has_wiki': False, 'has_pages': False, 'forks_count': 25214, 'mirror_url': None, 'archived': False, 'disabled': False, 'open_issues_count': 243, 'license': {'key': 'apache-2.0', 'name': 'Apache License 2.0', 'spdx_id': 'Apache-2.0', 'url': 'https://api.github.com/licenses/apache-2.0', 'node_id': 'MDc6TGljZW5zZTI='}, 'allow_forking': True, 'is_template': False, 'topics': ['big-data', 'java', 'jdbc', 'python', 'r', 'scala', 'spark', 'sql'], 'visibility': 'public', 'forks': 25214, 'open_issues': 243, 'watchers': 32048, 'default_branch': 'master'}}, '_links': {'self': {'href': 'https://api.github.com/repos/apache/spark/pulls/35498'}, 'html': {'href': 'https://github.com/apache/spark/pull/35498'}, 'issue': {'href': 'https://api.github.com/repos/apache/spark/issues/35498'}, 'comments': {'href': 'https://api.github.com/repos/apache/spark/issues/35498/comments'}, 'review_comments': {'href': 'https://api.github.com/repos/apache/spark/pulls/35498/comments'}, 'review_comment': {'href': 'https://api.github.com/repos/apache/spark/pulls/comments{/number}'}, 'commits': {'href': 'https://api.github.com/repos/apache/spark/pulls/35498/commits'}, 'statuses': {'href': 'https://api.github.com/repos/apache/spark/statuses/67eb28f353ba367cf98a3918002ed49362b83dcc'}}, 'author_association': 'CONTRIBUTOR', 'auto_merge': None, 'active_lock_reason': None, 'merged': False, 'mergeable': None, 'rebaseable': None, 'mergeable_state': 'unknown', 'merged_by': None, 'comments': 0, 'review_comments': 0, 'maintainer_can_modify': True, 'commits': 1, 'additions': 10, 'deletions': 6, 'changed_files': 2}}",opened,Scala
20169340317,PushEvent,warrenzhu25/spark,0.0,2022-02-10T08:59:11Z,"{'push_id': 9060131653, 'size': 1000, 'distinct_size': 1001, 'ref': 'refs/heads/k8s-log', 'head': '41d58298a219f068630a2bb1f56405651d81b49d', 'before': 'f9057749f7d7d25d396c03a8041a0a55e97148ab', 'commits': [{'sha': 'b60e576eafba864fe3269a465b2d547e1ef056b9', 'author': {'email': 'warren.zhu25@gmail.com', 'name': 'Warren Zhu'}, 'message': '[SPARK-36893][BUILD][MESOS] Upgrade mesos into 1.4.3\n\n### What changes were proposed in this pull request?\nUpgrade mesos into 1.4.3\n\n### Why are the changes needed?\nFix CVE-2018-11793\n\n### Does this PR introduce _any_ user-facing change?\nNo\n\n### How was this patch tested?\nManually\n\nCloses #34144 from warrenzhu25/mesos.\n\nAuthored-by: Warren Zhu <warren.zhu25@gmail.com>\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>', 'distinct': True, 'url': 'https://api.github.com/repos/warrenzhu25/spark/commits/b60e576eafba864fe3269a465b2d547e1ef056b9'}, {'sha': 'ad5a53511e46d4a432f1143cd3d6dee8af1f224a', 'author': {'email': 'xinrong.meng@databricks.com', 'name': 'Xinrong Meng'}, 'message': '[SPARK-36896][PYTHON] Return boolean for `dropTempView` and `dropGlobalTempView`\n\n### What changes were proposed in this pull request?\nCurrently `dropTempView` and `dropGlobalTempView` don\'t have return value, which conflicts with their docstring:\n`Returns true if this view is dropped successfully, false otherwise.`. And that\'s not consistent with the same API in other languages.\n\nThe PR proposes a fix for that.\n\n### Why are the changes needed?\nBe consistent with API in other languages.\n\n### Does this PR introduce _any_ user-facing change?\nYes.\n#### From\n```py\n# dropTempView\n>>> spark.createDataFrame([(1, 1)]).createTempView(""my_table"")\n>>> spark.table(""my_table"").collect()\n[Row(_1=1, _2=1)]\n>>> spark.catalog.dropTempView(""my_table"")\n>>> spark.catalog.dropTempView(""my_table"")\n\n# dropGlobalTempView\n>>> spark.createDataFrame([(1, 1)]).createGlobalTempView(""my_table"")\n>>> spark.table(""global_temp.my_table"").collect()\n[Row(_1=1, _2=1)]\n>>> spark.catalog.dropGlobalTempView(""my_table"")\n>>> spark.catalog.dropGlobalTempView(""my_table"")\n```\n\n#### To\n```py\n# dropTempView\n>>> spark.createDataFrame([(1, 1)]).createTempView(""my_table"")\n>>> spark.table(""my_table"").collect()\n[Row(_1=1, _2=1)]\n>>> spark.catalog.dropTempView(""my_table"")\nTrue\n>>> spark.catalog.dropTempView(""my_table"")\nFalse\n\n# dropGlobalTempView\n>>> spark.createDataFrame([(1, 1)]).createGlobalTempView(""my_table"")\n>>> spark.table(""global_temp.my_table"").collect()\n[Row(_1=1, _2=1)]\n>>> spark.catalog.dropGlobalTempView(""my_table"")\nTrue\n>>> spark.catalog.dropGlobalTempView(""my_table"")\nFalse\n```\n\n### How was this patch tested?\nExisting tests.\n\nCloses #34147 from xinrong-databricks/fix_return.\n\nAuthored-by: Xinrong Meng <xinrong.meng@databricks.com>\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>', 'distinct': True, 'url': 'https://api.github.com/repos/warrenzhu25/spark/commits/ad5a53511e46d4a432f1143cd3d6dee8af1f224a'}, {'sha': '17e3ca6df5eb4b7b74cd8d04868da39eb0137826', 'author': {'email': 'yodal@oss.nttdata.com', 'name': 'Leona Yoda'}, 'message': '[SPARK-36899][R] Support ILIKE API on R\n\n### What changes were proposed in this pull request?\n\nSupport ILIKE (case insensitive LIKE) API on R.\n\n### Why are the changes needed?\n\nILIKE statement on SQL interface is supported by SPARK-36674.\nThis PR will support R API for it.\n\n### Does this PR introduce _any_ user-facing change?\n\nYes. Users can call ilike from R.\n\n### How was this patch tested?\n\nUnit tests.\n\nCloses #34152 from yoda-mon/r-ilike.\n\nAuthored-by: Leona Yoda <yodal@oss.nttdata.com>\nSigned-off-by: Kousuke Saruta <sarutak@oss.nttdata.com>', 'distinct': True, 'url': 'https://api.github.com/repos/warrenzhu25/spark/commits/17e3ca6df5eb4b7b74cd8d04868da39eb0137826'}, {'sha': '5a32e41e9c992c6f08a48454d783e7cd97c971fc', 'author': {'email': 'sarutak@oss.nttdata.com', 'name': 'Kousuke Saruta'}, 'message': ""[SPARK-36865][PYTHON][DOCS] Add PySpark API document of session_window\n\n### What changes were proposed in this pull request?\n\nThis PR adds PySpark API document of `session_window`.\nThe docstring of the function doesn't comply with numpydoc format so this PR also fix it.\nFurther, the API document of `window` doesn't have `Parameters` section so it's also added in this PR.\n\n### Why are the changes needed?\n\nTo provide PySpark users with the API document of the newly added function.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo.\n\n### How was this patch tested?\n\n`make html` in `python/docs` and get the following docs.\n\n[window]\n![time-window-python-doc-after](https://user-images.githubusercontent.com/4736016/134963797-ce25b268-20ca-48e3-ac8d-cbcbd85ebb3e.png)\n\n[session_window]\n![session-window-python-doc-after](https://user-images.githubusercontent.com/4736016/134963853-dd9d8417-139b-41ee-9924-14544b1a91af.png)\n\nCloses #34118 from sarutak/python-session-window-doc.\n\nAuthored-by: Kousuke Saruta <sarutak@oss.nttdata.com>\nSigned-off-by: Jungtaek Lim <kabhwan.opensource@gmail.com>"", 'distinct': True, 'url': 'https://api.github.com/repos/warrenzhu25/spark/commits/5a32e41e9c992c6f08a48454d783e7cd97c971fc'}, {'sha': '7c155806ed6e1a2488d4ec2fa8da620318fea8dd', 'author': {'email': 'sarutak@oss.nttdata.com', 'name': 'Kousuke Saruta'}, 'message': '[SPARK-36830][SQL] Support reading and writing ANSI intervals from/to JSON datasources\n\n### What changes were proposed in this pull request?\n\nThis PR aims to support reading and writing ANSI intervals from/to JSON datasources.\nAith this change, a interval data is written as a literal form like `{""col"":""INTERVAL \'1-2\' YEAR TO MONTH""}`.\nFor the reading part, we need to specify the schema explicitly like:\n```\nval readDF = spark.read.schema(""col INTERVAL YEAR TO MONTH"").json(...)\n```\n\n### Why are the changes needed?\n\nFor better usability. There should be no reason to prohibit from reading/writing ANSI intervals from/to JSON datasources.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo.\n\n### How was this patch tested?\n\nNew test. It covers both V1 and V2 sources.\n\nCloses #34155 from sarutak/ansi-interval-json-source.\n\nAuthored-by: Kousuke Saruta <sarutak@oss.nttdata.com>\nSigned-off-by: Max Gekk <max.gekk@gmail.com>', 'distinct': True, 'url': 'https://api.github.com/repos/warrenzhu25/spark/commits/7c155806ed6e1a2488d4ec2fa8da620318fea8dd'}, {'sha': '4aeddb81d1cc9c464d5dddbd4d54dda4836a6078', 'author': {'email': 'karen.feng@databricks.com', 'name': 'Karen Feng'}, 'message': '[SPARK-36870][SQL] Introduce INTERNAL_ERROR error class\n\n### What changes were proposed in this pull request?\n\nAdds the `INTERNAL_ERROR` error class and the `isInternalError` API to `SparkThrowable`.\nRemoves existing error classes that are internal-only and replaces them with `INTERNAL_ERROR`.\n\n### Why are the changes needed?\n\nMakes it easy for end-users to diagnose whether an error is an internal error. If an end-user encounters an internal error, it should be reported immediately. This also limits the number of error classes, making it easy to audit. We do not need high-quality error messages for internal errors, as they should not be exposed to the end-user.\n\n### Does this PR introduce _any_ user-facing change?\n\nYes; this changes the error class in master.\n\n### How was this patch tested?\n\nUnit tests\n\nCloses #34123 from karenfeng/internal-error-class.\n\nAuthored-by: Karen Feng <karen.feng@databricks.com>\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>', 'distinct': True, 'url': 'https://api.github.com/repos/warrenzhu25/spark/commits/4aeddb81d1cc9c464d5dddbd4d54dda4836a6078'}, {'sha': '13ddc916683c01a0c076c79e3c0e491395eec9ff', 'author': {'email': 'haejoon.lee@databricks.com', 'name': 'itholic'}, 'message': '[SPARK-36435][PYTHON] Implement MultIndex.equal_levels\n\n### What changes were proposed in this pull request?\nThis PR proposes implementing `MultiIndex.equal_levels`.\n\n```python\n>>> psmidx1 = ps.MultiIndex.from_tuples([(""a"", ""x""), (""b"", ""y""), (""c"", ""z"")])\n>>> psmidx2 = ps.MultiIndex.from_tuples([(""b"", ""y""), (""a"", ""x""), (""c"", ""z"")])\n>>> psmidx1.equal_levels(psmidx2)\nTrue\n\n>>> psmidx1 = ps.MultiIndex.from_tuples([(""a"", ""x""), (""b"", ""y""), (""c"", ""z""), (""a"", ""y"")])\n>>> psmidx2 = ps.MultiIndex.from_tuples([(""a"", ""y""), (""b"", ""x""), (""c"", ""z""), (""c"", ""x"")])\n>>> psmidx1.equal_levels(psmidx2)\nTrue\n```\n\nThis was originally proposed in https://github.com/databricks/koalas/pull/1789, and all reviews in origin PR has been resolved.\n\n### Why are the changes needed?\n\nWe should support the pandas API as much as possible for pandas-on-Spark module.\n\n### Does this PR introduce _any_ user-facing change?\n\nYes, the `MultiIndex.equal_levels` API is available.\n\n### How was this patch tested?\n\nUnittests\n\nCloses #34113 from itholic/SPARK-36435.\n\nLead-authored-by: itholic <haejoon.lee@databricks.com>\nCo-authored-by: Haejoon Lee <44108233+itholic@users.noreply.github.com>\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>', 'distinct': True, 'url': 'https://api.github.com/repos/warrenzhu25/spark/commits/13ddc916683c01a0c076c79e3c0e491395eec9ff'}, {'sha': '73747ecb970595d49c478b0eb65f5132c8b0bf02', 'author': {'email': 'vsowrirajan@linkedin.com', 'name': 'Venkata krishnan Sowrirajan'}, 'message': '[SPARK-36038][CORE] Speculation metrics summary at stage level\n\n### What changes were proposed in this pull request?\n\nCurrently there are no speculation metrics available for Spark either at application/job/stage level. This PR is to add some basic speculation metrics for a stage when speculation execution is enabled.\n\nThis is similar to the existing stage level metrics tracking numTotal (total number of speculated tasks), numCompleted (total number of successful speculated tasks), numFailed (total number of failed speculated tasks), numKilled (total number of killed speculated tasks) etc.\n\nWith this new set of metrics, it helps further understanding speculative execution feature in the context of the application and also helps in further tuning the speculative execution config knobs.\n\nScreenshot of Spark UI with speculation summary:\n![Screen Shot 2021-09-22 at 12 12 20 PM](https://user-images.githubusercontent.com/8871522/135321311-db7699ad-f1ae-4729-afea-d1e2c4e86103.png)\n\nScreenshot of Spark UI with API output:\n![Screen Shot 2021-09-22 at 12 10 37 PM](https://user-images.githubusercontent.com/8871522/135321486-4dbb7a67-5580-47f8-bccf-81c758c2e988.png)\n\n### Why are the changes needed?\n\nAdditional metrics for speculative execution.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo\n\n### How was this patch tested?\n\nUnit tests added and also deployed in our internal platform for quite some time now.\n\nLead-authored by: Venkata krishnan Sowrirajan <vsowrirajanlinkedin.com>\nCo-authored by: Ron Hu <rhulinkedin.com>\nCo-authored by: Thejdeep Gudivada <tgudivadalinkedin.com>\n\nCloses #33253 from venkata91/speculation-metrics.\n\nAuthored-by: Venkata krishnan Sowrirajan <vsowrirajan@linkedin.com>\nSigned-off-by: Kousuke Saruta <sarutak@oss.nttdata.com>', 'distinct': True, 'url': 'https://api.github.com/repos/warrenzhu25/spark/commits/73747ecb970595d49c478b0eb65f5132c8b0bf02'}, {'sha': '25db6b45c7636a1c62b6fd6ad189836b019374a3', 'author': {'email': 'fishman.code@gmail.com', 'name': 'Dmitriy Fishman'}, 'message': '[MINOR][DOCS] Typo fix in cloud-integration.md\n\n### What changes were proposed in this pull request?\nTypo fix\n\n### Why are the changes needed?\n\n### Does this PR introduce _any_ user-facing change?\n\n### How was this patch tested?\n\nCloses #34129 from fishmandev/patch-1.\n\nAuthored-by: Dmitriy Fishman <fishman.code@gmail.com>\nSigned-off-by: Sean Owen <srowen@gmail.com>', 'distinct': True, 'url': 'https://api.github.com/repos/warrenzhu25/spark/commits/25db6b45c7636a1c62b6fd6ad189836b019374a3'}, {'sha': '14d4ceeb73c44dd957246f36eea9ece527d8d8d7', 'author': {'email': 'sunchao@apple.com', 'name': 'Chao Sun'}, 'message': '[SPARK-36891][SQL] Refactor SpecificParquetRecordReaderBase and add more coverage on vectorized Parquet decoding\n\n### What changes were proposed in this pull request?\n\nAdd a new test suite `ParquetVectorizedSuite` to provide more coverage on vectorized Parquet decoding logic, with different combinations on column index, dictionary, batch size, page size, etc.\n\nTo facilitate the test, this also refactored `SpecificParquetRecordReaderBase` and makes the Parquet row group reader pluggable.\n\n### Why are the changes needed?\n\nCurrently `ParquetIOSuite` and `ParquetColumnIndexSuite` only test on the high-level API which is insufficient, especially after the introduction of column index support, for which we want to cover various combinations involving row ranges, first row index, batch size, page size, etc.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo.\n\n### How was this patch tested?\n\nAdded new test suite.\n\nCloses #34149 from sunchao/SPARK-36891-parquet-test.\n\nAuthored-by: Chao Sun <sunchao@apple.com>\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>', 'distinct': True, 'url': 'https://api.github.com/repos/warrenzhu25/spark/commits/14d4ceeb73c44dd957246f36eea9ece527d8d8d7'}, {'sha': '38d39812c176e4b52a08397f7936f87ea32930e7', 'author': {'email': 'ueshin@databricks.com', 'name': 'Takuya UESHIN'}, 'message': '[SPARK-36907][PYTHON] Fix DataFrameGroupBy.apply without shortcut\n\n### What changes were proposed in this pull request?\n\nFix `DataFrameGroupBy.apply` without shortcut.\n\nPandas\' `DataFrameGroupBy.apply` sometimes behaves weirdly when the udf returns `Series` and whether there is only one group or more. E.g.,:\n\n```py\n>>> pdf = pd.DataFrame(\n...      {""a"": [1, 2, 3, 4, 5, 6], ""b"": [1, 1, 2, 3, 5, 8], ""c"": [1, 4, 9, 16, 25, 36]},\n...      columns=[""a"", ""b"", ""c""],\n... )\n\n>>> pdf.groupby(\'b\').apply(lambda x: x[\'a\'])\nb\n1  0    1\n   1    2\n2  2    3\n3  3    4\n5  4    5\n8  5    6\nName: a, dtype: int64\n>>> pdf[pdf[\'b\'] == 1].groupby(\'b\').apply(lambda x: x[\'a\'])\na  0  1\nb\n1  1  2\n```\n\nIf there is only one group, it returns a ""wide"" `DataFrame` instead of `Series`.\n\nIn our non-shortcut path, there is always only one group because it will be run in `groupby-applyInPandas`, so we will get `DataFrame`, then we should convert it to `Series` ourselves.\n\n### Why are the changes needed?\n\n`DataFrameGroupBy.apply` without shortcut could raise an exception when it returns `Series`.\n\n```py\n>>> ps.options.compute.shortcut_limit = 3\n>>> psdf = ps.DataFrame(\n...     {""a"": [1, 2, 3, 4, 5, 6], ""b"": [1, 1, 2, 3, 5, 8], ""c"": [1, 4, 9, 16, 25, 36]},\n...     columns=[""a"", ""b"", ""c""],\n... )\n>>> psdf.groupby(""b"").apply(lambda x: x[""a""])\norg.apache.spark.api.python.PythonException: Traceback (most recent call last):\n...\nValueError: Length mismatch: Expected axis has 2 elements, new values have 3 elements\n```\n\n### Does this PR introduce _any_ user-facing change?\n\nThe error above will be gone:\n\n```py\n>>> psdf.groupby(""b"").apply(lambda x: x[""a""])\nb\n1  0    1\n   1    2\n2  2    3\n3  3    4\n5  4    5\n8  5    6\nName: a, dtype: int64\n```\n\n### How was this patch tested?\n\nAdded tests.\n\nCloses #34160 from ueshin/issues/SPARK-36907/groupby-apply.\n\nAuthored-by: Takuya UESHIN <ueshin@databricks.com>\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>', 'distinct': True, 'url': 'https://api.github.com/repos/warrenzhu25/spark/commits/38d39812c176e4b52a08397f7936f87ea32930e7'}, {'sha': '167664896d4b0ca656bbb2ab6d5a045411e64cd1', 'author': {'email': 'huaxin_gao@apple.com', 'name': 'Huaxin Gao'}, 'message': '[MINOR][SQL] Use SQLConf.resolver for caseSensitiveResolution/caseInsensitiveResolution\n\n### What changes were proposed in this pull request?\nUse `SQLConf.resolver` for `caseSensitiveResolution`/`caseInsensitveResolution` instead of having a new method\n\n### Why are the changes needed?\nremove redundant code\n\n### Does this PR introduce _any_ user-facing change?\nNo\n\n### How was this patch tested?\nexisting code\n\nCloses #34171 from huaxingao/minor.\n\nAuthored-by: Huaxin Gao <huaxin_gao@apple.com>\nSigned-off-by: Liang-Chi Hsieh <viirya@gmail.com>', 'distinct': True, 'url': 'https://api.github.com/repos/warrenzhu25/spark/commits/167664896d4b0ca656bbb2ab6d5a045411e64cd1'}, {'sha': '81812606cc32a41863c86695c4710b0e914b7e3c', 'author': {'email': 'xinrong.meng@databricks.com', 'name': 'Xinrong Meng'}, 'message': '[SPARK-36906][PYTHON] Inline type hints for conf.py and observation.py in python/pyspark/sql\n\n### What changes were proposed in this pull request?\nInline type hints for conf.py and observation.py in python/pyspark/sql.\n\n### Why are the changes needed?\nCurrently, there is type hint stub files (*.pyi) to show the expected types for functions, but we can also take advantage of static type checking within the functions by inlining the type hints.\n\n### Does this PR introduce _any_ user-facing change?\nNo.\n\nIt has a DOC typo fix:\n`Metrics are aggregation expressions, which are applied to the DataFrame while **is** is being`\nis changed to\n`Metrics are aggregation expressions, which are applied to the DataFrame while **it** is being`\n\n### How was this patch tested?\nExisting test.\n\nCloses #34159 from xinrong-databricks/inline_conf_observation.\n\nAuthored-by: Xinrong Meng <xinrong.meng@databricks.com>\nSigned-off-by: Takuya UESHIN <ueshin@databricks.com>', 'distinct': True, 'url': 'https://api.github.com/repos/warrenzhu25/spark/commits/81812606cc32a41863c86695c4710b0e914b7e3c'}, {'sha': 'b30e2144833faf7965cde58034186a0a05236069', 'author': {'email': 'xinrong.meng@databricks.com', 'name': 'Xinrong Meng'}, 'message': '[SPARK-36881][PYTHON] Inline type hints for python/pyspark/sql/catalog.py\n\n### What changes were proposed in this pull request?\nInline type hints for python/pyspark/sql/catalog.py.\n\n### Why are the changes needed?\nCurrently, a type hint stub file hints for python/pyspark/sql/catalog.pyi is used. We may leverage static type check by inlining type hints.\n\n### Does this PR introduce _any_ user-facing change?\nNo.\n\n### How was this patch tested?\nExisting test.\n\nCloses #34133 from xinrong-databricks/inline_catalog.\n\nAuthored-by: Xinrong Meng <xinrong.meng@databricks.com>\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>', 'distinct': True, 'url': 'https://api.github.com/repos/warrenzhu25/spark/commits/b30e2144833faf7965cde58034186a0a05236069'}, {'sha': '65eb4a212914bf1fbf9864ac0c0882643eb41a77', 'author': {'email': 'max.gekk@gmail.com', 'name': 'Max Gekk'}, 'message': '[SPARK-36920][SQL] Support ANSI intervals by `ABS()`\n\n### What changes were proposed in this pull request?\nIn the PR, I propose to handle ANSI interval types by the `Abs` expression, and the `abs()` function as a consequence of that:\n- for positive and zero intervals, `ABS()` returns the same input value,\n- for minimal supported values (`Int.MinValue` months for year-month interval and `Long.MinValue` microseconds for day-time interval), `ABS()` throws the arithmetic overflow exception.\n- for other supported negative intervals, `ABS()` negate its input and returns a positive interval.\n\nFor example:\n```sql\nspark-sql> SELECT ABS(INTERVAL -\'10-8\' YEAR TO MONTH);\n10-8\nspark-sql> SELECT ABS(INTERVAL \'-10 01:02:03.123456\' DAY TO SECOND);\n10 01:02:03.123456000\n```\n\n### Why are the changes needed?\nTo improve user experience with Spark SQL.\n\n### Does this PR introduce _any_ user-facing change?\nNo, this PR just extends `ABS()` by supporting new types.\n\n### How was this patch tested?\nBy running new tests:\n```\n$ build/sbt ""test:testOnly *ArithmeticExpressionSuite""\n$ build/sbt ""sql/testOnly org.apache.spark.sql.SQLQueryTestSuite -- -z interval.sql""\n$ build/sbt ""sql/test:testOnly org.apache.spark.sql.expressions.ExpressionInfoSuite""\n```\n\nCloses #34169 from MaxGekk/abs-ansi-intervals.\n\nAuthored-by: Max Gekk <max.gekk@gmail.com>\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>', 'distinct': True, 'url': 'https://api.github.com/repos/warrenzhu25/spark/commits/65eb4a212914bf1fbf9864ac0c0882643eb41a77'}, {'sha': 'fa1805db48ca53ece4cbbe42ebb2a9811a142ed2', 'author': {'email': 'sarutak@oss.nttdata.com', 'name': 'Kousuke Saruta'}, 'message': '[SPARK-36874][SQL] DeduplicateRelations should copy dataset_id tag to avoid ambiguous self join\n\n### What changes were proposed in this pull request?\n\nThis PR fixes an issue that ambiguous self join can\'t be detected if the left and right DataFrame are swapped.\nThis is an example.\n```\nval df1 = Seq((1, 2, ""A1""),(2, 1, ""A2"")).toDF(""key1"", ""key2"", ""value"")\nval df2 = df1.filter($""value"" === ""A2"")\n\ndf1.join(df2, df1(""key1"") === df2(""key2"")) // Ambiguous self join is detected and AnalysisException is thrown.\n\ndf2.join(df1, df1(""key1"") === df2(""key2)) // Ambiguous self join is not detected.\n```\n\nThe root cause seems that an inner function `collectConflictPlans` in `DeduplicateRelations.` doesn\'t copy the `dataset_id` tag when it copies a `LogicalPlan`.\n\n### Why are the changes needed?\n\nBug fix.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo.\n\n### How was this patch tested?\n\nNew tests.\n\nCloses #34172 from sarutak/fix-deduplication-issue.\n\nAuthored-by: Kousuke Saruta <sarutak@oss.nttdata.com>\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>', 'distinct': True, 'url': 'https://api.github.com/repos/warrenzhu25/spark/commits/fa1805db48ca53ece4cbbe42ebb2a9811a142ed2'}, {'sha': 'd6786e036d610476a3be0fca5b16ba819dcbc013', 'author': {'email': 'dgd_contributor@viettel.com.vn', 'name': 'dchvn nguyen'}, 'message': '[SPARK-36711][PYTHON] Support multi-index in new syntax\n\n### What changes were proposed in this pull request?\nSupport multi-index in new syntax to specify index data type\n\n### Why are the changes needed?\nSupport multi-index in new syntax to specify index data type\n\nhttps://issues.apache.org/jira/browse/SPARK-36707\n\n### Does this PR introduce _any_ user-facing change?\nAfter this PR user can use\n\n``` python\n>>> ps.DataFrame[[int, int],[int, int]]\ntyping.Tuple[pyspark.pandas.typedef.typehints.IndexNameType, pyspark.pandas.typedef.typehints.IndexNameType, pyspark.pandas.typedef.typehints.NameType, pyspark.pandas.typedef.typehints.NameType]\n\n>>> arrays = [[1, 1, 2], [\'red\', \'blue\', \'red\']]\n>>> idx = pd.MultiIndex.from_arrays(arrays, names=(\'number\', \'color\'))\n>>> pdf = pd.DataFrame([[1,2,3],[2,3,4],[4,5,6]], index=idx, columns=[""a"", ""b"", ""c""])\n>>> ps.DataFrame[pdf.index.dtypes, pdf.dtypes]\ntyping.Tuple[pyspark.pandas.typedef.typehints.IndexNameType, pyspark.pandas.typedef.typehints.IndexNameType, pyspark.pandas.typedef.typehints.NameType, pyspark.pandas.typedef.typehints.NameType, pyspark.pandas.typedef.typehints.NameType]\n\n>>> ps.DataFrame[[(""index"", int), (""index-2"", int)], [(""id"", int), (""A"", int)]]\ntyping.Tuple[pyspark.pandas.typedef.typehints.IndexNameType, pyspark.pandas.typedef.typehints.IndexNameType, pyspark.pandas.typedef.typehints.NameType, pyspark.pandas.typedef.typehints.NameType]\n\n>>> ps.DataFrame[zip(pdf.index.names, pdf.index.dtypes), zip(pdf.columns, pdf.dtypes)]\ntyping.Tuple[pyspark.pandas.typedef.typehints.IndexNameType, pyspark.pandas.typedef.typehints.IndexNameType, pyspark.pandas.typedef.typehints.NameType, pyspark.pandas.typedef.typehints.NameType, pyspark.pandas.typedef.typehints.NameType]\n\n```\n\n### How was this patch tested?\nexist tests\n\nCloses #34176 from dchvn/SPARK-36711.\n\nAuthored-by: dchvn nguyen <dgd_contributor@viettel.com.vn>\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>', 'distinct': True, 'url': 'https://api.github.com/repos/warrenzhu25/spark/commits/d6786e036d610476a3be0fca5b16ba819dcbc013'}, {'sha': '7aee29016a3b2986846195c45cf6dea44db61738', 'author': {'email': 'xinrong.meng@databricks.com', 'name': 'Xinrong Meng'}, 'message': '[SPARK-36880][PYTHON] Inline type hints for python/pyspark/sql/functions.py\n\n### What changes were proposed in this pull request?\nInline type hints from `python/pyspark/sql/functions.pyi` to `python/pyspark/sql/functions.py`.\n\n### Why are the changes needed?\nCurrently, there is type hint stub files `python/pyspark/sql/functions.pyi` to show the expected types for functions, but we can also take advantage of static type checking within the functions by inlining the type hints.\n\n### Does this PR introduce _any_ user-facing change?\nNo.\n\n### How was this patch tested?\nExisting test.\n\nCloses #34130 from xinrong-databricks/inline_functions.\n\nAuthored-by: Xinrong Meng <xinrong.meng@databricks.com>\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>', 'distinct': True, 'url': 'https://api.github.com/repos/warrenzhu25/spark/commits/7aee29016a3b2986846195c45cf6dea44db61738'}, {'sha': 'fb919afac7e785fbb6d2b2507495437b7536e5f2', 'author': {'email': 'wenchen@databricks.com', 'name': 'Wenchen Fan'}, 'message': '[SPARK-36926][SQL] Decimal average mistakenly overflow\n\n### What changes were proposed in this pull request?\n\nThis bug was introduced by https://github.com/apache/spark/pull/33177\n\nWhen checking overflow of the sum value in the average function, we should use the `sumDataType` instead of the input decimal type.\n\n### Why are the changes needed?\n\nfix a regression\n\n### Does this PR introduce _any_ user-facing change?\n\nYes, the result was wrong before this PR.\n\n### How was this patch tested?\n\na new test\n\nCloses #34180 from cloud-fan/bug.\n\nLead-authored-by: Wenchen Fan <wenchen@databricks.com>\nCo-authored-by: Wenchen Fan <cloud0fan@gmail.com>\nSigned-off-by: Gengliang Wang <gengliang@apache.org>', 'distinct': True, 'url': 'https://api.github.com/repos/warrenzhu25/spark/commits/fb919afac7e785fbb6d2b2507495437b7536e5f2'}, {'sha': '6e8a4626117f0cb5535875f7181f56350ad4f195', 'author': {'email': 'peng.8lei@gmail.com', 'name': 'PengLei'}, 'message': '[SPARK-36841][SQL] Add ansi syntax `set catalog xxx` to change the current catalog\n\n### What changes were proposed in this pull request?\n1Add the statement of `set catalog xxx` to change the current catalog\n2Retain the `USE` statement to change the current catalog\n3Forcible loading the new catalog when change the new catalog.\n\n### Why are the changes needed?\nAnsi SQL use `SET CATALOG XXX` statement to change the catalog.\n\n[DISCUSS](https://github.com/apache/spark/pull/34030#issuecomment-925936538)\n\n<img width=""521"" alt=""set-catalog"" src=""https://user-images.githubusercontent.com/41178002/134658562-4e4dd879-b6e5-484c-9461-6345c3faaf2e.png"">\n\n### Does this PR introduce _any_ user-facing change?\nYes, User can use `SET CATALOG XXX` to change the current catalog\n\n### How was this patch tested?\nAdd ut testcase\n\nCloses #34096 from Peng-Lei/set-catalog-statement.\n\nAuthored-by: PengLei <peng.8lei@gmail.com>\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>', 'distinct': True, 'url': 'https://api.github.com/repos/warrenzhu25/spark/commits/6e8a4626117f0cb5535875f7181f56350ad4f195'}]}",,Scala
20169268606,PullRequestReviewCommentEvent,apache/spark,0.0,2022-02-10T08:55:14Z,"{'action': 'created', 'comment': {'url': 'https://api.github.com/repos/apache/spark/pulls/comments/803436029', 'pull_request_review_id': 878509833, 'id': 803436029, 'node_id': 'PRRC_kwDOAQXtWs4v43X9', 'diff_hunk': '@@ -177,7 +177,14 @@ private[spark] class BasicExecutorFeatureStep(\n             .withValue(opt)\n             .build()\n         }\n-      }\n+      } ++ {\n+      if (kubernetesConf.get(KUBERNETES_LOG_TO_FILE)) {\n+        Seq(new EnvVarBuilder()\n+          .withName(ENV_SPARK_LOG_PATH)\n+          .withValue(kubernetesConf.get(KUBERNETES_LOG_TO_FILE_PATH))\n+          .build())\n+      } else None', 'path': 'resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/features/BasicExecutorFeatureStep.scala', 'position': 11, 'original_position': 11, 'commit_id': 'f9057749f7d7d25d396c03a8041a0a55e97148ab', 'original_commit_id': 'f9057749f7d7d25d396c03a8041a0a55e97148ab', 'user': {'login': 'warrenzhu25', 'id': 1633312, 'node_id': 'MDQ6VXNlcjE2MzMzMTI=', 'avatar_url': 'https://avatars.githubusercontent.com/u/1633312?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/warrenzhu25', 'html_url': 'https://github.com/warrenzhu25', 'followers_url': 'https://api.github.com/users/warrenzhu25/followers', 'following_url': 'https://api.github.com/users/warrenzhu25/following{/other_user}', 'gists_url': 'https://api.github.com/users/warrenzhu25/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/warrenzhu25/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/warrenzhu25/subscriptions', 'organizations_url': 'https://api.github.com/users/warrenzhu25/orgs', 'repos_url': 'https://api.github.com/users/warrenzhu25/repos', 'events_url': 'https://api.github.com/users/warrenzhu25/events{/privacy}', 'received_events_url': 'https://api.github.com/users/warrenzhu25/received_events', 'type': 'User', 'site_admin': False}, 'body': 'I think `None` also worked, one example is https://github.com/warrenzhu25/spark/blob/f9057749f7d7d25d396c03a8041a0a55e97148ab/resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/features/BasicExecutorFeatureStep.scala#L156', 'created_at': '2022-02-10T08:55:14Z', 'updated_at': '2022-02-10T08:55:14Z', 'html_url': 'https://github.com/apache/spark/pull/34035#discussion_r803436029', 'pull_request_url': 'https://api.github.com/repos/apache/spark/pulls/34035', 'author_association': 'CONTRIBUTOR', '_links': {'self': {'href': 'https://api.github.com/repos/apache/spark/pulls/comments/803436029'}, 'html': {'href': 'https://github.com/apache/spark/pull/34035#discussion_r803436029'}, 'pull_request': {'href': 'https://api.github.com/repos/apache/spark/pulls/34035'}}, 'reactions': {'url': 'https://api.github.com/repos/apache/spark/pulls/comments/803436029/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'start_line': None, 'original_start_line': None, 'start_side': None, 'line': 186, 'original_line': 186, 'side': 'RIGHT', 'in_reply_to_id': 803413800}, 'pull_request': {'url': 'https://api.github.com/repos/apache/spark/pulls/34035', 'id': 736626805, 'node_id': 'PR_kwDOAQXtWs4r6Ah1', 'html_url': 'https://github.com/apache/spark/pull/34035', 'diff_url': 'https://github.com/apache/spark/pull/34035.diff', 'patch_url': 'https://github.com/apache/spark/pull/34035.patch', 'issue_url': 'https://api.github.com/repos/apache/spark/issues/34035', 'number': 34035, 'state': 'closed', 'locked': False, 'title': '[SPARK-36793][K8S] Support write container stdout/stderr to file', 'user': {'login': 'warrenzhu25', 'id': 1633312, 'node_id': 'MDQ6VXNlcjE2MzMzMTI=', 'avatar_url': 'https://avatars.githubusercontent.com/u/1633312?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/warrenzhu25', 'html_url': 'https://github.com/warrenzhu25', 'followers_url': 'https://api.github.com/users/warrenzhu25/followers', 'following_url': 'https://api.github.com/users/warrenzhu25/following{/other_user}', 'gists_url': 'https://api.github.com/users/warrenzhu25/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/warrenzhu25/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/warrenzhu25/subscriptions', 'organizations_url': 'https://api.github.com/users/warrenzhu25/orgs', 'repos_url': 'https://api.github.com/users/warrenzhu25/repos', 'events_url': 'https://api.github.com/users/warrenzhu25/events{/privacy}', 'received_events_url': 'https://api.github.com/users/warrenzhu25/received_events', 'type': 'User', 'site_admin': False}, 'body': '### What changes were proposed in this pull request?\r\nSupport write container stdout/stderr to file\r\n\r\n### Why are the changes needed?\r\nIf users want to sidecar logging agent to send stdout/stderr to external log storage,  only way is to change entrypoint.sh, which might break compatibility with community version.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nYes. User can enable this feature by spark config.\r\n\r\n### How was this patch tested?\r\nAdded UT in BasicDriverFeatureStepSuite and BasicExecutorFeatureStepSuite\r\n', 'created_at': '2021-09-17T19:33:38Z', 'updated_at': '2022-02-10T08:55:14Z', 'closed_at': '2022-01-03T00:12:12Z', 'merged_at': None, 'merge_commit_sha': 'fc1b5f6e4aefb0d79e9069cef3250031991775ab', 'assignee': None, 'assignees': [], 'requested_reviewers': [], 'requested_teams': [], 'labels': [{'id': 1406605057, 'node_id': 'MDU6TGFiZWwxNDA2NjA1MDU3', 'url': 'https://api.github.com/repos/apache/spark/labels/KUBERNETES', 'name': 'KUBERNETES', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1753478521, 'node_id': 'MDU6TGFiZWwxNzUzNDc4NTIx', 'url': 'https://api.github.com/repos/apache/spark/labels/Stale', 'name': 'Stale', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981471081, 'node_id': 'MDU6TGFiZWwxOTgxNDcxMDgx', 'url': 'https://api.github.com/repos/apache/spark/labels/DOCS', 'name': 'DOCS', 'color': 'ededed', 'default': False, 'description': None}], 'milestone': None, 'draft': False, 'commits_url': 'https://api.github.com/repos/apache/spark/pulls/34035/commits', 'review_comments_url': 'https://api.github.com/repos/apache/spark/pulls/34035/comments', 'review_comment_url': 'https://api.github.com/repos/apache/spark/pulls/comments{/number}', 'comments_url': 'https://api.github.com/repos/apache/spark/issues/34035/comments', 'statuses_url': 'https://api.github.com/repos/apache/spark/statuses/f9057749f7d7d25d396c03a8041a0a55e97148ab', 'head': {'label': 'warrenzhu25:k8s-log', 'ref': 'k8s-log', 'sha': 'f9057749f7d7d25d396c03a8041a0a55e97148ab', 'user': {'login': 'warrenzhu25', 'id': 1633312, 'node_id': 'MDQ6VXNlcjE2MzMzMTI=', 'avatar_url': 'https://avatars.githubusercontent.com/u/1633312?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/warrenzhu25', 'html_url': 'https://github.com/warrenzhu25', 'followers_url': 'https://api.github.com/users/warrenzhu25/followers', 'following_url': 'https://api.github.com/users/warrenzhu25/following{/other_user}', 'gists_url': 'https://api.github.com/users/warrenzhu25/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/warrenzhu25/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/warrenzhu25/subscriptions', 'organizations_url': 'https://api.github.com/users/warrenzhu25/orgs', 'repos_url': 'https://api.github.com/users/warrenzhu25/repos', 'events_url': 'https://api.github.com/users/warrenzhu25/events{/privacy}', 'received_events_url': 'https://api.github.com/users/warrenzhu25/received_events', 'type': 'User', 'site_admin': False}, 'repo': {'id': 217627454, 'node_id': 'MDEwOlJlcG9zaXRvcnkyMTc2Mjc0NTQ=', 'name': 'spark', 'full_name': 'warrenzhu25/spark', 'private': False, 'owner': {'login': 'warrenzhu25', 'id': 1633312, 'node_id': 'MDQ6VXNlcjE2MzMzMTI=', 'avatar_url': 'https://avatars.githubusercontent.com/u/1633312?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/warrenzhu25', 'html_url': 'https://github.com/warrenzhu25', 'followers_url': 'https://api.github.com/users/warrenzhu25/followers', 'following_url': 'https://api.github.com/users/warrenzhu25/following{/other_user}', 'gists_url': 'https://api.github.com/users/warrenzhu25/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/warrenzhu25/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/warrenzhu25/subscriptions', 'organizations_url': 'https://api.github.com/users/warrenzhu25/orgs', 'repos_url': 'https://api.github.com/users/warrenzhu25/repos', 'events_url': 'https://api.github.com/users/warrenzhu25/events{/privacy}', 'received_events_url': 'https://api.github.com/users/warrenzhu25/received_events', 'type': 'User', 'site_admin': False}, 'html_url': 'https://github.com/warrenzhu25/spark', 'description': 'Apache Spark', 'fork': True, 'url': 'https://api.github.com/repos/warrenzhu25/spark', 'forks_url': 'https://api.github.com/repos/warrenzhu25/spark/forks', 'keys_url': 'https://api.github.com/repos/warrenzhu25/spark/keys{/key_id}', 'collaborators_url': 'https://api.github.com/repos/warrenzhu25/spark/collaborators{/collaborator}', 'teams_url': 'https://api.github.com/repos/warrenzhu25/spark/teams', 'hooks_url': 'https://api.github.com/repos/warrenzhu25/spark/hooks', 'issue_events_url': 'https://api.github.com/repos/warrenzhu25/spark/issues/events{/number}', 'events_url': 'https://api.github.com/repos/warrenzhu25/spark/events', 'assignees_url': 'https://api.github.com/repos/warrenzhu25/spark/assignees{/user}', 'branches_url': 'https://api.github.com/repos/warrenzhu25/spark/branches{/branch}', 'tags_url': 'https://api.github.com/repos/warrenzhu25/spark/tags', 'blobs_url': 'https://api.github.com/repos/warrenzhu25/spark/git/blobs{/sha}', 'git_tags_url': 'https://api.github.com/repos/warrenzhu25/spark/git/tags{/sha}', 'git_refs_url': 'https://api.github.com/repos/warrenzhu25/spark/git/refs{/sha}', 'trees_url': 'https://api.github.com/repos/warrenzhu25/spark/git/trees{/sha}', 'statuses_url': 'https://api.github.com/repos/warrenzhu25/spark/statuses/{sha}', 'languages_url': 'https://api.github.com/repos/warrenzhu25/spark/languages', 'stargazers_url': 'https://api.github.com/repos/warrenzhu25/spark/stargazers', 'contributors_url': 'https://api.github.com/repos/warrenzhu25/spark/contributors', 'subscribers_url': 'https://api.github.com/repos/warrenzhu25/spark/subscribers', 'subscription_url': 'https://api.github.com/repos/warrenzhu25/spark/subscription', 'commits_url': 'https://api.github.com/repos/warrenzhu25/spark/commits{/sha}', 'git_commits_url': 'https://api.github.com/repos/warrenzhu25/spark/git/commits{/sha}', 'comments_url': 'https://api.github.com/repos/warrenzhu25/spark/comments{/number}', 'issue_comment_url': 'https://api.github.com/repos/warrenzhu25/spark/issues/comments{/number}', 'contents_url': 'https://api.github.com/repos/warrenzhu25/spark/contents/{+path}', 'compare_url': 'https://api.github.com/repos/warrenzhu25/spark/compare/{base}...{head}', 'merges_url': 'https://api.github.com/repos/warrenzhu25/spark/merges', 'archive_url': 'https://api.github.com/repos/warrenzhu25/spark/{archive_format}{/ref}', 'downloads_url': 'https://api.github.com/repos/warrenzhu25/spark/downloads', 'issues_url': 'https://api.github.com/repos/warrenzhu25/spark/issues{/number}', 'pulls_url': 'https://api.github.com/repos/warrenzhu25/spark/pulls{/number}', 'milestones_url': 'https://api.github.com/repos/warrenzhu25/spark/milestones{/number}', 'notifications_url': 'https://api.github.com/repos/warrenzhu25/spark/notifications{?since,all,participating}', 'labels_url': 'https://api.github.com/repos/warrenzhu25/spark/labels{/name}', 'releases_url': 'https://api.github.com/repos/warrenzhu25/spark/releases{/id}', 'deployments_url': 'https://api.github.com/repos/warrenzhu25/spark/deployments', 'created_at': '2019-10-25T23:12:23Z', 'updated_at': '2020-07-10T17:08:49Z', 'pushed_at': '2021-09-30T02:42:29Z', 'git_url': 'git://github.com/warrenzhu25/spark.git', 'ssh_url': 'git@github.com:warrenzhu25/spark.git', 'clone_url': 'https://github.com/warrenzhu25/spark.git', 'svn_url': 'https://github.com/warrenzhu25/spark', 'homepage': '', 'size': 385122, 'stargazers_count': 0, 'watchers_count': 0, 'language': 'Scala', 'has_issues': False, 'has_projects': True, 'has_downloads': True, 'has_wiki': False, 'has_pages': False, 'forks_count': 0, 'mirror_url': None, 'archived': False, 'disabled': False, 'open_issues_count': 0, 'license': {'key': 'apache-2.0', 'name': 'Apache License 2.0', 'spdx_id': 'Apache-2.0', 'url': 'https://api.github.com/licenses/apache-2.0', 'node_id': 'MDc6TGljZW5zZTI='}, 'allow_forking': True, 'is_template': False, 'topics': [], 'visibility': 'public', 'forks': 0, 'open_issues': 0, 'watchers': 0, 'default_branch': 'master'}}, 'base': {'label': 'apache:master', 'ref': 'master', 'sha': 'f9644cc2538dbcfbfc4f844bf0b50b17bb8f315d', 'user': {'login': 'apache', 'id': 47359, 'node_id': 'MDEyOk9yZ2FuaXphdGlvbjQ3MzU5', 'avatar_url': 'https://avatars.githubusercontent.com/u/47359?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/apache', 'html_url': 'https://github.com/apache', 'followers_url': 'https://api.github.com/users/apache/followers', 'following_url': 'https://api.github.com/users/apache/following{/other_user}', 'gists_url': 'https://api.github.com/users/apache/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/apache/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/apache/subscriptions', 'organizations_url': 'https://api.github.com/users/apache/orgs', 'repos_url': 'https://api.github.com/users/apache/repos', 'events_url': 'https://api.github.com/users/apache/events{/privacy}', 'received_events_url': 'https://api.github.com/users/apache/received_events', 'type': 'Organization', 'site_admin': False}, 'repo': {'id': 17165658, 'node_id': 'MDEwOlJlcG9zaXRvcnkxNzE2NTY1OA==', 'name': 'spark', 'full_name': 'apache/spark', 'private': False, 'owner': {'login': 'apache', 'id': 47359, 'node_id': 'MDEyOk9yZ2FuaXphdGlvbjQ3MzU5', 'avatar_url': 'https://avatars.githubusercontent.com/u/47359?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/apache', 'html_url': 'https://github.com/apache', 'followers_url': 'https://api.github.com/users/apache/followers', 'following_url': 'https://api.github.com/users/apache/following{/other_user}', 'gists_url': 'https://api.github.com/users/apache/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/apache/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/apache/subscriptions', 'organizations_url': 'https://api.github.com/users/apache/orgs', 'repos_url': 'https://api.github.com/users/apache/repos', 'events_url': 'https://api.github.com/users/apache/events{/privacy}', 'received_events_url': 'https://api.github.com/users/apache/received_events', 'type': 'Organization', 'site_admin': False}, 'html_url': 'https://github.com/apache/spark', 'description': 'Apache Spark - A unified analytics engine for large-scale data processing', 'fork': False, 'url': 'https://api.github.com/repos/apache/spark', 'forks_url': 'https://api.github.com/repos/apache/spark/forks', 'keys_url': 'https://api.github.com/repos/apache/spark/keys{/key_id}', 'collaborators_url': 'https://api.github.com/repos/apache/spark/collaborators{/collaborator}', 'teams_url': 'https://api.github.com/repos/apache/spark/teams', 'hooks_url': 'https://api.github.com/repos/apache/spark/hooks', 'issue_events_url': 'https://api.github.com/repos/apache/spark/issues/events{/number}', 'events_url': 'https://api.github.com/repos/apache/spark/events', 'assignees_url': 'https://api.github.com/repos/apache/spark/assignees{/user}', 'branches_url': 'https://api.github.com/repos/apache/spark/branches{/branch}', 'tags_url': 'https://api.github.com/repos/apache/spark/tags', 'blobs_url': 'https://api.github.com/repos/apache/spark/git/blobs{/sha}', 'git_tags_url': 'https://api.github.com/repos/apache/spark/git/tags{/sha}', 'git_refs_url': 'https://api.github.com/repos/apache/spark/git/refs{/sha}', 'trees_url': 'https://api.github.com/repos/apache/spark/git/trees{/sha}', 'statuses_url': 'https://api.github.com/repos/apache/spark/statuses/{sha}', 'languages_url': 'https://api.github.com/repos/apache/spark/languages', 'stargazers_url': 'https://api.github.com/repos/apache/spark/stargazers', 'contributors_url': 'https://api.github.com/repos/apache/spark/contributors', 'subscribers_url': 'https://api.github.com/repos/apache/spark/subscribers', 'subscription_url': 'https://api.github.com/repos/apache/spark/subscription', 'commits_url': 'https://api.github.com/repos/apache/spark/commits{/sha}', 'git_commits_url': 'https://api.github.com/repos/apache/spark/git/commits{/sha}', 'comments_url': 'https://api.github.com/repos/apache/spark/comments{/number}', 'issue_comment_url': 'https://api.github.com/repos/apache/spark/issues/comments{/number}', 'contents_url': 'https://api.github.com/repos/apache/spark/contents/{+path}', 'compare_url': 'https://api.github.com/repos/apache/spark/compare/{base}...{head}', 'merges_url': 'https://api.github.com/repos/apache/spark/merges', 'archive_url': 'https://api.github.com/repos/apache/spark/{archive_format}{/ref}', 'downloads_url': 'https://api.github.com/repos/apache/spark/downloads', 'issues_url': 'https://api.github.com/repos/apache/spark/issues{/number}', 'pulls_url': 'https://api.github.com/repos/apache/spark/pulls{/number}', 'milestones_url': 'https://api.github.com/repos/apache/spark/milestones{/number}', 'notifications_url': 'https://api.github.com/repos/apache/spark/notifications{?since,all,participating}', 'labels_url': 'https://api.github.com/repos/apache/spark/labels{/name}', 'releases_url': 'https://api.github.com/repos/apache/spark/releases{/id}', 'deployments_url': 'https://api.github.com/repos/apache/spark/deployments', 'created_at': '2014-02-25T08:00:08Z', 'updated_at': '2022-02-10T08:49:32Z', 'pushed_at': '2022-02-10T08:44:55Z', 'git_url': 'git://github.com/apache/spark.git', 'ssh_url': 'git@github.com:apache/spark.git', 'clone_url': 'https://github.com/apache/spark.git', 'svn_url': 'https://github.com/apache/spark', 'homepage': 'https://spark.apache.org/', 'size': 406520, 'stargazers_count': 32023, 'watchers_count': 32023, 'language': 'Scala', 'has_issues': False, 'has_projects': True, 'has_downloads': True, 'has_wiki': False, 'has_pages': False, 'forks_count': 25196, 'mirror_url': None, 'archived': False, 'disabled': False, 'open_issues_count': 251, 'license': {'key': 'apache-2.0', 'name': 'Apache License 2.0', 'spdx_id': 'Apache-2.0', 'url': 'https://api.github.com/licenses/apache-2.0', 'node_id': 'MDc6TGljZW5zZTI='}, 'allow_forking': True, 'is_template': False, 'topics': ['big-data', 'java', 'jdbc', 'python', 'r', 'scala', 'spark', 'sql'], 'visibility': 'public', 'forks': 25196, 'open_issues': 251, 'watchers': 32023, 'default_branch': 'master'}}, '_links': {'self': {'href': 'https://api.github.com/repos/apache/spark/pulls/34035'}, 'html': {'href': 'https://github.com/apache/spark/pull/34035'}, 'issue': {'href': 'https://api.github.com/repos/apache/spark/issues/34035'}, 'comments': {'href': 'https://api.github.com/repos/apache/spark/issues/34035/comments'}, 'review_comments': {'href': 'https://api.github.com/repos/apache/spark/pulls/34035/comments'}, 'review_comment': {'href': 'https://api.github.com/repos/apache/spark/pulls/comments{/number}'}, 'commits': {'href': 'https://api.github.com/repos/apache/spark/pulls/34035/commits'}, 'statuses': {'href': 'https://api.github.com/repos/apache/spark/statuses/f9057749f7d7d25d396c03a8041a0a55e97148ab'}}, 'author_association': 'CONTRIBUTOR', 'auto_merge': None, 'active_lock_reason': None}}",created,Scala
20169268554,PullRequestReviewEvent,apache/spark,0.0,2022-02-10T08:55:14Z,"{'action': 'created', 'review': {'id': 878509833, 'node_id': 'PRR_kwDOAQXtWs40XP8J', 'user': {'login': 'warrenzhu25', 'id': 1633312, 'node_id': 'MDQ6VXNlcjE2MzMzMTI=', 'avatar_url': 'https://avatars.githubusercontent.com/u/1633312?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/warrenzhu25', 'html_url': 'https://github.com/warrenzhu25', 'followers_url': 'https://api.github.com/users/warrenzhu25/followers', 'following_url': 'https://api.github.com/users/warrenzhu25/following{/other_user}', 'gists_url': 'https://api.github.com/users/warrenzhu25/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/warrenzhu25/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/warrenzhu25/subscriptions', 'organizations_url': 'https://api.github.com/users/warrenzhu25/orgs', 'repos_url': 'https://api.github.com/users/warrenzhu25/repos', 'events_url': 'https://api.github.com/users/warrenzhu25/events{/privacy}', 'received_events_url': 'https://api.github.com/users/warrenzhu25/received_events', 'type': 'User', 'site_admin': False}, 'body': None, 'commit_id': 'f9057749f7d7d25d396c03a8041a0a55e97148ab', 'submitted_at': '2022-02-10T08:55:14Z', 'state': 'commented', 'html_url': 'https://github.com/apache/spark/pull/34035#pullrequestreview-878509833', 'pull_request_url': 'https://api.github.com/repos/apache/spark/pulls/34035', 'author_association': 'CONTRIBUTOR', '_links': {'html': {'href': 'https://github.com/apache/spark/pull/34035#pullrequestreview-878509833'}, 'pull_request': {'href': 'https://api.github.com/repos/apache/spark/pulls/34035'}}}, 'pull_request': {'url': 'https://api.github.com/repos/apache/spark/pulls/34035', 'id': 736626805, 'node_id': 'PR_kwDOAQXtWs4r6Ah1', 'html_url': 'https://github.com/apache/spark/pull/34035', 'diff_url': 'https://github.com/apache/spark/pull/34035.diff', 'patch_url': 'https://github.com/apache/spark/pull/34035.patch', 'issue_url': 'https://api.github.com/repos/apache/spark/issues/34035', 'number': 34035, 'state': 'closed', 'locked': False, 'title': '[SPARK-36793][K8S] Support write container stdout/stderr to file', 'user': {'login': 'warrenzhu25', 'id': 1633312, 'node_id': 'MDQ6VXNlcjE2MzMzMTI=', 'avatar_url': 'https://avatars.githubusercontent.com/u/1633312?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/warrenzhu25', 'html_url': 'https://github.com/warrenzhu25', 'followers_url': 'https://api.github.com/users/warrenzhu25/followers', 'following_url': 'https://api.github.com/users/warrenzhu25/following{/other_user}', 'gists_url': 'https://api.github.com/users/warrenzhu25/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/warrenzhu25/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/warrenzhu25/subscriptions', 'organizations_url': 'https://api.github.com/users/warrenzhu25/orgs', 'repos_url': 'https://api.github.com/users/warrenzhu25/repos', 'events_url': 'https://api.github.com/users/warrenzhu25/events{/privacy}', 'received_events_url': 'https://api.github.com/users/warrenzhu25/received_events', 'type': 'User', 'site_admin': False}, 'body': '### What changes were proposed in this pull request?\r\nSupport write container stdout/stderr to file\r\n\r\n### Why are the changes needed?\r\nIf users want to sidecar logging agent to send stdout/stderr to external log storage,  only way is to change entrypoint.sh, which might break compatibility with community version.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nYes. User can enable this feature by spark config.\r\n\r\n### How was this patch tested?\r\nAdded UT in BasicDriverFeatureStepSuite and BasicExecutorFeatureStepSuite\r\n', 'created_at': '2021-09-17T19:33:38Z', 'updated_at': '2022-02-10T08:55:14Z', 'closed_at': '2022-01-03T00:12:12Z', 'merged_at': None, 'merge_commit_sha': 'fc1b5f6e4aefb0d79e9069cef3250031991775ab', 'assignee': None, 'assignees': [], 'requested_reviewers': [], 'requested_teams': [], 'labels': [{'id': 1406605057, 'node_id': 'MDU6TGFiZWwxNDA2NjA1MDU3', 'url': 'https://api.github.com/repos/apache/spark/labels/KUBERNETES', 'name': 'KUBERNETES', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1753478521, 'node_id': 'MDU6TGFiZWwxNzUzNDc4NTIx', 'url': 'https://api.github.com/repos/apache/spark/labels/Stale', 'name': 'Stale', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981471081, 'node_id': 'MDU6TGFiZWwxOTgxNDcxMDgx', 'url': 'https://api.github.com/repos/apache/spark/labels/DOCS', 'name': 'DOCS', 'color': 'ededed', 'default': False, 'description': None}], 'milestone': None, 'draft': False, 'commits_url': 'https://api.github.com/repos/apache/spark/pulls/34035/commits', 'review_comments_url': 'https://api.github.com/repos/apache/spark/pulls/34035/comments', 'review_comment_url': 'https://api.github.com/repos/apache/spark/pulls/comments{/number}', 'comments_url': 'https://api.github.com/repos/apache/spark/issues/34035/comments', 'statuses_url': 'https://api.github.com/repos/apache/spark/statuses/f9057749f7d7d25d396c03a8041a0a55e97148ab', 'head': {'label': 'warrenzhu25:k8s-log', 'ref': 'k8s-log', 'sha': 'f9057749f7d7d25d396c03a8041a0a55e97148ab', 'user': {'login': 'warrenzhu25', 'id': 1633312, 'node_id': 'MDQ6VXNlcjE2MzMzMTI=', 'avatar_url': 'https://avatars.githubusercontent.com/u/1633312?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/warrenzhu25', 'html_url': 'https://github.com/warrenzhu25', 'followers_url': 'https://api.github.com/users/warrenzhu25/followers', 'following_url': 'https://api.github.com/users/warrenzhu25/following{/other_user}', 'gists_url': 'https://api.github.com/users/warrenzhu25/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/warrenzhu25/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/warrenzhu25/subscriptions', 'organizations_url': 'https://api.github.com/users/warrenzhu25/orgs', 'repos_url': 'https://api.github.com/users/warrenzhu25/repos', 'events_url': 'https://api.github.com/users/warrenzhu25/events{/privacy}', 'received_events_url': 'https://api.github.com/users/warrenzhu25/received_events', 'type': 'User', 'site_admin': False}, 'repo': {'id': 217627454, 'node_id': 'MDEwOlJlcG9zaXRvcnkyMTc2Mjc0NTQ=', 'name': 'spark', 'full_name': 'warrenzhu25/spark', 'private': False, 'owner': {'login': 'warrenzhu25', 'id': 1633312, 'node_id': 'MDQ6VXNlcjE2MzMzMTI=', 'avatar_url': 'https://avatars.githubusercontent.com/u/1633312?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/warrenzhu25', 'html_url': 'https://github.com/warrenzhu25', 'followers_url': 'https://api.github.com/users/warrenzhu25/followers', 'following_url': 'https://api.github.com/users/warrenzhu25/following{/other_user}', 'gists_url': 'https://api.github.com/users/warrenzhu25/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/warrenzhu25/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/warrenzhu25/subscriptions', 'organizations_url': 'https://api.github.com/users/warrenzhu25/orgs', 'repos_url': 'https://api.github.com/users/warrenzhu25/repos', 'events_url': 'https://api.github.com/users/warrenzhu25/events{/privacy}', 'received_events_url': 'https://api.github.com/users/warrenzhu25/received_events', 'type': 'User', 'site_admin': False}, 'html_url': 'https://github.com/warrenzhu25/spark', 'description': 'Apache Spark', 'fork': True, 'url': 'https://api.github.com/repos/warrenzhu25/spark', 'forks_url': 'https://api.github.com/repos/warrenzhu25/spark/forks', 'keys_url': 'https://api.github.com/repos/warrenzhu25/spark/keys{/key_id}', 'collaborators_url': 'https://api.github.com/repos/warrenzhu25/spark/collaborators{/collaborator}', 'teams_url': 'https://api.github.com/repos/warrenzhu25/spark/teams', 'hooks_url': 'https://api.github.com/repos/warrenzhu25/spark/hooks', 'issue_events_url': 'https://api.github.com/repos/warrenzhu25/spark/issues/events{/number}', 'events_url': 'https://api.github.com/repos/warrenzhu25/spark/events', 'assignees_url': 'https://api.github.com/repos/warrenzhu25/spark/assignees{/user}', 'branches_url': 'https://api.github.com/repos/warrenzhu25/spark/branches{/branch}', 'tags_url': 'https://api.github.com/repos/warrenzhu25/spark/tags', 'blobs_url': 'https://api.github.com/repos/warrenzhu25/spark/git/blobs{/sha}', 'git_tags_url': 'https://api.github.com/repos/warrenzhu25/spark/git/tags{/sha}', 'git_refs_url': 'https://api.github.com/repos/warrenzhu25/spark/git/refs{/sha}', 'trees_url': 'https://api.github.com/repos/warrenzhu25/spark/git/trees{/sha}', 'statuses_url': 'https://api.github.com/repos/warrenzhu25/spark/statuses/{sha}', 'languages_url': 'https://api.github.com/repos/warrenzhu25/spark/languages', 'stargazers_url': 'https://api.github.com/repos/warrenzhu25/spark/stargazers', 'contributors_url': 'https://api.github.com/repos/warrenzhu25/spark/contributors', 'subscribers_url': 'https://api.github.com/repos/warrenzhu25/spark/subscribers', 'subscription_url': 'https://api.github.com/repos/warrenzhu25/spark/subscription', 'commits_url': 'https://api.github.com/repos/warrenzhu25/spark/commits{/sha}', 'git_commits_url': 'https://api.github.com/repos/warrenzhu25/spark/git/commits{/sha}', 'comments_url': 'https://api.github.com/repos/warrenzhu25/spark/comments{/number}', 'issue_comment_url': 'https://api.github.com/repos/warrenzhu25/spark/issues/comments{/number}', 'contents_url': 'https://api.github.com/repos/warrenzhu25/spark/contents/{+path}', 'compare_url': 'https://api.github.com/repos/warrenzhu25/spark/compare/{base}...{head}', 'merges_url': 'https://api.github.com/repos/warrenzhu25/spark/merges', 'archive_url': 'https://api.github.com/repos/warrenzhu25/spark/{archive_format}{/ref}', 'downloads_url': 'https://api.github.com/repos/warrenzhu25/spark/downloads', 'issues_url': 'https://api.github.com/repos/warrenzhu25/spark/issues{/number}', 'pulls_url': 'https://api.github.com/repos/warrenzhu25/spark/pulls{/number}', 'milestones_url': 'https://api.github.com/repos/warrenzhu25/spark/milestones{/number}', 'notifications_url': 'https://api.github.com/repos/warrenzhu25/spark/notifications{?since,all,participating}', 'labels_url': 'https://api.github.com/repos/warrenzhu25/spark/labels{/name}', 'releases_url': 'https://api.github.com/repos/warrenzhu25/spark/releases{/id}', 'deployments_url': 'https://api.github.com/repos/warrenzhu25/spark/deployments', 'created_at': '2019-10-25T23:12:23Z', 'updated_at': '2020-07-10T17:08:49Z', 'pushed_at': '2021-09-30T02:42:29Z', 'git_url': 'git://github.com/warrenzhu25/spark.git', 'ssh_url': 'git@github.com:warrenzhu25/spark.git', 'clone_url': 'https://github.com/warrenzhu25/spark.git', 'svn_url': 'https://github.com/warrenzhu25/spark', 'homepage': '', 'size': 385122, 'stargazers_count': 0, 'watchers_count': 0, 'language': 'Scala', 'has_issues': False, 'has_projects': True, 'has_downloads': True, 'has_wiki': False, 'has_pages': False, 'forks_count': 0, 'mirror_url': None, 'archived': False, 'disabled': False, 'open_issues_count': 0, 'license': {'key': 'apache-2.0', 'name': 'Apache License 2.0', 'spdx_id': 'Apache-2.0', 'url': 'https://api.github.com/licenses/apache-2.0', 'node_id': 'MDc6TGljZW5zZTI='}, 'allow_forking': True, 'is_template': False, 'topics': [], 'visibility': 'public', 'forks': 0, 'open_issues': 0, 'watchers': 0, 'default_branch': 'master'}}, 'base': {'label': 'apache:master', 'ref': 'master', 'sha': 'f9644cc2538dbcfbfc4f844bf0b50b17bb8f315d', 'user': {'login': 'apache', 'id': 47359, 'node_id': 'MDEyOk9yZ2FuaXphdGlvbjQ3MzU5', 'avatar_url': 'https://avatars.githubusercontent.com/u/47359?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/apache', 'html_url': 'https://github.com/apache', 'followers_url': 'https://api.github.com/users/apache/followers', 'following_url': 'https://api.github.com/users/apache/following{/other_user}', 'gists_url': 'https://api.github.com/users/apache/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/apache/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/apache/subscriptions', 'organizations_url': 'https://api.github.com/users/apache/orgs', 'repos_url': 'https://api.github.com/users/apache/repos', 'events_url': 'https://api.github.com/users/apache/events{/privacy}', 'received_events_url': 'https://api.github.com/users/apache/received_events', 'type': 'Organization', 'site_admin': False}, 'repo': {'id': 17165658, 'node_id': 'MDEwOlJlcG9zaXRvcnkxNzE2NTY1OA==', 'name': 'spark', 'full_name': 'apache/spark', 'private': False, 'owner': {'login': 'apache', 'id': 47359, 'node_id': 'MDEyOk9yZ2FuaXphdGlvbjQ3MzU5', 'avatar_url': 'https://avatars.githubusercontent.com/u/47359?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/apache', 'html_url': 'https://github.com/apache', 'followers_url': 'https://api.github.com/users/apache/followers', 'following_url': 'https://api.github.com/users/apache/following{/other_user}', 'gists_url': 'https://api.github.com/users/apache/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/apache/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/apache/subscriptions', 'organizations_url': 'https://api.github.com/users/apache/orgs', 'repos_url': 'https://api.github.com/users/apache/repos', 'events_url': 'https://api.github.com/users/apache/events{/privacy}', 'received_events_url': 'https://api.github.com/users/apache/received_events', 'type': 'Organization', 'site_admin': False}, 'html_url': 'https://github.com/apache/spark', 'description': 'Apache Spark - A unified analytics engine for large-scale data processing', 'fork': False, 'url': 'https://api.github.com/repos/apache/spark', 'forks_url': 'https://api.github.com/repos/apache/spark/forks', 'keys_url': 'https://api.github.com/repos/apache/spark/keys{/key_id}', 'collaborators_url': 'https://api.github.com/repos/apache/spark/collaborators{/collaborator}', 'teams_url': 'https://api.github.com/repos/apache/spark/teams', 'hooks_url': 'https://api.github.com/repos/apache/spark/hooks', 'issue_events_url': 'https://api.github.com/repos/apache/spark/issues/events{/number}', 'events_url': 'https://api.github.com/repos/apache/spark/events', 'assignees_url': 'https://api.github.com/repos/apache/spark/assignees{/user}', 'branches_url': 'https://api.github.com/repos/apache/spark/branches{/branch}', 'tags_url': 'https://api.github.com/repos/apache/spark/tags', 'blobs_url': 'https://api.github.com/repos/apache/spark/git/blobs{/sha}', 'git_tags_url': 'https://api.github.com/repos/apache/spark/git/tags{/sha}', 'git_refs_url': 'https://api.github.com/repos/apache/spark/git/refs{/sha}', 'trees_url': 'https://api.github.com/repos/apache/spark/git/trees{/sha}', 'statuses_url': 'https://api.github.com/repos/apache/spark/statuses/{sha}', 'languages_url': 'https://api.github.com/repos/apache/spark/languages', 'stargazers_url': 'https://api.github.com/repos/apache/spark/stargazers', 'contributors_url': 'https://api.github.com/repos/apache/spark/contributors', 'subscribers_url': 'https://api.github.com/repos/apache/spark/subscribers', 'subscription_url': 'https://api.github.com/repos/apache/spark/subscription', 'commits_url': 'https://api.github.com/repos/apache/spark/commits{/sha}', 'git_commits_url': 'https://api.github.com/repos/apache/spark/git/commits{/sha}', 'comments_url': 'https://api.github.com/repos/apache/spark/comments{/number}', 'issue_comment_url': 'https://api.github.com/repos/apache/spark/issues/comments{/number}', 'contents_url': 'https://api.github.com/repos/apache/spark/contents/{+path}', 'compare_url': 'https://api.github.com/repos/apache/spark/compare/{base}...{head}', 'merges_url': 'https://api.github.com/repos/apache/spark/merges', 'archive_url': 'https://api.github.com/repos/apache/spark/{archive_format}{/ref}', 'downloads_url': 'https://api.github.com/repos/apache/spark/downloads', 'issues_url': 'https://api.github.com/repos/apache/spark/issues{/number}', 'pulls_url': 'https://api.github.com/repos/apache/spark/pulls{/number}', 'milestones_url': 'https://api.github.com/repos/apache/spark/milestones{/number}', 'notifications_url': 'https://api.github.com/repos/apache/spark/notifications{?since,all,participating}', 'labels_url': 'https://api.github.com/repos/apache/spark/labels{/name}', 'releases_url': 'https://api.github.com/repos/apache/spark/releases{/id}', 'deployments_url': 'https://api.github.com/repos/apache/spark/deployments', 'created_at': '2014-02-25T08:00:08Z', 'updated_at': '2022-02-10T08:49:32Z', 'pushed_at': '2022-02-10T08:44:55Z', 'git_url': 'git://github.com/apache/spark.git', 'ssh_url': 'git@github.com:apache/spark.git', 'clone_url': 'https://github.com/apache/spark.git', 'svn_url': 'https://github.com/apache/spark', 'homepage': 'https://spark.apache.org/', 'size': 406520, 'stargazers_count': 32023, 'watchers_count': 32023, 'language': 'Scala', 'has_issues': False, 'has_projects': True, 'has_downloads': True, 'has_wiki': False, 'has_pages': False, 'forks_count': 25196, 'mirror_url': None, 'archived': False, 'disabled': False, 'open_issues_count': 251, 'license': {'key': 'apache-2.0', 'name': 'Apache License 2.0', 'spdx_id': 'Apache-2.0', 'url': 'https://api.github.com/licenses/apache-2.0', 'node_id': 'MDc6TGljZW5zZTI='}, 'allow_forking': True, 'is_template': False, 'topics': ['big-data', 'java', 'jdbc', 'python', 'r', 'scala', 'spark', 'sql'], 'visibility': 'public', 'forks': 25196, 'open_issues': 251, 'watchers': 32023, 'default_branch': 'master'}}, '_links': {'self': {'href': 'https://api.github.com/repos/apache/spark/pulls/34035'}, 'html': {'href': 'https://github.com/apache/spark/pull/34035'}, 'issue': {'href': 'https://api.github.com/repos/apache/spark/issues/34035'}, 'comments': {'href': 'https://api.github.com/repos/apache/spark/issues/34035/comments'}, 'review_comments': {'href': 'https://api.github.com/repos/apache/spark/pulls/34035/comments'}, 'review_comment': {'href': 'https://api.github.com/repos/apache/spark/pulls/comments{/number}'}, 'commits': {'href': 'https://api.github.com/repos/apache/spark/pulls/34035/commits'}, 'statuses': {'href': 'https://api.github.com/repos/apache/spark/statuses/f9057749f7d7d25d396c03a8041a0a55e97148ab'}}, 'author_association': 'CONTRIBUTOR', 'auto_merge': None, 'active_lock_reason': None}}",created,Scala
20168203153,IssueCommentEvent,apache/spark,0.0,2022-02-10T07:49:32Z,"{'action': 'created', 'issue': {'url': 'https://api.github.com/repos/apache/spark/issues/30175', 'repository_url': 'https://api.github.com/repos/apache/spark', 'labels_url': 'https://api.github.com/repos/apache/spark/issues/30175/labels{/name}', 'comments_url': 'https://api.github.com/repos/apache/spark/issues/30175/comments', 'events_url': 'https://api.github.com/repos/apache/spark/issues/30175/events', 'html_url': 'https://github.com/apache/spark/pull/30175', 'id': 731606024, 'node_id': 'MDExOlB1bGxSZXF1ZXN0NTExNjgzMzE0', 'number': 30175, 'title': '[SPARK-33274][SS] Stop query in cp mode when total cores less than total kafka partition', 'user': {'login': 'warrenzhu25', 'id': 1633312, 'node_id': 'MDQ6VXNlcjE2MzMzMTI=', 'avatar_url': 'https://avatars.githubusercontent.com/u/1633312?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/warrenzhu25', 'html_url': 'https://github.com/warrenzhu25', 'followers_url': 'https://api.github.com/users/warrenzhu25/followers', 'following_url': 'https://api.github.com/users/warrenzhu25/following{/other_user}', 'gists_url': 'https://api.github.com/users/warrenzhu25/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/warrenzhu25/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/warrenzhu25/subscriptions', 'organizations_url': 'https://api.github.com/users/warrenzhu25/orgs', 'repos_url': 'https://api.github.com/users/warrenzhu25/repos', 'events_url': 'https://api.github.com/users/warrenzhu25/events{/privacy}', 'received_events_url': 'https://api.github.com/users/warrenzhu25/received_events', 'type': 'User', 'site_admin': False}, 'labels': [{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1406587328, 'node_id': 'MDU6TGFiZWwxNDA2NTg3MzI4', 'url': 'https://api.github.com/repos/apache/spark/labels/STRUCTURED%20STREAMING', 'name': 'STRUCTURED STREAMING', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1753478521, 'node_id': 'MDU6TGFiZWwxNzUzNDc4NTIx', 'url': 'https://api.github.com/repos/apache/spark/labels/Stale', 'name': 'Stale', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}], 'state': 'closed', 'locked': False, 'assignee': None, 'assignees': [], 'milestone': None, 'comments': 39, 'created_at': '2020-10-28T16:35:58Z', 'updated_at': '2022-02-10T07:49:32Z', 'closed_at': '2021-06-08T00:19:21Z', 'author_association': 'CONTRIBUTOR', 'active_lock_reason': None, 'draft': False, 'pull_request': {'url': 'https://api.github.com/repos/apache/spark/pulls/30175', 'html_url': 'https://github.com/apache/spark/pull/30175', 'diff_url': 'https://github.com/apache/spark/pull/30175.diff', 'patch_url': 'https://github.com/apache/spark/pull/30175.patch', 'merged_at': None}, 'body': ""### What changes were proposed in this pull request?\r\nAdd check for total executor cores when `SetReaderPartitions` message received.\r\n\r\n### Why are the changes needed?\r\nIn continuous processing mode, EpochCoordinator won't add offsets to query until got ReportPartitionOffset from all partitions. Normally, each kafka topic partition will be handled by one core, if total cores is smaller than total kafka topic partition counts, the job will hang without any error message.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nYes, if total executor cores is smaller than total kafka partition count, the exception with below error will be thrown:\r\n`Total %s (kafka partitions) * %s (cpus per task) = %s needed, \r\nbut only have %s (executors) * %s (cores per executor) = %s (total cores).\r\nPlease increase total number of executor cores to at least %s.`\r\n\r\n### How was this patch tested?\r\nAdded test in EpochCoordinatorSuite\r\n"", 'reactions': {'url': 'https://api.github.com/repos/apache/spark/issues/30175/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/apache/spark/issues/30175/timeline', 'performed_via_github_app': None}, 'comment': {'url': 'https://api.github.com/repos/apache/spark/issues/comments/1034594248', 'html_url': 'https://github.com/apache/spark/pull/30175#issuecomment-1034594248', 'issue_url': 'https://api.github.com/repos/apache/spark/issues/30175', 'id': 1034594248, 'node_id': 'IC_kwDOAQXtWs49qqfI', 'user': {'login': 'warrenzhu25', 'id': 1633312, 'node_id': 'MDQ6VXNlcjE2MzMzMTI=', 'avatar_url': 'https://avatars.githubusercontent.com/u/1633312?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/warrenzhu25', 'html_url': 'https://github.com/warrenzhu25', 'followers_url': 'https://api.github.com/users/warrenzhu25/followers', 'following_url': 'https://api.github.com/users/warrenzhu25/following{/other_user}', 'gists_url': 'https://api.github.com/users/warrenzhu25/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/warrenzhu25/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/warrenzhu25/subscriptions', 'organizations_url': 'https://api.github.com/users/warrenzhu25/orgs', 'repos_url': 'https://api.github.com/users/warrenzhu25/repos', 'events_url': 'https://api.github.com/users/warrenzhu25/events{/privacy}', 'received_events_url': 'https://api.github.com/users/warrenzhu25/received_events', 'type': 'User', 'site_admin': False}, 'created_at': '2022-02-10T07:49:31Z', 'updated_at': '2022-02-10T07:49:31Z', 'author_association': 'CONTRIBUTOR', 'body': '@viirya @Ngone51 Could you help take a look and remove `stale` label?', 'reactions': {'url': 'https://api.github.com/repos/apache/spark/issues/comments/1034594248/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'performed_via_github_app': None}}",created,Scala
20168154709,IssueCommentEvent,apache/spark,0.0,2022-02-10T07:46:08Z,"{'action': 'created', 'issue': {'url': 'https://api.github.com/repos/apache/spark/issues/31869', 'repository_url': 'https://api.github.com/repos/apache/spark', 'labels_url': 'https://api.github.com/repos/apache/spark/issues/31869/labels{/name}', 'comments_url': 'https://api.github.com/repos/apache/spark/issues/31869/comments', 'events_url': 'https://api.github.com/repos/apache/spark/issues/31869/events', 'html_url': 'https://github.com/apache/spark/pull/31869', 'id': 834005667, 'node_id': 'MDExOlB1bGxSZXF1ZXN0NTk0OTA4ODky', 'number': 31869, 'title': '[SPARK-34777][UI] StagePage input/output size records not show when records greater than zero', 'user': {'login': 'warrenzhu25', 'id': 1633312, 'node_id': 'MDQ6VXNlcjE2MzMzMTI=', 'avatar_url': 'https://avatars.githubusercontent.com/u/1633312?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/warrenzhu25', 'html_url': 'https://github.com/warrenzhu25', 'followers_url': 'https://api.github.com/users/warrenzhu25/followers', 'following_url': 'https://api.github.com/users/warrenzhu25/following{/other_user}', 'gists_url': 'https://api.github.com/users/warrenzhu25/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/warrenzhu25/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/warrenzhu25/subscriptions', 'organizations_url': 'https://api.github.com/users/warrenzhu25/orgs', 'repos_url': 'https://api.github.com/users/warrenzhu25/repos', 'events_url': 'https://api.github.com/users/warrenzhu25/events{/privacy}', 'received_events_url': 'https://api.github.com/users/warrenzhu25/received_events', 'type': 'User', 'site_admin': False}, 'labels': [{'id': 1406605079, 'node_id': 'MDU6TGFiZWwxNDA2NjA1MDc5', 'url': 'https://api.github.com/repos/apache/spark/labels/WEB%20UI', 'name': 'WEB UI', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1753478521, 'node_id': 'MDU6TGFiZWwxNzUzNDc4NTIx', 'url': 'https://api.github.com/repos/apache/spark/labels/Stale', 'name': 'Stale', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}], 'state': 'closed', 'locked': False, 'assignee': None, 'assignees': [], 'milestone': None, 'comments': 4, 'created_at': '2021-03-17T17:10:59Z', 'updated_at': '2022-02-10T07:46:07Z', 'closed_at': '2021-09-13T00:09:23Z', 'author_association': 'CONTRIBUTOR', 'active_lock_reason': None, 'draft': False, 'pull_request': {'url': 'https://api.github.com/repos/apache/spark/pulls/31869', 'html_url': 'https://github.com/apache/spark/pull/31869', 'diff_url': 'https://github.com/apache/spark/pull/31869.diff', 'patch_url': 'https://github.com/apache/spark/pull/31869.patch', 'merged_at': None}, 'body': '### What changes were proposed in this pull request?\r\nDetermine whether show input/output size and records based on either has value, rather than only size.\r\n\r\nBefore:\r\n![image](https://user-images.githubusercontent.com/1633312/113449257-48d03b00-93b2-11eb-9f48-f473ca51cbf3.png)\r\n\r\nAfter:\r\n![image](https://user-images.githubusercontent.com/1633312/113449020-ce072000-93b1-11eb-9ecb-bf568f5ace48.png)\r\n\r\n\r\n### Why are the changes needed?\r\nStage page UI not show input/output size and records even when records greater than zero. This is common when spark streaming job read from kafka source.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nNo\r\n\r\n### How was this patch tested?\r\nManually\r\n', 'reactions': {'url': 'https://api.github.com/repos/apache/spark/issues/31869/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/apache/spark/issues/31869/timeline', 'performed_via_github_app': None}, 'comment': {'url': 'https://api.github.com/repos/apache/spark/issues/comments/1034591698', 'html_url': 'https://github.com/apache/spark/pull/31869#issuecomment-1034591698', 'issue_url': 'https://api.github.com/repos/apache/spark/issues/31869', 'id': 1034591698, 'node_id': 'IC_kwDOAQXtWs49qp3S', 'user': {'login': 'warrenzhu25', 'id': 1633312, 'node_id': 'MDQ6VXNlcjE2MzMzMTI=', 'avatar_url': 'https://avatars.githubusercontent.com/u/1633312?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/warrenzhu25', 'html_url': 'https://github.com/warrenzhu25', 'followers_url': 'https://api.github.com/users/warrenzhu25/followers', 'following_url': 'https://api.github.com/users/warrenzhu25/following{/other_user}', 'gists_url': 'https://api.github.com/users/warrenzhu25/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/warrenzhu25/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/warrenzhu25/subscriptions', 'organizations_url': 'https://api.github.com/users/warrenzhu25/orgs', 'repos_url': 'https://api.github.com/users/warrenzhu25/repos', 'events_url': 'https://api.github.com/users/warrenzhu25/events{/privacy}', 'received_events_url': 'https://api.github.com/users/warrenzhu25/received_events', 'type': 'User', 'site_admin': False}, 'created_at': '2022-02-10T07:46:07Z', 'updated_at': '2022-02-10T07:46:07Z', 'author_association': 'CONTRIBUTOR', 'body': '@HeartSaVioR, @Ngone51 Could you help take a look and remove `stale` label?', 'reactions': {'url': 'https://api.github.com/repos/apache/spark/issues/comments/1034591698/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'performed_via_github_app': None}}",created,Scala
20168125872,IssueCommentEvent,apache/spark,0.0,2022-02-10T07:44:07Z,"{'action': 'created', 'issue': {'url': 'https://api.github.com/repos/apache/spark/issues/31870', 'repository_url': 'https://api.github.com/repos/apache/spark', 'labels_url': 'https://api.github.com/repos/apache/spark/issues/31870/labels{/name}', 'comments_url': 'https://api.github.com/repos/apache/spark/issues/31870/comments', 'events_url': 'https://api.github.com/repos/apache/spark/issues/31870/events', 'html_url': 'https://github.com/apache/spark/pull/31870', 'id': 834013244, 'node_id': 'MDExOlB1bGxSZXF1ZXN0NTk0OTE1Mjk3', 'number': 31870, 'title': '[SPARK-32288][UI] Add exception summary for failed tasks in stage page ', 'user': {'login': 'warrenzhu25', 'id': 1633312, 'node_id': 'MDQ6VXNlcjE2MzMzMTI=', 'avatar_url': 'https://avatars.githubusercontent.com/u/1633312?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/warrenzhu25', 'html_url': 'https://github.com/warrenzhu25', 'followers_url': 'https://api.github.com/users/warrenzhu25/followers', 'following_url': 'https://api.github.com/users/warrenzhu25/following{/other_user}', 'gists_url': 'https://api.github.com/users/warrenzhu25/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/warrenzhu25/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/warrenzhu25/subscriptions', 'organizations_url': 'https://api.github.com/users/warrenzhu25/orgs', 'repos_url': 'https://api.github.com/users/warrenzhu25/repos', 'events_url': 'https://api.github.com/users/warrenzhu25/events{/privacy}', 'received_events_url': 'https://api.github.com/users/warrenzhu25/received_events', 'type': 'User', 'site_admin': False}, 'labels': [{'id': 1406605079, 'node_id': 'MDU6TGFiZWwxNDA2NjA1MDc5', 'url': 'https://api.github.com/repos/apache/spark/labels/WEB%20UI', 'name': 'WEB UI', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1753478521, 'node_id': 'MDU6TGFiZWwxNzUzNDc4NTIx', 'url': 'https://api.github.com/repos/apache/spark/labels/Stale', 'name': 'Stale', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}], 'state': 'closed', 'locked': False, 'assignee': None, 'assignees': [], 'milestone': None, 'comments': 2, 'created_at': '2021-03-17T17:20:10Z', 'updated_at': '2022-02-10T07:44:07Z', 'closed_at': '2021-06-30T00:07:45Z', 'author_association': 'CONTRIBUTOR', 'active_lock_reason': None, 'draft': False, 'pull_request': {'url': 'https://api.github.com/repos/apache/spark/pulls/31870', 'html_url': 'https://github.com/apache/spark/pull/31870', 'diff_url': 'https://github.com/apache/spark/pull/31870.diff', 'patch_url': 'https://github.com/apache/spark/pull/31870.patch', 'merged_at': None}, 'body': ""### What changes were proposed in this pull request?\r\nAdd exception summary table for failed tasks in stage page.\r\nBefore:\r\n![image](https://user-images.githubusercontent.com/1633312/97725663-aec73800-1a8b-11eb-891e-182cea919167.png)\r\n\r\nAfter:\r\n![image](https://user-images.githubusercontent.com/1633312/97725069-f7cabc80-1a8a-11eb-9130-00a8aa6ced65.png)\r\n\r\n### Why are the changes needed?\r\nWhen there're many task failure during one stage, it's hard to find failure pattern such as aggregation task failure by exception type and message. If we have such information, we can easily know which type of exception of failure is the root cause of stage failure.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nYes\r\n\r\n### How was this patch tested?\r\nAdded UT in UISeleniumSuite\r\n"", 'reactions': {'url': 'https://api.github.com/repos/apache/spark/issues/31870/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/apache/spark/issues/31870/timeline', 'performed_via_github_app': None}, 'comment': {'url': 'https://api.github.com/repos/apache/spark/issues/comments/1034590115', 'html_url': 'https://github.com/apache/spark/pull/31870#issuecomment-1034590115', 'issue_url': 'https://api.github.com/repos/apache/spark/issues/31870', 'id': 1034590115, 'node_id': 'IC_kwDOAQXtWs49qpej', 'user': {'login': 'warrenzhu25', 'id': 1633312, 'node_id': 'MDQ6VXNlcjE2MzMzMTI=', 'avatar_url': 'https://avatars.githubusercontent.com/u/1633312?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/warrenzhu25', 'html_url': 'https://github.com/warrenzhu25', 'followers_url': 'https://api.github.com/users/warrenzhu25/followers', 'following_url': 'https://api.github.com/users/warrenzhu25/following{/other_user}', 'gists_url': 'https://api.github.com/users/warrenzhu25/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/warrenzhu25/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/warrenzhu25/subscriptions', 'organizations_url': 'https://api.github.com/users/warrenzhu25/orgs', 'repos_url': 'https://api.github.com/users/warrenzhu25/repos', 'events_url': 'https://api.github.com/users/warrenzhu25/events{/privacy}', 'received_events_url': 'https://api.github.com/users/warrenzhu25/received_events', 'type': 'User', 'site_admin': False}, 'created_at': '2022-02-10T07:44:07Z', 'updated_at': '2022-02-10T07:44:07Z', 'author_association': 'CONTRIBUTOR', 'body': '@gengliangwang @sarutak @tgravescs  Could you help take a look and remove `stale` label?', 'reactions': {'url': 'https://api.github.com/repos/apache/spark/issues/comments/1034590115/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'performed_via_github_app': None}}",created,Scala
20168102115,IssueCommentEvent,apache/spark,0.0,2022-02-10T07:42:27Z,"{'action': 'created', 'issue': {'url': 'https://api.github.com/repos/apache/spark/issues/34035', 'repository_url': 'https://api.github.com/repos/apache/spark', 'labels_url': 'https://api.github.com/repos/apache/spark/issues/34035/labels{/name}', 'comments_url': 'https://api.github.com/repos/apache/spark/issues/34035/comments', 'events_url': 'https://api.github.com/repos/apache/spark/issues/34035/events', 'html_url': 'https://github.com/apache/spark/pull/34035', 'id': 999661296, 'node_id': 'PR_kwDOAQXtWs4r6Ah1', 'number': 34035, 'title': '[SPARK-36793][K8S] Support write container stdout/stderr to file', 'user': {'login': 'warrenzhu25', 'id': 1633312, 'node_id': 'MDQ6VXNlcjE2MzMzMTI=', 'avatar_url': 'https://avatars.githubusercontent.com/u/1633312?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/warrenzhu25', 'html_url': 'https://github.com/warrenzhu25', 'followers_url': 'https://api.github.com/users/warrenzhu25/followers', 'following_url': 'https://api.github.com/users/warrenzhu25/following{/other_user}', 'gists_url': 'https://api.github.com/users/warrenzhu25/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/warrenzhu25/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/warrenzhu25/subscriptions', 'organizations_url': 'https://api.github.com/users/warrenzhu25/orgs', 'repos_url': 'https://api.github.com/users/warrenzhu25/repos', 'events_url': 'https://api.github.com/users/warrenzhu25/events{/privacy}', 'received_events_url': 'https://api.github.com/users/warrenzhu25/received_events', 'type': 'User', 'site_admin': False}, 'labels': [{'id': 1406605057, 'node_id': 'MDU6TGFiZWwxNDA2NjA1MDU3', 'url': 'https://api.github.com/repos/apache/spark/labels/KUBERNETES', 'name': 'KUBERNETES', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1753478521, 'node_id': 'MDU6TGFiZWwxNzUzNDc4NTIx', 'url': 'https://api.github.com/repos/apache/spark/labels/Stale', 'name': 'Stale', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981471081, 'node_id': 'MDU6TGFiZWwxOTgxNDcxMDgx', 'url': 'https://api.github.com/repos/apache/spark/labels/DOCS', 'name': 'DOCS', 'color': 'ededed', 'default': False, 'description': None}], 'state': 'closed', 'locked': False, 'assignee': None, 'assignees': [], 'milestone': None, 'comments': 3, 'created_at': '2021-09-17T19:33:38Z', 'updated_at': '2022-02-10T07:42:27Z', 'closed_at': '2022-01-03T00:12:12Z', 'author_association': 'CONTRIBUTOR', 'active_lock_reason': None, 'draft': False, 'pull_request': {'url': 'https://api.github.com/repos/apache/spark/pulls/34035', 'html_url': 'https://github.com/apache/spark/pull/34035', 'diff_url': 'https://github.com/apache/spark/pull/34035.diff', 'patch_url': 'https://github.com/apache/spark/pull/34035.patch', 'merged_at': None}, 'body': '### What changes were proposed in this pull request?\r\nSupport write container stdout/stderr to file\r\n\r\n### Why are the changes needed?\r\nIf users want to sidecar logging agent to send stdout/stderr to external log storage,  only way is to change entrypoint.sh, which might break compatibility with community version.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nYes. User can enable this feature by spark config.\r\n\r\n### How was this patch tested?\r\nAdded UT in BasicDriverFeatureStepSuite and BasicExecutorFeatureStepSuite\r\n', 'reactions': {'url': 'https://api.github.com/repos/apache/spark/issues/34035/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/apache/spark/issues/34035/timeline', 'performed_via_github_app': None}, 'comment': {'url': 'https://api.github.com/repos/apache/spark/issues/comments/1034588906', 'html_url': 'https://github.com/apache/spark/pull/34035#issuecomment-1034588906', 'issue_url': 'https://api.github.com/repos/apache/spark/issues/34035', 'id': 1034588906, 'node_id': 'IC_kwDOAQXtWs49qpLq', 'user': {'login': 'warrenzhu25', 'id': 1633312, 'node_id': 'MDQ6VXNlcjE2MzMzMTI=', 'avatar_url': 'https://avatars.githubusercontent.com/u/1633312?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/warrenzhu25', 'html_url': 'https://github.com/warrenzhu25', 'followers_url': 'https://api.github.com/users/warrenzhu25/followers', 'following_url': 'https://api.github.com/users/warrenzhu25/following{/other_user}', 'gists_url': 'https://api.github.com/users/warrenzhu25/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/warrenzhu25/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/warrenzhu25/subscriptions', 'organizations_url': 'https://api.github.com/users/warrenzhu25/orgs', 'repos_url': 'https://api.github.com/users/warrenzhu25/repos', 'events_url': 'https://api.github.com/users/warrenzhu25/events{/privacy}', 'received_events_url': 'https://api.github.com/users/warrenzhu25/received_events', 'type': 'User', 'site_admin': False}, 'created_at': '2022-02-10T07:42:27Z', 'updated_at': '2022-02-10T07:42:27Z', 'author_association': 'CONTRIBUTOR', 'body': '@HyukjinKwon Could you help take a look and remove `stale`  label?', 'reactions': {'url': 'https://api.github.com/repos/apache/spark/issues/comments/1034588906/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'performed_via_github_app': None}}",created,Scala
20118257716,IssuesEvent,monotosh-k/gcp-docs,0.0,2022-02-07T22:05:27Z,"{'action': 'opened', 'issue': {'url': 'https://api.github.com/repos/monotosh-k/gcp-docs/issues/4', 'repository_url': 'https://api.github.com/repos/monotosh-k/gcp-docs', 'labels_url': 'https://api.github.com/repos/monotosh-k/gcp-docs/issues/4/labels{/name}', 'comments_url': 'https://api.github.com/repos/monotosh-k/gcp-docs/issues/4/comments', 'events_url': 'https://api.github.com/repos/monotosh-k/gcp-docs/issues/4/events', 'html_url': 'https://github.com/monotosh-k/gcp-docs/issues/4', 'id': 1126550719, 'node_id': 'I_kwDOC5qLNM5DJcy_', 'number': 4, 'title': 'Provide docker install way', 'user': {'login': 'warrenzhu25', 'id': 1633312, 'node_id': 'MDQ6VXNlcjE2MzMzMTI=', 'avatar_url': 'https://avatars.githubusercontent.com/u/1633312?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/warrenzhu25', 'html_url': 'https://github.com/warrenzhu25', 'followers_url': 'https://api.github.com/users/warrenzhu25/followers', 'following_url': 'https://api.github.com/users/warrenzhu25/following{/other_user}', 'gists_url': 'https://api.github.com/users/warrenzhu25/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/warrenzhu25/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/warrenzhu25/subscriptions', 'organizations_url': 'https://api.github.com/users/warrenzhu25/orgs', 'repos_url': 'https://api.github.com/users/warrenzhu25/repos', 'events_url': 'https://api.github.com/users/warrenzhu25/events{/privacy}', 'received_events_url': 'https://api.github.com/users/warrenzhu25/received_events', 'type': 'User', 'site_admin': False}, 'labels': [], 'state': 'open', 'locked': False, 'assignee': None, 'assignees': [], 'milestone': None, 'comments': 0, 'created_at': '2022-02-07T22:05:27Z', 'updated_at': '2022-02-07T22:05:27Z', 'closed_at': None, 'author_association': 'NONE', 'active_lock_reason': None, 'body': ""It's hard to install node-gyp correctly. Could we have  a docker version?"", 'reactions': {'url': 'https://api.github.com/repos/monotosh-k/gcp-docs/issues/4/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/monotosh-k/gcp-docs/issues/4/timeline', 'performed_via_github_app': None}}",opened,JavaScript
